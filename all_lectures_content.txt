-------------------------------------------
FILE: ARIMA_AmazonDataCamp_compressed.pdf
-------------------------------------------
Intro to ACF and
PACF
F ORECA S TIN G US IN G A RIMA MODELS IN P YTH ON

James Fulton
Climate informatics researcher

Motivation

FORECASTING USING ARIMA MODELS IN PYTHON

ACF and PACF
ACF - Autocorrelation Function
PACF - Partial autocorrelation function

FORECASTING USING ARIMA MODELS IN PYTHON

What is the ACF
lag-1 autocorrelation → corr(yt , yt−1 )
lag-2 autocorrelation → corr(yt , yt−2 )
...
lag-n autocorrelation → corr(yt , yt−n )

FORECASTING USING ARIMA MODELS IN PYTHON

What is the ACF

FORECASTING USING ARIMA MODELS IN PYTHON

What is the PACF

FORECASTING USING ARIMA MODELS IN PYTHON

Using ACF and PACF to choose model order

AR(2) model →

FORECASTING USING ARIMA MODELS IN PYTHON

Using ACF and PACF to choose model order

MA(2) model →

FORECASTING USING ARIMA MODELS IN PYTHON

Using ACF and PACF to choose model order

FORECASTING USING ARIMA MODELS IN PYTHON

Using ACF and PACF to choose model order

FORECASTING USING ARIMA MODELS IN PYTHON

Implementation in Python
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Create figure
fig, (ax1, ax2) = plt.subplots(2,1, figsize=(8,8))
# Make ACF plot
plot_acf(df, lags=10, zero=False, ax=ax1)
# Make PACF plot
plot_pacf(df, lags=10, zero=False, ax=ax2)
plt.show()

FORECASTING USING ARIMA MODELS IN PYTHON

Implementation in Python

FORECASTING USING ARIMA MODELS IN PYTHON

Over/under differencing and ACF and PACF

FORECASTING USING ARIMA MODELS IN PYTHON

Over/under differencing and ACF and PACF

FORECASTING USING ARIMA MODELS IN PYTHON

Let's practice!
F ORECA S TIN G US IN G A RIMA MODELS IN P YTH ON

AIC and BIC
F ORECA S TIN G US IN G A RIMA MODELS IN P YTH ON

James Fulton
Climate informatics researcher

AIC - Akaike information criterion
Lower AIC indicates a better model
AIC likes to choose simple models with lower order

FORECASTING USING ARIMA MODELS IN PYTHON

BIC - Bayesian information criterion
Very similar to AIC
Lower BIC indicates a better model
BIC likes to choose simple models with lower order

FORECASTING USING ARIMA MODELS IN PYTHON

AIC vs BIC
BIC favors simpler models than AIC
AIC is better at choosing predictive models
BIC is better at choosing good explanatory model

FORECASTING USING ARIMA MODELS IN PYTHON

AIC and BIC in statsmodels
# Create model
model = SARIMAX(df, order=(1,0,1))
# Fit model
results = model.fit()
# Print fit summary
print(results.summary())

Statespace Model Results
==============================================================================
Dep. Variable:

y

No. Observations:

1000

Model:

SARIMAX(2, 0, 0)

Log Likelihood

-1399.704

Date:

Fri, 10 May 2019

AIC

2805.407

Time:

01:06:11

BIC

2820.131

Sample:

01-01-2013

HQIC

2811.003

- 09-27-2015
Covariance Type:

opg

FORECASTING USING ARIMA MODELS IN PYTHON

AIC and BIC in statsmodels
# Create model
model = SARIMAX(df, order=(1,0,1))
# Fit model
results = model.fit()
# Print AIC and BIC
print('AIC:', results.aic)
print('BIC:', results.bic)

AIC: 2806.36
BIC: 2821.09

FORECASTING USING ARIMA MODELS IN PYTHON

Searching over AIC and BIC
# Loop over AR order
for p in range(3):
# Loop over MA order
for q in range(3):
# Fit model
model = SARIMAX(df, order=(p,0,q))
results = model.fit()
# print the model order and the AIC/BIC values
print(p, q, results.aic, results.bic)

0 0 2900.13 2905.04
0 1 2828.70 2838.52
0 2 2806.69 2821.42
1 0 2810.25 2820.06
1 1 2806.37 2821.09
1 2 2807.52 2827.15
...

FORECASTING USING ARIMA MODELS IN PYTHON

Searching over AIC and BIC
order_aic_bic =[]
# Loop over AR order
for p in range(3):
# Loop over MA order
for q in range(3):
# Fit model
model = SARIMAX(df, order=(p,0,q))
results = model.fit()
# Add order and scores to list
order_aic_bic.append((p, q, results.aic, results.bic))

# Make DataFrame of model order and AIC/BIC scores
order_df = pd.DataFrame(order_aic_bic, columns=['p','q', 'aic', 'bic'])

FORECASTING USING ARIMA MODELS IN PYTHON

Searching over AIC and BIC
# Sort by AIC

# Sort by BIC

print(order_df.sort_values('aic'))

print(order_df.sort_values('bic'))

p

q

aic

bic

7

2

1

2804.54

2824.17

6

2

0

2805.41

4

1

1

2

0

2

...

p

q

aic

bic

3

1

0

2810.25

2820.06

2820.13

6

2

0

2805.41

2820.13

2806.37

2821.09

4

1

1

2806.37

2821.09

2806.69

2821.42

2

0

2

2806.69

2821.42

...

FORECASTING USING ARIMA MODELS IN PYTHON

Non-stationary model orders
# Fit model
model = SARIMAX(df, order=(2,0,1))
results = model.fit()

ValueError: Non-stationary starting autoregressive parameters
found with `enforce_stationarity` set to True.

FORECASTING USING ARIMA MODELS IN PYTHON

When certain orders don't work
# Loop over AR order
for p in range(3):
# Loop over MA order
for q in range(3):
# Fit model
model = SARIMAX(df, order=(p,0,q))
results = model.fit()
# Print the model order and the AIC/BIC values
print(p, q, results.aic, results.bic)

FORECASTING USING ARIMA MODELS IN PYTHON

When certain orders don't work
# Loop over AR order
for p in range(3):
# Loop over MA order
for q in range(3):
try:
# Fit model
model = SARIMAX(df, order=(p,0,q))
results = model.fit()
# Print the model order and the AIC/BIC values
print(p, q, results.aic, results.bic)
except:
# Print AIC and BIC as None when fails
print(p, q, None, None)

FORECASTING USING ARIMA MODELS IN PYTHON

Let's practice!
F ORECA S TIN G US IN G A RIMA MODELS IN P YTH ON

Model diagnostics
F ORECA S TIN G US IN G A RIMA MODELS IN P YTH ON

James Fulton
Climate informatics researcher

Introduction to model diagnostics
How good is the nal model?

FORECASTING USING ARIMA MODELS IN PYTHON

Residuals

FORECASTING USING ARIMA MODELS IN PYTHON

Residuals
# Fit model
model = SARIMAX(df, order=(p,d,q))
results = model.fit()
# Assign residuals to variable
residuals = results.resid

2013-01-23

1.013129

2013-01-24

0.114055

2013-01-25

0.430698

2013-01-26

-1.247046

2013-01-27

-0.499565

...

...

FORECASTING USING ARIMA MODELS IN PYTHON

Mean absolute error
How far our the predictions from the real values?
mae = np.mean(np.abs(residuals))

FORECASTING USING ARIMA MODELS IN PYTHON

Plot diagnostics
If the model ts well the residuals will be white
Gaussian noise
# Create the 4 diagostics plots
results.plot_diagnostics()
plt.show()

FORECASTING USING ARIMA MODELS IN PYTHON

Residuals plot

FORECASTING USING ARIMA MODELS IN PYTHON

Residuals plot

FORECASTING USING ARIMA MODELS IN PYTHON

Histogram plus estimated density

FORECASTING USING ARIMA MODELS IN PYTHON

Normal Q-Q

FORECASTING USING ARIMA MODELS IN PYTHON

Correlogram

FORECASTING USING ARIMA MODELS IN PYTHON

Summary statistics
print(results.summary())

...
===================================================================================
Ljung-Box (Q):

32.10

Jarque-Bera (JB):

0.02

Prob(Q):

0.81

Prob(JB):

0.99

Heteroskedasticity (H):

1.28

Skew:

-0.02

Prob(H) (two-sided):

0.21

Kurtosis:

2.98

===================================================================================

Prob(Q) - p-value for null hypothesis that residuals are uncorrelated
Prob(JB) - p-value for null hypothesis that residuals are normal

FORECASTING USING ARIMA MODELS IN PYTHON

Let's practice!
F ORECA S TIN G US IN G A RIMA MODELS IN P YTH ON

Box-Jenkins method
F ORECA S TIN G US IN G A RIMA MODELS IN P YTH ON

James Fulton
Climate informatics researcher

The Box-Jenkins method
From raw data → production model
identi cation
estimation
model diagnostics

FORECASTING USING ARIMA MODELS IN PYTHON

Identi cation
Is the time series stationary?
What differencing will make it stationary?
What transforms will make it stationary?
What values of p and q are most promising?

FORECASTING USING ARIMA MODELS IN PYTHON

Identi cation tools
Plot the time series
df.plot()

Use augmented Dicky-Fuller test
adfuller()

Use transforms and/or differencing
df.diff() , np.log() , np.sqrt()
Plot ACF/PACF
plot_acf() , plot_pacf()

FORECASTING USING ARIMA MODELS IN PYTHON

Estimation
Use the data to train the model coef cients
Done for us using model.fit()
Choose between models using AIC and BIC
results.aic , results.bic

FORECASTING USING ARIMA MODELS IN PYTHON

Model diagnostics
Are the residuals uncorrelated
Are residuals normally distributed
results.plot_diagnostics()
results.summary()

FORECASTING USING ARIMA MODELS IN PYTHON

Decision

FORECASTING USING ARIMA MODELS IN PYTHON

Repeat
We go through the process again with more
information
Find a better model

FORECASTING USING ARIMA MODELS IN PYTHON

Production
Ready to make forecasts
results.get_forecast()

FORECASTING USING ARIMA MODELS IN PYTHON

Box-Jenkins

FORECASTING USING ARIMA MODELS IN PYTHON

Let's practice!
F ORECA S TIN G US IN G A RIMA MODELS IN P YTH ON




-------------------------------------------
FILE: DecisionTrees_compressed.pdf
-------------------------------------------
Decision trees
Predictive Analytics

Predictive Analytics

Decision trees

1/2

Acknowledgments

These slides draw upon and adapt selected materials by:
Fragkiskos D. Malliaros (CentraleSupélec, Université Paris–Saclay, France)
David Sontag (MIT CSAIL, USA)

Predictive Analytics

Decision trees

2/2

Another Classification Idea (2/2)
•

Gives axes aligned decision boundaries !

9!

Example of a Decision Tree

Tid Refund Marital
Status

Taxable
Income Cheat

1

Yes

Single

125K

No

2

No

Married

100K

No

3

No

Single

70K

No

4

Yes

Married

120K

No

5

No

Divorced 95K

Yes

6

No

Married

60K

No

7

Yes

Divorced 220K

No

8

No

Single

85K

Yes

9

No

Married

75K

No

10

No

Single

90K

Yes

Splitting Attributes!

Refund!
Yes!

No!

NO!

MarSt!
Single, Divorced!
TaxInc!

< 80K!
NO!

Married !
NO!

> 80K!
YES!

10

Training Data!

Model: Decision Tree!
10!

Another Example of Decision Tree

Tid Refund Marital
Status

Taxable
Income Cheat

1

Yes

Single

125K

No

2

No

Married

100K

No

3

No

Single

70K

No

4

Yes

Married

120K

No

5

No

Divorced 95K

Yes

6

No

Married

60K

No

7

Yes

Divorced 220K

No

8

No

Single

85K

Yes

9

No

Married

75K

No

10

No

Single

90K

Yes

Married !

MarSt!

NO!

Single,
Divorced!
Refund!
No!

Yes!
NO!

TaxInc!
< 80K!
NO!

> 80K!
YES!

There could be more than one tree that
fits the same data!!

10

11!

Decision Trees – Nodes and Branching
•
•
•

Internal nodes correspond to test attributes!
Branching is determined by attribute value!
Leaf nodes are outputs (class assignments)!
– YES or NO in this example!

Married !

MarSt!

NO!

Single,
Divorced!
Refund!
No!

Yes!
NO!

TaxInc!
< 80K!
NO!

> 80K!
YES!
12!

Decision Tree Classification Task
Tid

Attrib1

Attrib3

Class

1

Yes

Large

Attrib2

125K

No

2

No

Medium

100K

No

3

No

Small

70K

No

4

Yes

Medium

120K

No

5

No

Large

95K

Yes

6

No

Medium

60K

No

7

Yes

Large

220K

No

8

No

Small

85K

Yes

9

No

Medium

75K

No

10

No

Small

90K

Yes

Tree
Induction
algorithm
Induction
Learn
Model
Model

10

Training Set
Tid

Attrib1

11

No

12

Yes

13

Attrib2

Attrib3

Class

Small

55K

?

Medium

80K

?

Yes

Large

110K

?

14

No

Small

95K

?

15

No

Large

67K

?

Apply
Model

Decision
Tree!

Deduction

10

Test Set
13!

Apply Model to Test Data
Test Data!
Start from the root of tree!

Refund!
No!

NO!

MarSt!
Single, Divorced!
TaxInc!

NO!

Taxable
Income Cheat

No

80K

Married

?

10

Yes!

< 80K!

Refund Marital
Status

Married !
NO!

> 80K!
YES!

14!

Apply Model to Test Data
Test Data!

Refund!
No!

NO!

MarSt!
Single, Divorced!
TaxInc!

NO!

Taxable
Income Cheat

No

80K

Married

?

10

Yes!

< 80K!

Refund Marital
Status

Married !
NO!

> 80K!
YES!

15!

Apply Model to Test Data
Test Data!

Refund!
No!

NO!

MarSt!
Single, Divorced!
TaxInc!

NO!

Taxable
Income Cheat

No

80K

Married

?

10

Yes!

< 80K!

Refund Marital
Status

Married !
NO!

> 80K!
YES!

16!

Apply Model to Test Data
Test Data!

Refund!
No!

NO!

MarSt!
Single, Divorced!
TaxInc!

NO!

Taxable
Income Cheat

No

80K

Married

?

10

Yes!

< 80K!

Refund Marital
Status

Married !
NO!

> 80K!
YES!

17!

Apply Model to Test Data
Test Data!

Refund!
No!

NO!

MarSt!
Single, Divorced!
TaxInc!

NO!

Taxable
Income Cheat

No

80K

Married

?

10

Yes!

< 80K!

Refund Marital
Status

Married !
NO!

> 80K!
YES!

18!

Apply Model to Test Data
Test Data!

Refund!
No!

NO!

MarSt!
Single, Divorced!
TaxInc!

NO!

Taxable
Income Cheat

No

80K

Married

?

10

Yes!

< 80K!

Refund Marital
Status

Married !

Assign Cheat to “No”!

NO!
> 80K!
YES!

19!

Decision Tree Classification Task
Tid

Attrib1

Attrib3

Class

1

Yes

Large

Attrib2

125K

No

2

No

Medium

100K

No

3

No

Small

70K

No

4

Yes

Medium

120K

No

5

No

Large

95K

Yes

6

No

Medium

60K

No

7

Yes

Large

220K

No

8

No

Small

85K

Yes

9

No

Medium

75K

No

10

No

Small

90K

Yes

Tree
Induction
algorithm
Induction
Learn
Model
Model

10

Training Set
Tid

Attrib1

11

No

12

Yes

13

Attrib2

Attrib3

Class

Small

55K

?

Medium

80K

?

Yes

Large

110K

?

14

No

Small

95K

?

15

No

Large

67K

?

Apply
Model

Decision
Tree!

Deduction

10

Test Set
20!

Decision Tree Induction – The Idea (1/2)
• Basic algorithm!
– Tree is constructed in a top-down recursive manner!
– Initially, all the training examples are at the root!
– Attributes are categorical (if continuous-valued, they are
discretized in advance)!
– Examples are partitioned recursively based on the selected
attributes!
– Split attributes are selected on the basis of a heuristic or
statistical measure (e.g., gini index, information gain)!

• Most commercial DTs use variations of this algorithm!

21!

Decision Tree Induction – The Idea (2/2)
• Simple, greedy, recursive approach, builds up tree node-bynode!
1. Pick an attribute to split at a non-terminal node!
2. Split examples into groups based on attribute value!
3. For each group:!
–
–
–

If no examples – return class majority from parent node!
Else, if all examples are in the same class – return class!
Else, loop to step 1!

22!

Decision Tree Induction Algorithms
• Many algorithms!
– Hunt’s algorithm (one of the earliest)!
– CART!
– ID3, C4.5!
– SLIQ, SPRINT!

23!

General Structure of Hunt’s Algorithm
• Let Dt be the set of training
records that reach a node t!
• General procedure:!
– If Dt contains records that
belong to the same class yt,
then t is a leaf node labeled as
yt!
– If Dt is an empty set, then t is a
leaf node labeled by the default
class, yd!
– If Dt contains records that
belong to more than one
classes, use an attribute test to
split the data into smaller
subsets !
• Recursively apply the
procedure to each subset!

Tid Refund Marital
Status

Taxable
Income Cheat

1

Yes

Single

125K

No

2

No

Married

100K

No

3

No

Single

70K

No

4

Yes

Married

120K

No

5

No

Divorced 95K

Yes

6

No

Married

60K

No

7

Yes

Divorced 220K

No

8

No

Single

85K

Yes

9

No

Married

75K

No

10

No

Single

90K

Yes

10

Dt!

?!

24!

Hunt’s Algorithm
Refund!

Yes!

No!

Cheat!
= No!

Cheat!
= Yes!

Tid Refund Marital
Status

Taxable
Income Cheat

1

Yes

Single

125K

No

2

No

Married

100K

No

3

No

Single

70K

No

4

Yes

Married

120K

No

5

No

Divorced 95K

Yes

6

No

Married

60K

No

7

Yes

Divorced 220K

No

8

No

Single

85K

Yes

9

No

Married

75K

No

10

No

Single

90K

Yes

10

25!

Hunt’s Algorithm
Refund!

Yes!

No!

Cheat!
= No!

Cheat!
= Yes!

Refund!

Yes!
Cheat!
= No!
Single,!
Divorced!

Cheat!
= Yes!

No!
Marital!
Status!

Tid Refund Marital
Status

Taxable
Income Cheat

1

Yes

Single

125K

No

2

No

Married

100K

No

3

No

Single

70K

No

4

Yes

Married

120K

No

5

No

Divorced 95K

Yes

6

No

Married

60K

No

7

Yes

Divorced 220K

No

8

No

Single

85K

Yes

9

No

Married

75K

No

10

No

Single

90K

Yes

10

Married!

Cheat!
= No!

26!

Hunt’s Algorithm
Refund!

Yes!

No!

Cheat!
= No!

Cheat!
= Yes!

Refund!

Refund!

Yes!
Cheat!
= No!
Single,!
Divorced!

Cheat!
= Yes!

No!
Marital!
Status!
Married!

Yes!

No!

Cheat!
= No!

Marital!
Status!

Single,!
Divorced!

Taxable
Income Cheat

1

Yes

Single

125K

No

2

No

Married

100K

No

3

No

Single

70K

No

4

Yes

Married

120K

No

5

No

Divorced 95K

Yes

6

No

Married

60K

No

7

Yes

Divorced 220K

No

8

No

Single

85K

Yes

9

No

Married

75K

No

10

No

Single

90K

Yes

10

Married!

Cheat!
No!

Taxable!
Income!

Cheat!
= No!

Tid Refund Marital
Status

< 80K!

>= 80K!

Cheat!
= No!

Cheat!
= Yes!
27!

Tree Induction
• Greedy strategy!
– Split the records based on an attribute test that optimizes a
certain criterion!

• Issues!
– Determine how to split the records!
• How to specify the attribute test condition?!
• How to determine the best split?!
– Determine when to stop splitting!

28!

How to Specify Test Condition?
• Depends on attribute types!
– Nominal (categorical)!
• E.g., female, male!
– Ordinal (categorical, but there is an ordering)!
• E.g., economic status: low, medium, high!
– Continuous!

• Depends on the number of ways to split!
– 2-way split!
– Multi-way split!

29!

Splitting Based on Nominal Attributes
• Multi-way split: Use as many partitions as distinct values!
CarType!
Family!

Luxury!

No ordering of
the attributes!

Sport!

• Binary split: Divides values into two subsets !

!

(Need to find optimal partitioning)

{Sport,
Luxury}!

CarType!
{Family}!

OR!

{Family, !
Luxury}!

CarType!
{Sport}!

30!

Splitting Based on Ordinal Attributes
• Multi-way split: Use as many partitions as distinct values!
Size!
Small!

Large!

Medium!

• Binary split: Divides values into two subsets !
(Need to find optimal partitioning)!

{Small,
Medium}!

Size!
{Large}!

OR!

{Medium, !
Large}!

Size!
{Small}!

31!

Splitting Based on Continuous Attributes
• Different ways of handling!
– Discretization to form an ordinal categorical attribute!
• Static – discretize once at the beginning!
• Dynamic – ranges can be found by equal interval
bucketing, equal frequency bucketing!
(percentiles), or clustering.!
– Binary decision: (A < v) or (A ≥ v)!
• Considers all possible splits and finds the best cut!
• Can be more computational intensive!

32!

Splitting Based on Continuous Attributes

Taxable
Income
> 80K?

Taxable
Income?
< 10K

Yes

> 80K

No
[10K,25K)

(i) Binary split

[25K,50K)

[50K,80K)

(ii) Multi-way split

33!

Tree Induction
• Greedy strategy!
– Split the records based on an attribute test that optimizes
certain criterion!

• Issues!
– Determine how to split the records!
• How to specify the attribute test condition?!
• How to determine the best split?!
– Determine when to stop splitting!

34!

How to Determine the Best Split (1/2)
Before splitting: 10 records of class 0!
10 records of class 1!
Own
Car?
Yes

Suppose that we want to predict if a
student will get tax return or no?!

Car
Type?
No

Family

Student
ID?
Luxury

c1

Sports
C0: 6
C1: 4

C0: 4
C1: 6

C0: 1
C1: 3

C0: 8
C1: 0

C0: 1
C1: 7

C0: 1
C1: 0

c10

1
... C0:
C1: 0

c11
C0: 0
C1: 1

c20
0
... C0:
C1: 1

Which test condition is the best?!

35!

How to Determine the Best Split (1/2)
Before splitting: 10 records of class 0!
10 records of class 1!
Own
Car?
Yes

Suppose that we want to predict if a
student will get tax return or no?!

Car
Type?
No

Family

Student
ID?
Luxury

c1

Sports
C0: 6
C1: 4

C0: 4
C1: 6

C0: 1
C1: 3

C0: 8
C1: 0

C0: 1
C1: 7

C0: 1
C1: 0

c10

1
... C0:
C1: 0

c11
C0: 0
C1: 1

c20
0
... C0:
C1: 1

Purer partition

Which test condition is the best?!

36!

How to Determine the Best Split (2/2)
• Greedy approach: !
– Nodes with homogeneous (pure) class distribution are preferred!

• Need a measure of node impurity:!
!

C0: 5
C1: 5

C0: 9
C1: 1

Non-homogeneous,!

Homogeneous,!

High degree of impurity!

Low degree of impurity!

Very impure!

Medium impure!

Pure!

37!

Measures of Node Impurity
• Gini Index!
• Entropy and Information Gain!
• Misclassification error!

38!

How to Find the Best Split – Gini Index
Before Splitting:!

C0
C1

N00
N01

Two possible splits
on attributes A or B!

B?!

A?!
Yes!

Yes!

No!

No!

Node N1!

Node N2!

Node N3!

Node N4!

C0
C1

C0
C1

C0
C1

C0
C1

N10
N11

N20
N21

N30
N31

N40
N41

39!

How to Find the Best Split – Gini Index
Before Splitting:!

C0
C1

N00
N01

Two possible splits
on attributes A or B!

M0!
B?!

A?!
Yes!

Yes!

No!

No!

Node N1!

Node N2!

Node N3!

Node N4!

C0
C1

C0
C1

C0
C1

C0
C1

N10
N11

M1!

N20
N21

M2!

N30
N31

M3!

N40
N41

M4!

40!

How to Find the Best Split – Gini Index
Before Splitting:!

C0
C1

N00
N01

Two possible splits
on attributes A or B!

M0!
B?!

A?!
Yes!

Yes!

No!

No!

Node N1!

Node N2!

Node N3!

Node N4!

C0
C1

C0
C1

C0
C1

C0
C1

N10
N11

N20
N21

M2!

M1!

M12!

N30
N31

M3!

N40
N41

M4!

M34!

41!

How to Find the Best Split – Gini Index
Before Splitting:!

C0
C1

N00
N01

Two possible splits
on attributes A or B!

M0!
B?!

A?!
Yes!

Yes!

No!

No!

Node N1!

Node N2!

Node N3!

Node N4!

C0
C1

C0
C1

C0
C1

C0
C1

N10
N11

N20
N21

M2!

M1!

N30
N31

M3!

M12!

N40
N41

M4!

M34!
Gain = M0 – M12 vs. M0 – M34!
Reduction in
impurity!
42!

Measure of Impurity: Gini Index
Gini index for a
node t!

Gini(t) = 1

J
X
⇣

⌘2
p( j|t)

j=1

C1
C2

0
6

P(C1) = 0/6 = 0

C1
C2

1
5

P(C1) = 1/6

C1
C2

2
4

P(C1) = 2/6

Select the split with the
smallest Gini index!

•
•

J number of classes!
p( j | t) is the relative frequency
of class j at node t

P(C2) = 6/6 = 1!

Gini = 1 – P(C1)2 – P(C2)2 = 1 – 0 – 1 = 0 !
P(C2) = 5/6!

Gini = 1 – (1/6)2 – (5/6)2 = 0.278!
P(C2) = 4/6!

Gini = 1 – (2/6)2 – (4/6)2 = 0.444!

43!

Splitting based on Gini Index
• When a node t is split on attribute A into k partitions (children), the
quality of split is computed as!

GiniA =

k
X
n

i

i=1

•

n

Gini(i)

ni = number of records at child i!
n = number of records at node t!

Reduction in impurity (gain) after splitting node t (on attribute A), is
computed as !

GiniA = Gini(t)
•

GiniA

The attribute X with the smallest GiniX or the largest reduction in
impurity is chosen to split the node !

44!

Example
• Splits into two partitions!
Parent

– Goal: reduce impurity!

!
B?!
Yes!

No!

Node N1!

Gini(N1) !
= 1 – (5/7)2 – (2/7)2 !
= 0.408 !
Gini(N2) !
= 1 – (1/5)2 – (4/5)2 !
= 0.32!

C1

6

C2

6

Gini = 0.500

Node N2!

C1
C2

N1
5
2

N2
1
4

Gini=0.371

Split on attribute B!
GiniB !
= 7/12 * 0.408 + !
5/12 * 0.32!
= 0.371!

45!

Tree Induction
• Greedy strategy!
– Split the records based on an attribute test that optimizes
certain criterion!

• Issues!
– Determine how to split the records!
• How to specify the attribute test condition?!
• How to determine the best split?!
– Determine when to stop splitting!

46!

Stopping Criteria for Tree Induction
• Stop expanding a node when all the records belong to the
same class!
• Stop expanding a node when all the records have similar
attribute values!

47!

Decision Tree Based Classification
• Advantages!
– Inexpensive to construct (training phase)!
– Extremely fast at testing phase (classifying unseen data)!
– Easy to interpret for small-sized trees!
– Accuracy is comparable to other classification techniques for
many simple data sets!

48!

Overfitting and Tree Pruning
• Overfitting: An induced tree may overfit the training data!
– Too many branches, some may reflect anomalies due to noise or
outliers!

• Two approaches to avoid overfitting!
– Pre-pruning: Halt tree construction early; do not split a node if this
would result in the goodness measure falling below a threshold!
• Difficult to choose an appropriate threshold!
– Post-pruning: Remove branches from a fully grown tree!
• Get a sequence of progressively pruned trees!
• Use a dataset (different from the training data) to decide
which is the best pruned tree!

49!

scikit-learn

http://scikit-learn.org/stable/modules/generated/
sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier!

50!

Ensemble learning

51

Ensemble Methods
• Typical application: classification!
• Ensemble of classifiers: set of classifiers whose individual
decisions are combined in some way to classify new
examples!
• Simplest approach:!
1. Generate multiple classifiers (e.g., decision trees, logistic
regression)!
2. Each classifier votes (decides) on a test instance!
3. Take majority as classification!

• Classifiers are different due to different sampling of training
data, or randomized parameters within the classification
algorithm!
• Goal: take a simple algorithm and transform it into a ‘super
classifier’!
52!

Ensemble Learning – General Idea
D

Step 1:
Create Multiple
Data Sets
Step 2:
Build Multiple
Classifiers

Step 3:
Combine
Classifiers

D1

D2

C1

C2

....

Original
Training data

Dt-1

Dt

Ct -1

Ct

C*

53!

Ensemble Methods: Summary
• Differ in training strategy and combination method!
• Bagging (bootstrap aggregation)!
– Random sampling with replacement!
– Train separate models on overlapping training sets, average
their predictions!
– E.g., random forest classifier!

• Boosting!
– Sequential training, iteratively re-weighting the training
examples – the current classifier focuses on hard examples!
– E.g., AdaBoost!
54!

Bagging vs. Boosting

Source: https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/!
55!

Bagging: Bootstrap Estimation
• Repeatedly draw n samples from D!
• For each set of samples, estimate a statistic!
• The bootstrap estimate is the mean of the individual
estimates!
• Used to estimate a statistic (parameter) and its variance!
• Bagging: bootstrap aggregation (Breiman 1994)!

56!

Bagging
• Simple idea!
– Generate M bootstrap samples from your original training set !
– Train on each one to get ym , and average them!

yM
(x) =
bag

M

1 X
ym (x)
M
m=1

• Each bootstrap sample is drawn with replacement!
– Each one contains some duplicates of certain training points
and leaves out other training points completely!

• For regression: average the predictions!
• For classification: average class probabilities or take majority
vote!
57!

Boosting
• Also works by manipulating the training set, but the individual
classifiers are trained sequentially!
• Each classifier is trained based on knowledge of the
performance of previously trained classifiers!
– Focus on difficult examples!

• Final classifier: weighted sum of the component classifiers!

58!

AdaBoost: Making Weak Learners Stronger
• Suppose you have a weak learning module (a base classifier)
that can always get 0.5 + ε correct when given a binary
classification task!
– A little bit better than a random classifier!
– Weak learner!

• Can you apply this learning module many times to get a
strong learner that can get close to zero error rate on the
training data?!
– ML theorists showed how to do this and it actually led to an
effective new learning procedure (Freund & Shapire, 1996)!
– The AdaBoost algorithm!

59!

AdaBoost – The Idea
• Train T weak learners (models) sequentially!
• First train the base classifier on all the training data with equal
importance weights on each case!
• Then, re-weight the training data to emphasize the hard
cases and train a second model!
– Instances that were misclassified in the previous step!
– Q: How do we re-weight the data?!

• Keep training new models on the re-weighted data!
• Finally, use a weighted committee of all the models for the
test data!
– How do we weight the models in the committee?!
60!

How to Train Each Classifier
• Input:! (Dn = {(xi , ti )}ni=1 )
• Output:! y(xi ) 2 { 1, 1}
(t)
• Weight of instance (e.g., data point) xi for classifier t:! wi
• Cost function for classifier t: !

Jt =

n
X
i=1

(t)

wi I(yt (xi ) , ti ) =

X

weighted errors

1 if error, 0 otherwise!

61!

Weight of Instances for Classifier t
• Weighted error rate of a classifier t!
t = Pn

Jt

• The quality (coefficient) of classifier t is!
(
)
1
t
It is zero if the classifier has
↵t
ln
weighted error rate of 0.5 and
t

infinity if the classifier is perfect!

ln ((1 - γt) / γt)!

(t)
i=1 wi

γt!

• Update the weights for the next classifier (t+1)!
(t+1)

wi

(t)

wi exp{↵t I(yt (xi ) , ti )}

The weights ‘inform’ the training of the weak learner!
(decision trees can be grown that favor splitting sets of samples with
high weights)!

1 if error, 0 otherwise!
62!

How to Make Predictions

{w(1) }

{w(2) }

{w(T) }

y1 (x)

y2 (x)

yT (x)

YT (x) = sign

T
X

!
↵t yt (x)

• Weight the binary prediction
of each classifier by the
quality of that classifier!
YT (x) = sign

T
X

!
↵t yt (x)

t=1

t=1

63!

scikit-learn

http://scikit-learn.org/stable/modules/generated/
sklearn.ensemble.AdaBoostClassifier.html!

http://scikit-learn.org/stable/modules/ensemble.html!

65!

Hypotheses: decision trees f : X ! Y
• Each internal node
tests an attribute xi
• One branch for
each possible
attribute value xi=v
• Each leaf assigns a
class y
• To classify input x:
traverse the tree
from root to leaf,
output the labeled y

Cylinders	  

3	  
good

4	  
Maker	  

5	  
bad

america	  

asia	   europe	  

bad

good

good

6	  

8	  
bad

Horsepower	  

low	  
bad

med	   high	  
good

Human	  interpretable!	  

bad

Hypothesis space
• How many possible
hypotheses?
• What functions can be
represented?

Cylinders	  
3	  
good

4	  

Maker	  

5	  
bad

6	  

8	  
bad

america	   asia	   europe	  
bad

good

good

Horsepower	  

low	   med	   high	  
bad

good

bad

Expressiveness

Discrete-input, discrete-output case:
What	  funcGons	  
c– Decision
an	  btrees
e	  rcanepresented?	  
express any function of the input attribute
– E.g., for Boolean functions, truth table row

• Decision	  trees	  can	  represent	  
any	  funcGon	  of	  the	  input	  
aIributes!	  

A

B

A xor B

F
F
T
T

F
T
F
T

F
T
T
F

A

F
F
F

B

path to leaf:

T

T

F

T

T

B

T
F

(Figure	  from	  Stuart	  Russell)	  

Continuous-input, continuous-output case:

• For	  Boolean	  funcGons,	  path	  
– Can approximate any function arbitrarily closely
to	  leaf	  gives	  truth	  table	  row	  
•

Cylinders	  
Trivially, there is a consistent
decision tree for any training set
w/ one path to leaf for each example (unless f nondeterministic in
Could	  require	  exponenGally	  
but it probably won’t
3	   generalize
4	  
5	   6to
	   new8	   examples

many	  nodes	  

good
bad bad
Horsepower	  
Need some kind
of Maker	  
regularization
to ensure
more compact decisio

america	   asia	   europe	  
bad

good

good

low	   med	   high	  
bad

good

cyl=3 ∨ (cyl=4 ∧ (maker=asia ∨ maker=europe)) ∨ …

bad

CS194-10 Fall 2

Learning	  simplest	  decision	  tree	  is	  NP-­‐hard	  
• Learning	  the	  simplest	  (smallest)	  decision	  tree	  is	  
an	  NP-­‐complete	  problem	  [Hyaﬁl	  &	  Rivest	  ’76]	  	  
• Resort	  to	  a	  greedy	  heurisGc:	  
– Start	  from	  empty	  decision	  tree	  
– Split	  on	  next	  best	  a1ribute	  (feature)	  
– Recurse	  

Spli^ng:	  choosing	  a	  good	  aIribute	  
Would we prefer to split on X1 or X2?

t
Y=t : 4
Y=f : 0

X1

f

Y=t : 1
Y=f : 3

t
Y=t : 3
Y=f : 1

X2

f

Y=t : 2
Y=f : 2

Idea: use counts at leaves to define
probability distributions, so we can
measure uncertainty!

X1
T
T
T
T
F
F
F
F

X2
T
F
T
F
T
F
T
F

Y
T
T
T
T
T
F
F
F

Entropy	  
Entropy	  H(Y)	  of	  a	  random	  variable	  Y

Entropy	  of	  a	  coin	  ﬂip	  

Information Theory interpretation:
H(Y) is the expected number of bits
needed to encode a randomly
drawn value of Y (under most
efficient code)

Entropy	  

More uncertainty, more entropy!

Probability	  of	  heads	  

High,	  Low	  Entropy	  
• “High	  Entropy”	  	  
– Y	  is	  from	  a	  uniform	  like	  distribuGon	  
– Flat	  histogram	  
– Values	  sampled	  from	  it	  are	  less	  predictable	  

• “Low	  Entropy”	  	  
– Y	  is	  from	  a	  varied	  (peaks	  and	  valleys)	  
distribuGon	  
– Histogram	  has	  many	  lows	  and	  highs	  
– Values	  sampled	  from	  it	  are	  more	  predictable	  
(Slide from Vibhav Gogate)

Entropy	  of	  a	  coin	  ﬂip	  

Entropy	  

Entropy	  Example	  

Probability	  of	  heads	  

P(Y=t) = 5/6
P(Y=f) = 1/6
H(Y) = - 5/6 log2 5/6 - 1/6 log2 1/6
= 0.65

X1
T
T

X2
T
F

Y
T
T

T
T

T
F

T
T

F
F

T
F

T
F

CondiGonal	  Entropy	  
CondiGonal	  Entropy	  H( Y |X)	  of	  a	  random	  variable	  Y	  condiGoned	  on	  a	  
random	  variable	  X

Example:
P(X1=t) = 4/6
P(X1=f) = 2/6

t
Y=t : 4
Y=f : 0

X1

f

Y=t : 1
Y=f : 1

H(Y|X1) = - 4/6 (1 log2 1 + 0 log2 0)
- 2/6 (1/2 log2 1/2 + 1/2 log2 1/2)
= 2/6

X1
T
T
T
T
F
F

X2
T
F
T
F
T
F

Y
T
T
T
T
T
F

InformaGon	  gain	  
• Decrease	  in	  entropy	  (uncertainty)	  a_er	  spli^ng	  

In our running example:
IG(X1) = H(Y) – H(Y|X1)
= 0.65 – 0.33
IG(X1) > 0 ! we prefer the split!

X1
T
T
T
T

X2
T
F
T
F

Y
T
T
T
T

F
F

T
F

T
F

Learning	  decision	  trees	  
• Start	  from	  empty	  decision	  tree	  
• Split	  on	  next	  best	  a1ribute	  (feature)	  
– Use,	  for	  example,	  informaGon	  gain	  to	  select	  
aIribute:	  

• Recurse	  

When	  to	  stop?	  

First split looks good! But, when do we stop?

Base Case
One

Don’t split a
node if all
matching
records have
the same
output value

Base Case
Two

Don’t split a
node if data
points are
identical on
remaining
attributes




-------------------------------------------
FILE: L17_compressed.pdf
-------------------------------------------
STAT 22000 Lecture Slides
Hypothesis Testing About Population Means

Yibi Huang
Department of Statistics
University of Chicago

Outline

• Hypothesis Testing About Population Means (Section 4.3)
• Relationships Between Confidence Intervals and Hypothesis
Tests (Section 4.3.2)
• Common Misunderstandings About Hypothesis Testing (Not in
the textbook)

1

Hypothesis Tests about Population
Means

Example: Number of College Applications

To know how many colleges students applied to, the dean of a certain university took a random sample of size 106 from their newly
admitted students. This sample yielded an average of 9.7 college
applications with a standard deviation of 7. College Board website
states that counselors recommend students apply to roughly 8 colleges. Do these data provide convincing evidence that the average
number of colleges all freshmen in this university apply to is higher
than recommended?

http:// www.collegeboard.com/ student/ apply/ the-application/ 151680.html

2

Example: Number of College Applications – Hypotheses
• Population: all freshmen in this university
• The parameter of interest µ is the average number of schools
applied to by all freshmen in this university
• There are two explanations why the sample mean is higher
than the recommended 8 schools.
• The true population mean is different.
• The true population mean is 8, and the difference between the
true population mean and the sample mean is simply due to
natural sampling variability.

• H0 : µ = 8 (the average number of colleges freshmen in this
university have applied to is 8, as recommended)
• HA : µ > 8 (the average number of colleges freshmen in this
university have applied to is > 8)
3

Wrong Ways to State H0 and HA
H0 and HA are ALWAYS stated in terms of population parameters,
not sample statistics
Neither
H0 : x̄ = 8,

HA : x̄ > 8

or
H0 : average number of colleges applied in the sample is 8
HA : average number of colleges applied in the sample is 9.7
is correct. The correct statements should be
H0 : µ = 8,

HA : µ > 8

Also please clearly specify what is µ.
e.g., µ is the average number of colleges freshmen in this
university have applied to.
4

Number of College Applications — Test Statistic
By CLT, under H0 : µ = 8, the sampling distribution of the sample
mean is
7

!

x̄ ∼ N µ = 8, SE = √
= 0.68
106

µ=8

x = 9.7

To gauge how unusual the observed sample mean x̄ = 9.7 is
relative to its the hypothesized sampling distribution above, the test
statistic we used is the z-statistic, which is the z-score of the
sample mean relative to the distribution above
z-statistic =

9.7 − 8
x̄ − µ0
x̄ − µ0
=
≈ 2.5
√ =
√
SE
σ/ n
7/ 106

5

Number of College Applications — P-Value
Recall p-value is the probability of observing data such that the
evidence for the HA is at least as strong as our current data set
(x̄ > 9.7), if in fact H0 : µ = 8 were true.

µ=8

x = 9.7

p-value = P (x̄ > 9.7 | µ = 8) = P (Z > 2.50) = 0.0062
• Since p-value is low (lower than 5%) we reject H0 .
• The data provide convincing evidence that freshmen in this
university have applied to more than 8 schools on average.
• The diff. between the null value of 8 schools and observed sample
mean of 9.7 schools is not due to chance or sampling variability.
6

Example: Number of College Applications – Conditions
As CLT is used in the hypothesis test above, we need to check the
same conditions as we construct confidence intervals for the
population mean.
• Observations must be independent
• Use your knowledge to judge if the data might be dependent

• The population distribution of the number of colleges students
apply to should not be extremely skewed.
• In the z-statistic =

x̄ − µ0

√ , if the unknown population SD σ is
σ/ n

replaced with the sample SD s, we need to further check that
• sample size cannot be too small (at least 30)
• no outliers & not too skewed ⇒ Check the histogram of data!
7

Two-Sided Hypothesis Test
If the dean wanted to know whether the data provide convincing
evidence that the average number of colleges applied is different
than the recommended 8 schools, the alternative hypothesis would
be different.
H0 : µ = 8
HA : µ , 8
In this case, a sample mean x̄ far below 8 would also be evidence
in favor of HA . Hence the p-value would be the two-tail probability
p-value

= 0.0062 × 2
= 0.0124

=
x = 6.3 µ = 8

x = 9.7

z = − 2.5

0

z = 2.5
8

Recap: Hypothesis Testing for a Population Mean
1. Set the hypotheses
• H0 : µ = µ0
• HA : µ < or > or , µ0
2. Check assumptions and conditions
• Independence
• Normality: nearly normal population or n ≥ 30, no extreme
skew – or use the t distribution (Section 5.1)

3. Calculate a test statistic and a p-value (draw a picture!)
Z=

x̄ − µ0
s
, where SE = √
SE
n

4. (Optional) Make a decision
• If p-value < α, reject H0
• If p-value > α, do not reject H0
9

Relationship Between Confidence
Intervals and Two-Sided
Hypothesis Tests

Confidence Intervals and Two-Sided Hypothesis Tests
For a two-sided test:
H0 : µ = µ0

versus HA : µ , µ0

the following are equivalent:
• p-value > α (and hence H0 : µ = µ0 is not rejected at level α )
• |z-statistic | = |(x̄ − µ0 )/SE | < z ∗ , where z ∗ is a value such that

α 2

−z*

1−α

α 2

z*

• µ0 is in the 100(1 − α)% confidence interval for µ
x̄ − z ∗ SE < µ0 < x̄ + z ∗ SE
10

Example
Suppose in a study,
• 90% CI for µ is (4.81, 11.39);
• 95% CI for µ: (4.18, 12.02);
• 99% CI for µ: (2.95, 13.25).
Then
• H0 : µ = 4 is rejected at 5% level but not at 1% level
(2-sided p-value is between 1% and 5%)

because 4 is in the 99% CI but not in the 95% CI
• H0 : µ = 4.5 is rejected at 10% level but not at 5% level
because 4.5 is in the 95% CI but not in the 90% CI
11

Common Misunderstandings
About Hypothesis Testing

In the lecture slide “General Framework of Hypothesis Testing”, we
have introduced a number of common misunderstanding about
hypothesis testing
• Rejecting H0 doesn’t means we are 100% that H0 is false. We
might make Type 1 errors. Setting a significance level just
guarantee we won’t make Type 1 error too often
• P-value is not P (H0 is true | data ) but it is P (data | H0 is true ).
We are going to talk about more common misunderstanding about
hypothesis testing here.

12

Failing to Reject H0 Does Not Prove H0 to Be True
Another mistake is to conclude from a high p-value that the H0 is
probably true
• We have said that if our p-value is low, then this is evidence
that the H0 is not true
• If our p-value is high, can we conclude that H0 is true?
• No, we could make a type 2 error when failing to reject H0
• Moreover, unlike type 1 error rate is controlled at a low
level, type 2 error rate is usually quite high. It is quite
often that H0 is not true but the data fail to reject it.
• When we fail to reject H0 , often it just means the data are not
able to distinguish between H0 and HA (because the data are
to noisy, etc)
13

Real Example
• As an example, the Women’s Health Initiative found that
low-fat diets reduce the risk of breast cancer with a p-value of
0.07
• The New York Times headline: “Study finds low-fat diets won’t
stop cancer”
• The lead editorial claimed that the trial represented “strong
evidence that the war against fats was mostly in vain” and
sounded “the death knell for the belief that reducing the
percentage of total fat in the diet is important for health”
• Failing to prove the effect of low-fat diets doesn’t prove that
low-fat diets have no effect
http:// www.nytimes.com/ 2006/ 02/ 07/ health/ study-finds-lowfat-diet-wont-stop-cancer-or-heart-disease.html

14

Don’t Take the 0.05 Significance Level Too Seriously

• A p-value of 0.049 and a p-value of 0.051 give nearly the
same strength of evidence against H0
• For example, in the highly publicized 2009 study involving a
vaccine that may protect against HIV infection, the two-sided
p-value is 0.08, and the one-sided p-value of is 0.04
• Much debate and controversy ensued, partially because the
two ways of analyzing the data produce p-values on either
side of 0.05
• Much of this debate and controversy is fairly pointless; both
p-values tell you essentially the same thing — that the vaccine
holds promise, but that the results are not yet conclusive

15

Hypothesis Testing Cannot Tell Us...

Hypothesis testing cannot tell us
• whether the design of a study is flawed
• whether the data is appropriately collected
So we cannot conclude from a small P-value about whether one
variable has a causal effect on another variable or whether the
conclusion can be generalized to a bigger population.
Garbage In → Garbage Out

16

Statistical Significance Does Not Mean Practical Importance
Another mistake is reading too much into the term “statistically
significant”
• Saying that results are statistically significant informs the
reader that the findings are unlikely to be due to chance alone
• However, it says nothing about the practical importance of the
finding.
• E.g., rejecting the H0 : µ1 = µ2 just tells us µ1 , µ2 , but not
how big and how important µ1 − µ2 is. It is possible that the
difference is too small to be relevant even if it is significant.
• Remedy: Attach a confidence interval for the parameter so
that people can decide whether the difference is big enough to
be relevant.
17

Example

A 95% CI for the average number of colleges freshmen in this
university have applied is
7
s
x ± 1.96 √ = 9.7 ± 1.96 √
≈ 9.7 ± 1.3 = (8.4, 11.0).
n
106
from which one can decide whether the difference from 8 is big
enough to be relevant.

18

Recap: Common Misunderstandings about Hypothesis Testing

• Rejecting H0 doesn’t mean we are 100% sure that H0 is false.
We might make Type 1 errors
• P-value is not the probability that the H0 is true
• Failing to reject H0 does not prove H0 to be true
• Don’t take the 0.05 significance level too seriously
• Hypothesis testing cannot tell us if data were collected
properly or if the design of a study was flawed
• Statistical significance does not mean practical importance

19




-------------------------------------------
FILE: Lecture10_compressed.pdf
-------------------------------------------
Clustering
Predictive Analytics

Predictive Analytics

Clustering

1/2

Acknowledgments

These slides draw upon and adapt selected materials by:
Fragkiskos D. Malliaros (CentraleSupélec, Université Paris–Saclay, France)

Predictive Analytics

Clustering

2/2

Unsupervised learning
(clustering)

5

Supervised vs. Unsupervised Learning
Supervised learning!

Unsupervised learning!

!
!

• We have labeled
examples!

• The data is unlabeled!

• Given those examples,
learn a model that can
generalize to unseen
examples!

• Given the data, learn a model
that identifies structure in the
data (and generalize to new
data)!

• Key tasks:!

• Key task:!

– Classification!
– Regression!

– Clustering!

6!

What is Cluster Analysis?
• Cluster: a collection of data objects!
– Similar (or related) to one another within the same group!
– Dissimilar (or unrelated) with objects in other groups!

• Cluster analysis (or clustering)!
– Finding similarities between data according to the
characteristics of the data and …!
– … grouping similar data objects into clusters!

• Typical applications!
– As a stand-alone tool to get further insights about the data!
– As a preprocessing step for other algorithms!
7!

Any Natural Grouping?

Clustering is subjective

Simpson’s family!
Slide by Eamonn Keogh, UCR!

School employees!

Females!

Males!
8!

What is a Good Clustering?
• Good clusters have:!
– High intra-cluster similarity: cohesive within clusters!
– Low inter-cluster similarity: distinctive between clusters!

• The quality of a clustering method depends on!
– The similarity measure used by the method!
– Its ability to discover some or all of the hidden patterns!
Recall the distance and similarity
measures covered in kNN classification!

9!

Goals of Clustering
l

Group objects that are similar into clusters: classes that are
unknown beforehand!

10!

Goals of Clustering
l

Group objects that are similar into clusters: classes that are
unknown beforehand!

11!

Applications of Clustering
l

Understand general characteristics of the data!

l

Visualize the data!

l

l

Infer some properties of a data point based on how it relates to
other data points!
Examples!
-

Find subtypes of diseases!

-

Visualize protein families!

-

Find categories among images!

-

Find patterns in financial transactions!

-

Detect communities in social networks!

-

Find users with similar interests (e.g., Netflix, Amazon)!
12!

Cluster Centroids and Medoids
l

Centroid: mean of the points in the cluster!

C!

l

Medoid: point in the cluster that is closest to the centroid!

13!

Cluster Evaluation
l

l

l

Clustering is unsupervised!
There is no ground truth. How do we evaluate the quality
of a clustering algorithm?!
Based on the shape of the clusters:!
-

l

Based on the stability of the clusters:!
-

l

Points within the same cluster should be nearby/similar and
points far from each other should belong to different clusters!

We should get the same results if we remove some data points,
add noise, etc.!

Based on domain knowledge!
-

The clusters should “make sense”!
14!

Major Clustering Approaches
• Partitioning approach!
– Construct various partitions and then evaluate them by some criterion,
e.g., minimizing the sum of square errors!
– Typical methods: k-means, k-medoids!

• Hierarchical approach!
– Create a hierarchical decomposition of the set of data (or objects)
using some criterion!
– Typical methods: Diana, Agnes, BIRCH, CHAMELEON!

• Density-based approach!
– Based on connectivity and density functions!
– Typical methods: DBSCAN, OPTICS, DenClue!

• Grid-based approach!
– Based on a multiple-level granularity structure!
– Typical methods: STING, WaveCluster, CLIQUE!

15!

Hierarchical vs. Partitional
Hierarchical!

Slide by Eamonn Keogh, UCR!

Partitional!

16!

Hierarchical clustering

17

Hierarchical Clustering
l

Group data over a variety of possible scales, in a multilevel hierarchy!

18!

Hierarchical Clustering (2/2)
All topics!

sports!

soccer!

tennis

!

fashion!

Gucci!

Lacoste !

webpages!

• A hierarchy might be more natural!
• Different users might care about different levels of granularity
or even prunings!
19!

Construction
l

l

Agglomerative approach (bottom-up)!
-

Start with each element in its own cluster!

-

Iteratively join neighboring clusters!

Divisive approach (top-down)!
-

Start with all elements in the same cluster!

-

Iteratively separate into smaller clusters!

20!

Dendrogram
l

The results of a hierarchical clustering algorithm are
presented in a dendogram!

21!

Dendrogram
l

The results of a hierarchical clustering algorithm are
presented in a dendogram!

How many clusters
do I have?!

22!

Dendrogram
l

The results of a hierarchical clustering algorithm are
presented in a dendogram!

1

2

3

4

23!

Hierarchical Clustering
l

l

Advantages!
-

No need to pre-define the number of clusters!

-

Interpretability!

Drawbacks!
-

Computational complexity!

-

Must decide at which level of the hierarchy to split!

-

Lack of robustness (unstable)!

24!

k-means clustering

25

k-means Algorithm – The Idea
Most well-known and popular clustering
algorithm:!
!
1. Start with some initial cluster centers!
!
2. Iterate:!
– Assign each example to closest center!
– Recalculate centers as the mean of the points in a cluster!

26!

k-means: An Example

27!

k-means: Initialize Centers Randomly

28!

k-means: Assign Points to Nearest Center

29!

k-means: Readjust Centers

30!

k-means: Assign Points to Nearest Center

31!

k-means: Readjust Centers

32!

k-means: Assign Points to Nearest Center

33!

k-means: Readjust Centers

34!

k-means: Assign Points to nearest Center

No changes: Done!

Test demo at: http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html!
35!

k-means Clustering – Objective Function
l

Minimize the intra-cluster variance!
l

Within-cluster sum of squares!
For a cluster C!

For all clusters!

l

What will this partition of the space look like?!

36!

k-means Clustering – Objective Function
l

Minimize the intra-cluster variance!
l

Within-cluster sum of squares!
For a cluster C!

For all clusters!

l

For each cluster, the points in that cluster are those that
are closest to its centroid than to any other centroid!

37!

k-means Clustering – Objective Function
l

Minimize the intra-cluster variance!
l

Within-cluster sum of squares!
For a cluster C!

For all clusters!

l

Voronoi tesselation!

l

Optimal solution: hard problem!
38!

Lloyd’s Algorithm for k-means
l

k-means cannot be easily optimized!

l

We adopt a greedy strategy (Lloyd’s Algorithm)!
-

Partition the data into k clusters at random!

-

Compute the centroid of each cluster!

-

Assign each point to the cluster of the centroid centroid!

-

Repeat until cluster membership converges!

39!

How to Select k?

Elbow rule!

40!

Summary of k-means
• Advantages!
– Computational time is linear:!

t: number of iterations!

– Easily implementable!

• Drawbacks!

compute kn distances!
in p dimensions!

– Need to select k (user-defined parameter)!
– Sensitivity to noise and outliers!
– Non-deterministic (stochastic)!
• Different solutions with each iteration!
– The clusters are forced to have “spherical” (convex) shapes!

41!

Example

Source: https://pafnuty.wordpress.com/!

42!

Improving k-means: k-means++
• The quality of the solution depends on the initialization!
• Rationale behind random initialization!
– Choosing a random assignment may lead the algorithm to a
good local minimum!

• Another approach: k-means++ [Arthurs and Vassilvitskii ‘07]!
1. Select a random point and declare it centroid c1!
2. For all remaining data points xj compute distance d(xj, c1)!
3. Select a random point with probability proportional to d(xj, c1)2
and set it as c2!
• This will give a point far enough from c1!

4. Repeat steps 2 and 3 for all centroids k!

43!

scikit-learn

http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html!

http://scikit-learn.org/stable/auto_examples/cluster/
plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py!

44!

scikit-learn

http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html!

http://scikit-learn.org/stable/auto_examples/cluster/
plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py!

44!

Spectral clustering

45

Clustering Structure in Graphs

How to discover the clustering structure?!
46!

Graph Representation: Adjacency Matrix
• A graph can be represented by the adjacency matrix A!

Node indexing

– Matrix of size n x n, where n=|V| is the number of nodes!
– Aij > 0, if i and j are connected!
– Aij = 0, if i and j are not connected!
– In case of unweighted graphs, Aij = 1, if (i, j) is an edge of the graph !
– Space proportional to n2!
Node indexing

Undirected graph!

1

2

3

4

5

6

7

8

1 0

1

1

0

0

0

0

0

2 1

0

1

1

1

0

0

0

3 1

1

0

0

1

0

1

1

4 0

1

0

0

1

0

0

0

5 0

1

1

1

0

1

0

0

6 0

0

0

0

1

0

0

0

7 0

0

1

0

0

0

0

1

8 0

0

1

0

0

0

1

0

Adjacency matrix!

47!

Graph Representation: Laplacian Matrix
• Let G = (V, E) be a graph. Then, the Laplacian matrix is defined as!

8
>
<ki if i = j
Lij =
1 if (i, j) 2 E
>
:
0
otherwise

• Diagonal degree matrix D, where Dii = ki (node degrees)!
4
1

L=D
3

2

2

3
6 1
A=6
4 1
1

1
2
1
0

1
1
2
0

3
1
07
7
05
1
48!

Clustering Non-Graph Data
•

Apply graph clustering algorithms on data with no inherent graph structure
(e.g., points in a d-dimensional Euclidean space)!

• How?!
1.

Construct a similarity graph based on the topological relationships
and distances between data points (e.g., kNN graph)!

2.

Then, the problem of clustering the set of data points is transformed
to a graph clustering problem !
kNN graph, k = 10

Data points
1

1

Similarity graph
(e.g., kNN)

0

0.5

0

−0.5

−1

−1

−1.5

−2

−2

−2.5

−3

−3
−1

0

1

2

[von Luxburg ‘07], [Shi and Malik ‘00], [Ng, Jordan, Weiss ’02]!

−3.5
−1

−0.5

0

0.5

1

1.5

2

49!

Graph Partitioning (1/2)
• Undirected graph G=(V, E)!

5

1
2

4

3

• Bi-partitioning task:!

6

– Divide nodes into two disjoint groups A, B!

A!

5

1
2

3

4

B
6

Questions:!
• How can we define a good partition of G ?!
• How can we efficiently identify such a partition?!
50!

Graph Partitioning (2/2)
• What makes a good partition?!
– Maximize the number of within-group connections!
– Minimize the number of between-group connections!

5

1
2
3

A

6

4

B
51!

Graph Cuts
• Express partitioning objectives as a function of the edge cut
of the partition!
• Cut: Set of edges across two groups:!

cut(A, B) =

X

wij

i2A,j2B
Two partitions, A and B

A
2

3

B

5

1
4

6

cut(A, B) = 2

52!

Graph Cut Criterion for Clustering
• Criterion: Minimum-cut!
– Minimize the weight of connections between groups!

arg minA,B cut(A, B)
• Degenerate case!

“Optimal” cut
Minimum cut

Problem!

• Not satisfactory partition – often isolated nodes!
• Does not consider internal cluster connectivity!
53!

Ratio Cut

Normalize cut by the size of the groups!

!

ratio-cut(A, B) =

cut(A, B) cut(A, B)
+
|A|
|B|
Size of A and B

A!
B!

Internal group
connectivity is not
taken into account!
54!

Normalized Cut
• Criterion: Normalized cut!
– Connectivity between groups relative to the density of each
group!

normalized-cut(A, B) =

cut(A, B) cut(A, B)
+
vol (A)
vol (B)

– Vol(A): total weighted degree of the nodes in A, i.e., Σi∈A ki!

• Why use this criterion?!
– It produces more balanced partitions!

• How do we efficiently find a good partition?!

A!
B!

– Computing the optimal cut is NP-hard!
[Shi and Malik ‘97]!

55!

Ratio Cut vs. Normalized Cut (1/2)
Red is Min-Cut!

ratio-cut(A, B) =

cut(A, B) cut(A, B)
+
|A|
|B|

normalized-cut(A, B) =

cut(A, B) cut(A, B)
+
vol (A)
vol (B)

Ratio-Cut(Red) = 1/1 + 1/8 = 1.125!
Ratio-Cut(Green) = 2/5 + 2/4 = 0.9!

Lower value is better

Normalized-Cut(Red) = 1/1 + 1/26 = 1.03 !
Normalized-Cut(Green) = 2/12 + 2/16 = 0.29 !
Normalized is even better
for Green due to density!

56!

Spectral Clustering vs. k-Means
• 2-dimensional points!
• Find k=3 clusters!

k-means!

spectral clustering!

74!

scikit-learn

http://scikit-learn.org/stable/modules/clustering.html#spectral-clustering!

http://scikit-learn.org/stable/auto_examples/cluster/
plot_cluster_comparison.html!

75!

Comparison (scikit-learn)

76!




-------------------------------------------
FILE: Lecture11_compressed.pdf
-------------------------------------------
Clustering
Predictive Analytics

Predictive Analytics

Clustering

1/2

Acknowledgments

These slides are based on materials by:
Prof. Bizer (Universität Mannheim)

Predictive Analytics

Clustering

2/2

Example Applications in which Co-Occurrence Matters
 We are often interested in co-occurrence relationships
 Marketing
1. identify items that are bought together by
sufficiently many customers
2. use this information for marketing or
supermarket shelf management purposes

 Inventory Management
1. identify parts that are often needed
together for repairs
2. use this information to equip your
repair vehicles with the right parts

 Usage Mining
1. identify words that frequently appear together
in search queries
2. use this information to offer auto-completion
features to the user
University of Mannheim – Prof. Bizer: Data Mining

Slide 2

Outline

1. Correlation Analysis
2. Association Analysis
1. Frequent Itemset Generation
2. Rule Generation
3. Handling Continuous and
Categorical Attributes
4. Interestingness Measures

University of Mannheim – Prof. Bizer: Data Mining

Slide 3

1. Correlation Analysis
 Correlation analysis measures the degree of dependency
between two variables
• Continuous variables: Pearson’s correlation coefficient (PCC)
• Binary variables: Phi coefficient

PCC ( x, y ) 

 x  x y  y 
 x  x   y  y 
i

i

2

i

2

i

f f  f 01 f10
Phi ( x, y )  11 00
f1 f 1 f 0  f  0

 Value range [-1,1]
• 1 : positive correlation
• 0 : variables independent
• -1 : negative correlation

University of Mannheim – Prof. Bizer: Data Mining

Slide 4

Correlations between Products in Shopping Baskets
P1

P2 P3 P4 P5

Basket 1

1

1

0

1

1

Basket 2

1

0

0

1

1

Basket 3

1

0

0

0

1

1 : always bought together
0 : sometimes bought together
-1 : never bought together

Shortcoming: Measures correlation only between two items but
not between multiple items, e.g. {ThinkPad, Cover}  {Minimaus}
University of Mannheim – Prof. Bizer: Data Mining

Slide 5

2. Association Analysis
 Association analysis can find multiple item
co-occurrence relationships (descriptive method)
 focuses on occurring items, not absent items
 first algorithms developed in the early 90s
at IBM by Agrawal & Srikant
 initially used for shopping basket analysis to find
how items purchased by customers are related
 later extended to more complex data structures
• sequential patterns
• subgraph patterns
 and other application domains
• web usage mining, social science, life science
University of Mannheim – Prof. Bizer: Data Mining

Slide 6

Association Analysis
Given a set of transactions, find rules that will
predict the occurrence of an item based on the
occurrences of other items in the transaction.
Shopping Transactions
TID Items
1

Bread, Milk

2

Bread, Diaper, Beer, Eggs

3

Milk, Diaper, Beer, Coke

4

Bread, Milk, Diaper, Beer

5

Bread, Milk, Diaper, Coke

University of Mannheim – Prof. Bizer: Data Mining

Examples of Association Rules
{Diaper}  {Beer}
{Beer, Bread}  {Milk}
{Milk, Bread}  {Eggs, Coke}

Implication means
co-occurrence,
not causality!

Slide 7

Definition: Support and Frequent Itemset
 Itemset
• collection of one or more items
• example: {Milk, Bread, Diaper}
• k-itemset: An itemset that contains k items

 Support count ()
• frequency of occurrence of an itemset
• e.g. ({Milk, Bread, Diaper}) = 2

 Support (s)

TID Items
1

Bread, Milk

2

Bread, Diaper, Beer, Eggs

3

Milk, Diaper, Beer, Coke

4

Bread, Milk, Diaper, Beer

5

Bread, Milk, Diaper, Coke

• fraction of transactions that
contain an itemset
• e.g. s({Milk, Bread, Diaper}) = 2/5 = 0.4

 Frequent Itemset
• an itemset whose support is greater than
or equal to a minimal support (minsup)
threshold specified by the user
University of Mannheim – Prof. Bizer: Data Mining

Slide 8

Definition: Association Rule
 Association Rule
 an implication expression of the form
X  Y, where X and Y are itemsets
 an association rule states that when X
occurs, Y occurs with certain probability.
 Example:
{Milk, Diaper}  {Beer}
Condition
Consequent

TID Items
1

Bread, Milk

2

Bread, Diaper, Beer, Eggs

3

Milk, Diaper, Beer, Coke

4

Bread, Milk, Diaper, Beer

5

Bread, Milk, Diaper, Coke

 Rule Evaluation Metrics
 Support (s)
fraction of transactions
that contain both X and Y
 Confidence (c)
measures how often items
in Y appear in transactions
that contain X
University of Mannheim – Prof. Bizer: Data Mining

s X  Y  =

X  Y 
T 

s

c X  Y  =

 X  Y 
 X 

c

 ( Milk , Diaper, Beer )
|T|



2
 0 .4
5

 ( Milk, Diaper, Beer ) 2
  0 .67
 ( Milk , Diaper )
3

Slide 9

Main Challenges concerning Association Analysis

1. Mining associations from large amounts of data can be
computationally expensive
•

algorithms need to apply smart pruning strategies

2. Algorithms often discover a large number of associations
•

many of them are uninteresting or redundant

•

the user needs to select the subset of the associations
that is relevant given her task at hand

University of Mannheim – Prof. Bizer: Data Mining

Slide 10

The Association Rule Mining Task
 Given a set of transactions T, the goal of association rule mining is
to find all rules having
1. support ≥ minsup threshold
2. confidence ≥ minconf threshold

 minsup and minconf are provided by the user.
 Brute Force Approach:
1. list all possible association rules
2. compute the support and confidence for each rule
3. remove rules that fail the minsup and minconf thresholds

 Computationally prohibitive due to large number of candidates!

University of Mannheim – Prof. Bizer: Data Mining

Slide 11

Mining Association Rules
TID Items

Example rules:

1

Bread, Milk

2

Bread, Diaper, Beer, Eggs

3

Milk, Diaper, Beer, Coke

4

Bread, Milk, Diaper, Beer

5

Bread, Milk, Diaper, Coke

{Milk, Diaper}  {Beer} (s=0.4, c=0.67)
{Milk, Beer}  {Diaper} (s=0.4, c=1.0)
{Diaper, Beer}  {Milk} (s=0.4, c=0.67)
{Beer}  {Milk, Diaper} (s=0.4, c=0.67)
{Diaper}  {Milk, Beer} (s=0.4, c=0.5)
{Milk}  {Diaper, Beer} (s=0.4, c=0.5)

Observations:
 All the above rules are binary partitions of the same itemset:
{Milk, Diaper, Beer}
 Rules originating from the same itemset have identical support
but can have different confidence.
 Thus, we may decouple the support and confidence requirements.
University of Mannheim – Prof. Bizer: Data Mining

Slide 12

Mining Association Rules


Two-step approach:
1. Frequent Itemset Generation
–

generate all itemsets whose support  minsup

2. Rule Generation
–



generate high confidence rules from each frequent itemset,
where each rule is a binary partitioning of a frequent itemset

Frequent itemset generation is still computationally expensive

University of Mannheim – Prof. Bizer: Data Mining

Slide 13

2.1 Frequent Itemset Generation
null

A

B

C

D

E

AB

AC

AD

AE

BC

BD

BE

CD

CE

DE

ABC

ABD

ABE

ACD

ACE

ADE

BCD

BCE

BDE

CDE

ABCD

ABCE

ABDE

ABCDE

University of Mannheim – Prof. Bizer: Data Mining

ACDE

BCDE

Given d items,
there are 2d
candidate
itemsets!
Slide 14

Brute Force Approach
 Each itemset in the lattice is a candidate frequent itemset
 Count the support of each candidate by scanning the database
 Match each transaction against every candidate

Transactions
TID
1
2
3
4
5

Items
Bread, Milk
Bread, Diaper, Beer, Eggs
Milk, Diaper, Beer, Coke
Bread, Milk, Diaper, Beer
Bread, Milk, Diaper, Coke

 Complexity ~ O(NMw)  Expensive since M = 2d !!!
 A smarter algorithm is required
University of Mannheim – Prof. Bizer: Data Mining

Slide 15

Example: Brute Force Approach
 Example:
• Amazon has 10 million books (i.e., Amazon Germany, as of 2011)

 That is 210.000.000 possible itemsets
 As a number:
• 9.04981... × 103.010.299
• that is: a number with 3 million digits!

 However:
• most itemsets will not be important at all, e.g., books on Chinese
calligraphy, Inuit cooking, and data mining bought together
• thus, smarter algorithms should be possible
• intuition for the algorithm: All itemsets containing Inuit cooking are likely
infrequent
University of Mannheim – Prof. Bizer: Data Mining

Slide 16

Reducing the Number of Candidates
 Apriori Principle

If an itemset is frequent, then all of its subsets
must also be frequent.
 Apriori principle holds due to the following property of the support
measure:

 X , Y : ( X  Y )  s ( X )  s (Y )
• support of an itemset never exceeds the support of its subsets
• this is known as the anti-monotone property of support

University of Mannheim – Prof. Bizer: Data Mining

Slide 17

Using the Apriori Principle for Pruning
If an itemset is infrequent, then all of its
supersets must also be infrequent

Found to be
Infrequent

Pruned
supersets
University of Mannheim – Prof. Bizer: Data Mining

Slide 18

Example: Using the Apriori Principle for Pruning
Minimum Support Count = 3
Item
Bread
Coke
Milk
Beer
Diaper
Eggs

Count
4
2
4
3
4
1

No need to generate
candidates involving
Coke or Eggs

Items (1-itemsets)
Itemset
{Bread,Milk}
{Bread,Beer}
{Bread,Diaper}
{Milk,Beer}
{Milk,Diaper}
{Beer,Diaper}

Count
3
2
3
2
3
3

Pairs (2-itemsets)

Triplets (3-itemsets)
Ite m s e t
{ B r e a d ,M ilk ,D ia p e r }

C ount
3

No need to generate
candidate {Milk, Diaper, Beer}
as count {Milk, Beer} = 2
University of Mannheim – Prof. Bizer: Data Mining

Slide 19

The Apriori Algorithm

1. Let k=1
2. Generate frequent itemsets of length 1
3. Repeat until no new frequent itemsets are identified
1. Generate length (k+1) candidate itemsets from length k
frequent itemsets
2. Prune candidate itemsets that can not be frequent because
they contain subsets of length k that are infrequent (Apriori
Principle)
3. Count the support of each candidate by scanning the DB
4. Eliminate candidates that are infrequent, leaving only those
that are frequent

University of Mannheim – Prof. Bizer: Data Mining

Slide 20

Example: Apriori Algorithm
itemset:count

minsup=0.5

1. scan T
 Cand1: {1}:2, {2}:3, {3}:3, {4}:1, {5}:3
 Fequ1: {1}:2, {2}:3, {3}:3,

{5}:3

 Cand2: {1,2}, {1,3}, {1,5}, {2,3}, {2,5}, {3,5}

Dataset T
TID

Items

T100

1, 3, 4

T200

2, 3, 5

T300

1, 2, 3, 5

T400

2, 5

2. scan T
 Cand2: {1,2}:1, {1,3}:2, {1,5}:1, {2,3}:2, {2,5}:3, {3,5}:2
 Fequ2:

{1,3}:2,

 Cand3:

{2,3}:2, {2,5}:3, {3,5}:2
{2, 3, 5}

3. scan T
 C3: {2, 3, 5}:2
 F3: {2, 3, 5}
University of Mannheim – Prof. Bizer: Data Mining

Slide 21

Frequent Itemset Generation in Rapidminer and Python
RapidMiner

Python

FP-Growth
Alternative frequent itemset generation algorithm which compresses data into
tree structure in memory. Details Tan/Steinbach/Kumar: Chapter 4.6
University of Mannheim – Prof. Bizer: Data Mining

Slide 22

Frequent Itemsets in Rapidminer

University of Mannheim – Prof. Bizer: Data Mining

Slide 23

Example Application of Frequent Itemsets
1. Take top-k frequent itemsets of size 2 containing item A
2. Rank second item according to
• profit made by selling item
• whether you want to reduce
number of items B in stock
• knowledge about customer preferences

3. Offer special price for combination with top-ranked second item

University of Mannheim – Prof. Bizer: Data Mining

Slide 24

2.2 Rule Generation
 Given a frequent itemset L, find all non-empty subsets f  L such
that f  L – f satisfies the minimum confidence requirement.

Example Frequent Itemset:

{Milk , Diaper, Beer }
Example Rule:

{Milk , Diaper }  Beer

c

TID Items
1

Bread, Milk

2

Bread, Diaper, Beer, Eggs

3

Milk, Diaper, Beer, Coke

4

Bread, Milk, Diaper, Beer

5

Bread, Milk, Diaper, Coke

 ( Milk, Diaper, Beer ) 2
  0.67
3
 ( Milk , Diaper )

University of Mannheim – Prof. Bizer: Data Mining

Slide 25

Challenge: Large Number of Candidate Rules
 If {A,B,C,D} is a frequent itemset, then the candidate rules are:
ABC D,
A BCD,
AB CD,
BD AC,

ABD C,
B ACD,
AC  BD,
CD AB

ACD B,
C ABD,
AD  BC,

BCD A,
D ABC
BC AD,

 If |L| = k, then there are 2k – 2 candidate association rules
(ignoring L   and   L)

University of Mannheim – Prof. Bizer: Data Mining

Slide 26

Rule Generation
 How to efficiently generate rules from frequent itemsets?
• In general, confidence does not have an anti-monotone property
c(ABC D) can be larger or smaller than c(AB D)

• But confidence of rules generated from the same itemset
has an anti-monotone property
• e.g., L = {A,B,C,D}:
c(ABC  D)  c(AB  CD)  c(A  BCD)
• Confidence is anti-monotone with respect to the number of
items on the right hand side of the rule

University of Mannheim – Prof. Bizer: Data Mining

Slide 27

Explanation
Confidence is anti-monotone w.r.t. number of items on the RHS of the
rule
– i.e., “moving elements from left to right” cannot increase
confidence

Reason:

c( AB→C ):=

s( ABC )
s ( AB)

c( A→BC):=

s ( ABC )
s( A)

– Due to anti-monotone property of support, we know
s(AB) ≤ s(A)
– Hence
c(AB → C) ≥ C(A → BC)

University of Mannheim – Prof. Bizer: Data Mining

Slide 28

Candidate Rule Pruning
Moving elements from left to right
cannot increase confidence

Low
Confidence
Rule

Pruned
Rule
Candidates
University of Mannheim – Prof. Bizer: Data Mining

Slide 29

Candidate Rule Generation within Apriori Algorithm
 Candidate rule is generated by merging two rules that share the
same prefix in the rule consequent (right hand side of rule)
1. join(CD  AB, BD  AC)
would produce the candidate
rule D  ABC
2. Prune rule D  ABC if one of its
parent rules does not have
high confidence (e.g. AD  BC)

CD=>AB

BD=>AC

D=>ABC

 All the required information for confidence computation has
already been recorded in itemset generation.
 Thus, there is no need to scan the transaction data T any more
University of Mannheim – Prof. Bizer: Data Mining

Slide 30

Creating Association Rules in Rapidminer and Python
RapidMiner

Python

University of Mannheim – Prof. Bizer: Data Mining

Slide 31

Exploring Association Rules in Rapidminer

Filter by
conclusion

Filter by
confidence

University of Mannheim – Prof. Bizer: Data Mining

Slide 32

2.3 Handling Continuous and Categorical Attributes
 How to apply association analysis to attributes that are not
asymmetric binary variables?
Session Country Session
Id
Length
(sec)
1
USA
982

Number of
Web Pages
viewed
8

Gender Browser
Type

Buy

Male

Chrome

No

2

China

811

10

Female

Chrome

No

3

USA

2125

45

Female

Firefox

Yes

4

Germany

596

4

Male

IE

Yes

5

Australia

123

9

Male

Firefox

No

…

…

…

…

…

…

…

10

 Example Rule:
{Number of Pages [5,10)  (Browser=Firefox)}  {Buy = No}
University of Mannheim – Prof. Bizer: Data Mining

Slide 33

Handling Categorical Attributes
 Transform categorical attribute into asymmetric binary variables
 Introduce a new “item” for each distinct attribute-value pair
• e.g. replace “Browser Type” attribute with
– attribute: “Browser Type = Chrome”
– attribute: “Browser Type = Firefox”
– …..

 Issues

2. What if distribution of attribute values is highly skewed?
- example: 95% of the visitors have Buy = No
- most of the items will be associated with (Buy=No) item
- potential solution: drop the highly frequent item

The Support Distribution of Pumsb Dataset
100
80
Support (%)

1. What if attribute has many possible values?
 many of the attribute values may have very low support
 potential solution: aggregate low-support attribute values

60
40
20
0
0

500

1000

1500

2000

Sorted Items

caviar

milk

champagne
University of Mannheim – Prof. Bizer: Data Mining

Slide 34

Handling Continuous Attributes
 Transform continuous attribute into binary variables
using discretization
• equal-width binning
• equal-frequency binning

 Issue: Size of the discretization intervals affects support & confidence
{Refund = No, (Income = $51,251)}  {Cheat = No}
{Refund = No, (60K  Income  80K)}  {Cheat = No}
{Refund = No, (0K  Income  1B)}  {Cheat = No}
• If intervals are too small
• itemsets may not have enough support
• If intervals too large
• rules may not have enough confidence
• e.g. combination of different age groups compared to a specific age group
University of Mannheim – Prof. Bizer: Data Mining

Slide 35

Attribute Transformation in RapidMiner and Python
 Categorical attribute values to binary attributes

 Continuous attribute values to binary attributes

University of Mannheim – Prof. Bizer: Data Mining

Slide 36

2.4 Interestingness Measures
 Association rule algorithms tend to produce too many rules
• many of them are uninteresting or redundant
• redundant if {A,B,C}  {D} and {A,B}  {D}
have same support & confidence

 Interestingness of patterns depends on application
•

one man's rubbish may be another's treasure

 Interestingness measures can be used to prune or
rank the derived rules.
 In the original formulation of association rules, support &
confidence were the only interestingness measures used.
 Later, various other measures have been proposed
• See Tan/Steinbach/Kumar, Chapter 6.7
• We will have a look at one: Lift
University of Mannheim – Prof. Bizer: Data Mining

Slide 37

Drawback of Confidence
Contingency table

Coffee

Coffee

Tea

15

5

20

Tea

75

5

80

90

10

100

Association Rule: Tea  Coffee
– confidence(Tea  Coffee) = 0.75
– but support(Coffee) = 0.9
– although confidence is high, rule is misleading as the fraction
of coffee drinkers is higher than the confidence of the rule
– we want confidence(X  Y) > support(Y)
– otherwise rule is misleading as X reduces probability of Y
University of Mannheim – Prof. Bizer: Data Mining

Slide 38

Lift
 The lift of an association rule X  Y is defined as:

Lift 

c(X  Y )
s (Y )

 Confidence normalized by support of consequent
 Interpretation
• if lift > 1, then X and Y are positively correlated

• if lift = 1, then X and Y are independent
• if lift < 1, then X and Y are negatively correlated

University of Mannheim – Prof. Bizer: Data Mining

Slide 39

Example: Lift
Contingency table

Coffee

Coffee

Tea

15

5

20

Tea

75

5

80

90

10

100

Lift 

c(X  Y )
s (Y )

Association Rule: Tea  Coffee
– confidence(Tea  Coffee) = 0.75
– but support(Coffee) = 0.9

Lift(Tea  Coffee) = 0.75/0.9= 0.8333
– lift < 1, therefore is negatively correlated

University of Mannheim – Prof. Bizer: Data Mining

Slide 40

Exploring Association Rules in RapidMiner

Lift
close
to 1

Solid
lift

University of Mannheim – Prof. Bizer: Data Mining

Slide 41

Conclusion
 The algorithm does the counting for you and
finds patterns in the data
 You need to do the interpretation based on your
knowledge about the application domain.
• Which patterns are meaningful?
• Which patterns are surprising?

University of Mannheim – Prof. Bizer: Data Mining

Slide 42

Literature for this Slideset

Pang-Ning Tan, Michael Steinbach, Anuj Karpatne,
Vipin Kumar: Introduction to Data Mining.
2nd Edition. Pearson.

Chapter 4: Association Analysis:
Basic Concepts and Algorithms
Chapter 7: Association Analysis:
Advanced Concepts

University of Mannheim – Prof. Bizer: Data Mining

Slide 43




-------------------------------------------
FILE: Lecture12_compressed.pdf
-------------------------------------------
Introduction to Time Series Analysis
Predictive Analytics

1 / 32

What We’ll Learn Today
1

What is Time Series Data?

2

Moving Averages

3

Exponential Smoothing

4

Holt’s Method (Trend)

5

Holt-Winters (Trend + Seasonality)

6

Choosing the Right Method

7

Practical Tips

2 / 32

What is Time Series Data?
Simple Definition
A time series is data collected over time, in order.
Examples:
Daily temperature readings
Monthly sales figures
Weekly website visitors
Quarterly company revenue
Yearly population counts

3 / 32

Example: Monthly Ice Cream Sales

Sales go up and down throughout the year
There seems to be a pattern that repeats
Overall, sales might be growing over time
4 / 32

The Building Blocks of Time Series

1. Trend
Long-term direction
Going up? Going down? Flat?

3. Noise (Random Fluctuations)
Unpredictable ups and downs
“Static” in the data

2. Seasonality
Regular, repeating patterns
Summer vs. winter

Task:
Separate signal from noise!

Weekday vs. weekend

5 / 32

Visualizing the Components

6 / 32

The Problem: Too Much Noise!

7 / 32

The Moving Average
The Idea
Instead of looking at each point alone, look at the average of nearby points.
3-Month Moving Average Example:
Month

Sales

Moving Average

January
February
March
April
May

100
120
110
130
125

–
(100 + 120 + 110) ÷ 3 = 110
(120 + 110 + 130) ÷ 3 = 120
(110 + 130 + 125) ÷ 3 = 122
–

8 / 32

Moving Average Formula
Simple Formula
Moving Average =

Sum of last n values
n

Example: 3-Month Moving Average
MAMarch =

Small window (3 months):

100 + 120 + 110
Jan + Feb + Mar
=
= 110
3
3
Large window (12 months):

More responsive

Very smooth

Still somewhat bumpy

Slower to react
9 / 32

Moving Average in Action

10 / 32

Removing Seasonality with Moving Averages
The Magic of 12-Month Moving Average
When you average exactly one full year of monthly data, the seasonal ups and downs cancel
out!
Why does this work?
High summer sales + Low winter sales = Average
Every month appears exactly once in each window
What’s left is the underlying trend

Rule of Thumb
Use a window size equal to your seasonal period:
Monthly data with yearly pattern → 12-month MA
Daily data with weekly pattern → 7-day MA
Quarterly data with yearly pattern → 4-quarter MA
11 / 32

12-Month Moving Average: Revealing the Trend

Notice: The 12-month MA shows the pure trend without seasonal bumps!
12 / 32

Limitation of Moving Averages

Problem with Simple MA:
All points weighted equally
January counts same as March
But shouldn’t recent data matter more?

Month

Weight

January
February
March

33%
33%
33%

Simple MA: Equal weights

Better Idea
Give more weight to recent observations, less weight to older ones.

13 / 32

Exponential Smoothing: The Concept
Key Idea
Weights decrease exponentially as data gets older.

Recent data = High weight, Old data = Low weight

14 / 32

Simple Exponential Smoothing Formula

The Formula
Forecast = α × (Actual) + (1 − α) × (Previous Forecast)
Where α (alpha) is between 0 and 1
High α (e.g., 0.8):

Low α (e.g., 0.2):

Trust new data more

Trust history more

React quickly to changes

Smooth, stable forecasts

More “jumpy” forecasts

Slow to react

15 / 32

Exponential Smoothing Example

Data: Sales = 100, 120, 115, 130

(using α = 0.3)

Step by step:
1

Start: Forecast1 = 100 (use first value)

2

Forecast2 = 0.3 × 100 + 0.7 × 100 = 100

3

Forecast3 = 0.3 × 120 + 0.7 × 100 = 36 + 70 = 106

4

Forecast4 = 0.3 × 115 + 0.7 × 106 = 34.5 + 74.2 = 108.7

5

Forecast5 = 0.3 × 130 + 0.7 × 108.7 = 39 + 76.1 = 115.1

16 / 32

Comparing Different Alpha Values

17 / 32

The Problem: Data with a Trend

18 / 32

Holt’s Method: Adding Trend
The Solution
Track two things separately:
1

The Level (where are we now?)

2

The Trend (how fast are we going up/down?)

Sales
Trend = slope
Level = current position

Time
19 / 32

Holt’s Method: The Formulas
Two Equations
1. Update the Level:
Level = α × (Actual) + (1 − α) × (Previous Level + Previous Trend)
2. Update the Trend:
Trend = β × (Level − Previous Level) + (1 − β) × (Previous Trend)
Two smoothing parameters:
α controls smoothing of the level
β controls smoothing of the trend
20 / 32

Holt’s Method: Making Forecasts
Forecast Formula
Forecasth steps ahead = Level + h × Trend
Example:
Current Level = 500
Current Trend = +10 per month
Forecasts:
Next month (h = 1): 500 + 1 × 10 = 510
In 2 months (h = 2): 500 + 2 × 10 = 520
In 6 months (h = 6): 500 + 6 × 10 = 560
21 / 32

Holt’s Method in Action

Notice: Holt’s method follows the trend instead of lagging behind!
22 / 32

The Final Challenge: Seasonality

Real business data often has:
An overall trend (growing or shrinking)
AND seasonal patterns (summer highs, winter lows)
23 / 32

Holt-Winters: The Complete Solution
Track Three Things
1

Level — Where are we on average?

2

Trend — Which direction are we heading?

3

Seasonality — What’s the pattern within each year?

Component

Smoothing Parameter

Level
Trend
Seasonality

α (alpha)
β (beta)
γ (gamma)

24 / 32

Two Types of Seasonality
Additive Seasonality

Multiplicative Seasonality

Seasonal swings are constant

Seasonal swings grow with level

“Summer is always +50 units”

“Summer is always +20%”

Forecast = Level + Trend + Season

Forecast = (Level + Trend) × Season

25 / 32

Understanding Seasonal Factors
Example: Ice Cream Sales with Multiplicative Seasonality
Month

Seasonal Factor

Meaning

January
February
March
April
May
June
July
August
...

0.6
0.7
0.9
1.0
1.1
1.3
1.5
1.4
...

40% below average
30% below average
10% below average
Average
10% above average
30% above average
50% above average
40% above average
...

If Level = 1000 and it’s July: Forecast = 1000 × 1.5 = 1500
26 / 32

Holt-Winters: The Formulas (Multiplicative)

Three Update Equations
Level:
Lt = α ×

Actualt
+ (1 − α) × (Lt−1 + Tt−1 )
St−m

Trend:
Tt = β × (Lt − Lt−1 ) + (1 − β) × Tt−1
Seasonal:
St = γ ×

Actualt
+ (1 − γ) × St−m
Lt

27 / 32

Holt-Winters in Action

Holt-Winters captures both the trend AND the seasonal pattern!
28 / 32

Holt-Winters Forecast Example

Notice: The forecast continues the trend while maintaining the seasonal pattern.
29 / 32

Summary: Method Comparison

Method
Moving Average
Simple Exp. Smoothing
Holt’s Method
Holt-Winters

Trend

Seasonality

Parameters

×
×
✓
✓

×
×
×
✓

Window size
α
α, β
α, β, γ

30 / 32

All Methods Compared

31 / 32

Practical Tips
1

Always plot your data first!
Look for trends, seasons, outliers

2

Start simple
Try moving average first
Add complexity only if needed

3

Use software to find best parameters
Excel, Python, R can optimize α, β, γ

4

Check your forecasts
Compare predictions to actual values
Calculate forecast errors

32 / 32




-------------------------------------------
FILE: Lecture2_compressed.pdf
-------------------------------------------
Hypothesis Testing
Predictive Analytics

The Tea Lady

Figure: The classic story of the lady who claimed she could tell if milk
was added before or after the tea.

▶ This thought experiment was introduced by Ronald A. Fisher
in his 1935 book, ”The Design of Experiments”.
▶ It illustrates the fundamental principles of hypothesis testing.

Experimental Setup

▶ The lady is presented with 8 cups of tea.
▶ 4 cups have milk added first.
▶ 4 cups have tea added first.
▶ The cups are presented in a random order.
▶ The lady’s task is to identify the 4 cups that had milk added
first.

Null and Alternative Hypotheses

▶ Null Hypothesis (H0 ): The lady has no ability to distinguish
between the two methods of preparation. Any correct
identification is due to random chance.
▶ Alternative Hypothesis (H1 ): The lady does have the ability
to distinguish between the two methods.

How many ways can she choose the cups?

The total number of ways to choose 4 cups from a set of 8 is given
by the combination formula:
 
n
n!
=
k!(n − k)!
k
In this case, n = 8 and k = 4:
 
8
8!
8×7×6×5
=
=
= 70
4
4!(8 − 4)!
4×3×2×1
There are 70 possible ways for the lady to choose the 4 cups.

What is the probability of her guessing correctly?

▶ If the null hypothesis is true (she is guessing), there is only
one way for her to choose all 4 cups correctly.
▶ The probability of this happening by chance is:
P(all correct) =

1
Number of ways to choose correctly
=
≈ 0.014
Total number of ways to choose
70

▶ This probability is the p-value. It’s the probability of
observing the data (or more extreme data) if the null
hypothesis is true.

What is the probability of her guessing exactly 3 correctly?
To get exactly 3 correct, she must choose 3 of the 4 correct cups
AND 1 of the 4 incorrect cups.
▶ Ways to choose 3 correct cups from 4:
 
4
4!
=4
=
3!1!
3
▶ Ways to choose 1 incorrect cup from 4:
 
4
4!
=
=4
1
1!3!
The total number of ways to get exactly 3 correct is 4 × 4 = 16.
The probability is:

P(3 correct) =

16
≈ 0.2286
70

What is Hypothesis Testing?

Hypothesis testing is a formal procedure for investigating our ideas
about the world using statistics. It is used to test a claim about a
population parameter using data from a sample.

The two competing hypotheses are:
▶ Null Hypothesis (H0 ): A statement of no effect or no
difference.
▶ Alternative Hypothesis (H1 ): The claim we are trying to
find evidence for.

Summary of Errors

Reality
Decision
Reject H0
Fail to Reject H0

H0 is True
Type I Error
Correct Decision

H0 is False
Correct Decision
Type II Error




-------------------------------------------
FILE: Lecture3_compressed.pdf
-------------------------------------------
Dimensionality reduction: PCA
Predictive Analytics

Acknowledgment

Materials were adapted from lectures by Fragkiskos D. Malliaros
(CentraleSupélec, Université Paris-Saclay).

High-Dimensional Data: Why Reduce?

▶ Real datasets often have thousands to millions of features.
▶ Text: one dimension per vocabulary term (e.g.,
bag-of-words/TF–IDF).
▶ Social networks: one dimension per user/connection.
▶ High dimensionality brings the curse of dimensionality:
▶ Data are extremely sparse; density notions become unreliable
⇒ density-based clustering degrades.
▶ Algorithmic complexity grows with dimensionality d ⇒
time/memory blow up.

Can we compress?

change axis

From 3D to 2D

Why reduce dimensions

▶ Aim: uncover the true dimension of the data.
▶ Real datasets are messy, but useful reduction is still possible.
▶ Working assumption: data = signal + noise; the signal is
well-approximated in a lower-dimensional subspace/manifold.
▶ Dimensionality reduction not only shrinks data size, it often reveals
structure—making the informative part more salient.

Why reduce dimensions

▶ Find hidden patterns: discover words or features that often go
together (e.g., in texts).
▶ Remove useless noise: throw away features that don’t help (not
every word is important).
▶ Easier to understand: results are simpler to explain.
▶ Better visuals: easier to draw and see patterns on plots.
▶ Less storage: fewer numbers to save.
▶ Faster computing: algorithms run quicker and use less memory.

Matrix setup for dimensionality reduction

▶ Data: m objects described by n numeric attributes.
▶ Represent as a matrix A ∈ Rm×n (rows = objects, columns =
attributes).
▶ Use linear algebra on A to analyze/transform the data.
▶ Goal: build a new matrix B ∈ Rm×k with k ≪ n such that:
▶ it preserves as much information from A as possible;
▶ it reveals structure (latent patterns, groups, topics) present
in A.

From n columns to k ≪ n columns



|

|

 a(1) a(2) · · ·
|
|

|





|

−→  b (1) · · ·
a(n) 
|
|
m×n

|



b (k) 
|
m×k

PCA: Introduction

▶ (Almost) like SVD: PCA is closely related to the singular value
decomposition.
▶ Goal: find a low-dimensional subspace so that projecting the data
loses as little information as possible.
▶ Principle: choose directions (principal components) that maximize
variance.
▶ Standardize features (zero-mean, unit-variance) when scales differ.

PCA




-------------------------------------------
FILE: Lecture4_0_compressed.pdf
-------------------------------------------
STAT 220 Lecture Slides
Least Square Regression Line

Yibi Huang
Department of Statistics
University of Chicago

Poverty vs. HS graduate rate
The scatterplot below shows the relationship between HS graduate
rate in all 50 US states and DC and the % of residents who live
below the poverty line (income below $23,050 for a family of 4 in 2012).
18

AR NM
MS
LA

16

DC

WV

% in poverty

TX

14

AL

TN
SC
NC
CA

OK
KY

NY
AZ

MT
FLGA

12

IL ME
OR

ND

HI WA
TD
OHMI
VT
MA
MO
KS
CO UT
PA
IN
VA WI
NV
IA
DE
NJ
CT
MD

RI

10

ID

8

NE
WY
AK
MN

6

NH

80

85

% HS graduates

90
1

Eyeballing the line
Which line appears to best fit the linear relationship between % in
poverty and % HS grad?
18
(a)
(b)
(c)
(d)

% in poverty

16
14
12
10
8
6
80

85
% HS grad

90
2

Residuals (Prediction Errors)
The residual (ei ) of the ith observation (xi , yi ) is

=

ei

−

yi
(Observed y )

(Predicted y )

• Residuals are the

14

(Residual)

Observed y
12

●

10

distances from data

●

●

points to model line, not

●
●

the shortest distances

6

●
●
●

4

• Points above/below the

2

model line have
positive/negative

●

0

y

(signed) vertical

Residual

8

Predicted y

ŷi

0

1

2

3

4
x

5

6

7

8

residuals.
3

Residuals (cont.)
18

DC

y

% in poverty

16
14

y^

5.44
−4.16

12

y

10

y^

RI

8
6
80

85
% HS grad

90

• % living in poverty in DC is 5.44% more than predicted.
• % living in poverty in RI is 4.16% less than predicted.

4

Criteria for Choosing the “Best” Line
We want a line y = a + bx having small residuals:
Option 1: Minimize the sum of absolute values of residuals

|e1 | + |e2 | + · · · + |en | =

X
i

|yi − b
yi | =

X
i

|yi − a − bxi |

• Difficult to compute. Nowadays possible by computer technology
• Giving less penalty to large residuals.
The line selected is less sensitive to outliers
Option 2: Minimize the sum of squared residuals – least square method
e12 + e22 + · · · + en2 =

X
i

(yi − b
yi )2 =

X
i

(yi − a − bxi )2

• Easier to compute by hand and using software
• Giving more penalty to large residuals.
A residual 2x as large as another is often more than 2x as bad.
The line selected is more sensitive to outliers

5

Equation of the Least-Square (LS) Regression Line

The least-square (LS) regression line is the line y = a + bx that
minimizes the sum of squared errors:

X
i

ei2 =

X
i

(yi − b
yi )2 =

X
i

(yi − a − bxi )2

The slope and intercept of the LS regression line can be shown by
math to be
b = slope = r ·

sy
sx

a = intercept = ȳ − slope · x̄

6

Properties of the LS Regression Line
y = intercept +slope · x

| {z }
=y −slope·x

⇔ y − y = slope · (x − x ) = r
⇔

b
y −y
sy

|{z}
z −score of b
y

=r·

sy
sx

(x − x )

x −x
sx

|{z}
z −score of x

• The LS regression line always passes through (x̄ , ȳ )
• As x goes up by 1 SD of x, the predicted value b
y only goes up
by r × (SD of y)
• When r = 0, the LS regression line is horizontal y = y, and
the predicted value b
y is always the mean y
7

Poverty vs. HS graduate rate
18

% in poverty

16

% HS grad % in poverty
(x )
(y )
mean x̄ = 86.01 ȳ = 11.35
sd
sx = 3.73
sy = 3.1
correlation r = −0.75

14
12
10
8
6
80

85
% HS grad

90

The slope and the intercept of the least square regression line is
sy
3.1
slope = r
= (−0.75) ×
= −0.62
sx
3.73
intercept = ȳ − slope · x̄ = 11.35 − (−0.62) × 86.01 = 64.68
So the equation of the least square regression line is

% in[
poverty = 64.68 − 0.62 (% HS grad )
8

Interpretation of Slope
The slope indicates how much the response changes associated
with a unit change in x on average (may NOT be causal, unless
the data are from an experiment).

% in[
poverty = 64.68 − 0.62 % HS grad
• For each additional % point in HS graduation rate, we would
expect the % living in poverty to be lower on average by
0.62% points.
• For states with 90% HS graduation rate, their living-in-poverty
rates are 6.2% lower on average than those states with 80%
HS graduation rate.
• If a state manages to bring up its HS graduation rate by 1%,
will its living-in-poverty rate lowers by 0.62%?
9

Interpretation of the Intercept

The intercept is the predicted value of response when x = 0, which
might not have a practical meaning when x = 0 is not a possible
value

% in[
poverty = 64.68 − 0.62(% HS grad )
• States with no HS graduates are expected on average to have
64.68% of residents living below the poverty line
• meaningless. There is no such state.

10

Prediction
• Using the linear model to predict the value of the response
variable for a given value of the explanatory variable is called
prediction, simply by plugging in the value of x in the linear
model equation.
• There will be some uncertainty associated with the predicted
value.
18

% in poverty

16
14
12
10
8
6
80

85
% HS grad

90

11

Extrapolation
• Applying a model estimate to values outside of the realm of
the original data is called extrapolation.
• Sometimes the intercept might be an extrapolation.

70

intercept

% in poverty

60
50
40
30
20
10
0
0

20

40
60
% HS grad

80

100
12

Examples of Extrapolation

13

Examples of Extrapolation

14

Properties of Residuals

Residuals for a least square regression line have the following
properties.
1. Residuals always sum to zero,

i =1 ei = 0.

Pn

• If the sum > 0, can you improve the prediction?
2. Residuals and the explanatory variable xi ’s have zero
correlation.
• If non-zero, the residuals can be predicted by xi ’s, not the
best prediction.
• Residuals are the part in the response that CANNOT be
explained or predicted linearly by the explanatory
variables.

15

Variances of Residuals and Predicted Values
Recall

=

yi

(observed)

b
yi

+

(predicted)

ei

(residual)

There is a nontrivial identity:
1 P
2
i ( yi − y )
n −1

(variance of y )

=

1 P
2
i (ŷi − y )
n −1

(variance of ŷ )
k

+

1 P 2
i ei
n −1

 variability of y that 

(variance of residuals)
k
 variability of y that 

can be explained by x

cannot be explained by x

16

R 2 = R-squared = r 2
Moreover, one can show that

P
2
Variance of predicted y’s
i (ŷi − y )
=
r = P
2
Variance of observed y’s
i ( yi − y )
2

That is,
R 2 = r 2 = the square of the correlation coefficient

= the proportion of variation in the response
explained by the explanatory variable
The remainder of the variability is explained by variables not
included in the model or by inherent randomness in the data.
2
i ei

P
2

1−r = P

2
i (yi − y )

=

Variance of Residuals
Variance of y
17

Interpretation of R-squared
For the model we’ve been working with,
R 2 = r 2 = (−0.75)2 ≈ 0.56, which means — 56% of the variability
in the % of residents living in poverty among the 51 states is
explained by the variable “% of HS grad”.
18

% in poverty

16
14
12
10
8
6
80

85
% HS grad

90
18

There Are Two LS Regression Lines (1)

Recall the LS regression line for predicting poverty rate from HS
graduation rate is

% in[
poverty = 64.68 − 0.62 (% HS grad )
For a state with 80% HS graduation rate, the predicted poverty rate
is

% in[
poverty = 64.68 − 0.62 × 80 = 15.08%
For another state with 15.08% of residents living in poverty, will the
predicted HS graduation rate to be 80%?

19

There Are Two LS Regression Lines (2)
Residuals for predicting x from y are the horizontal distances from
18

18

16

16
% in poverty

% in poverty

data points to the line.

14
12
10

14
12
10
8

8

6

6
75

80

85
90
% HS grad

95

75

80

85
90
% HS grad

95

Red line predicts % in poverty from % HS grad.
Blue line predicts % HS grad from % in poverty.
20

There Are Two LS Regression Lines (3)

The LS regression line that predicts x from y minimizes

P

(horizontal distances from points to the line)2 .

The LS regression line that predicts y from x minimizes

P

(vertical distances from points to the line)2 .

The two lines are different.
The LS regression line that predicts y from x is only for predicting
y from x, not for predicting x from y .
Recall that correlation does not distinguish between x and y.
For regression, the two variables play different roles and are not
interchangeable.

21

There Are Two LS Regression Lines (4)

mean
sd

% HS grad
(y)
86.01
3.73
correlation

% in poverty
(x)
11.35
3.1
r = −0.75

Find the equation for the LS regression line that predicts % HS
grad from % in poverty.
• What is x ? What is y ? x = % in poverty, y = % HS grad
• slope = r sy /sx = −0.75 × 3.73/3.1 = −0.90
• intercept = ȳ − (slope) · x̄ = 86.01 − (−0.90) × 11.35 = 96.2

[
• equation: % HS
grad = 96.2 − 0.90 (% in poverty )
For another state with 15.08% of residents living in poverty, the
predicted HS graduation rate is 96.2 − 0.90 × 15.08 = 82.628%,
not 80%.

22

Summary
• The residual of a data point is
• the diff. between the observed y and the predicted y,
• the signed vertical distance (not the shortest distance) from
the data point to model line.

• The least-square (LS) regression line is the line y = a + bx
that minimizes the sum of squared errors:

X
i

(yi − b
yi )2 =

X
i

(yi − a − bxi )2

of which the slope and intercept are
sy
b = slope = r · , a = intercept = ȳ − slope · x̄
sx
• Statistical interpretation of
• the slope is the average change in y, associated with a unit
increase in x.
• the intercept is predicted value of y when x = 0 (only
meaningful if 0 is a possible value of x)

23

Summary (Cont’d)

• Extrapolation is usually unreliable
• For LS regression, residuals add up to zero, and have 0
correlation with the explanatory variable x .
• R 2 = R-squared = r 2 = proportion of variation in the
response y that can be explained by the explanatory variable
• Regression treats x and y differently.
The LS regression line that predicts y from x is NOT
the LS regression line that predicts x from y .
The LS regression line that predicts y from x can only predict
y from x, not x from y .

24




-------------------------------------------
FILE: Lecture4_2_compressed.pdf
-------------------------------------------
Linear Regression
Yerlan Kuzbakov

Yerlan Kuzbakov

Linear Regression

1 / 17

Framing the question and economic model

Goal: Define a clear question of interest and formulate an economic model.
Example (Mincer’s Model): wage = f (educ, expe).
Variables:
wage: hourly wage
educ: years of education
expe: years of experience

Yerlan Kuzbakov

Linear Regression

2 / 17

Econometric model (Mincer’s equation)

Specification:
wagei = β1 + β2 · educi + β3 · expei + ui
Stochastic term: ui captures unobserved factors with assumptions described
later.

Yerlan Kuzbakov

Linear Regression

3 / 17

Cross-sectional data basics

Definition: A sample of individuals, households, firms, cities, countries, etc.,
taken at a given point in time.
Use case: Widely used in applied microeconomics.
Sampling: Often obtained by random sampling from the population of
interest.
Random sample: {x1 , . . . , xN } is random if the N observations are drawn
independently from the same population (i.i.d.).

Yerlan Kuzbakov

Linear Regression

4 / 17

Example cross-sectional data (Wooldridge, USA 1976)

obsno

wage

educ

expe

female

married

1
2
3
4
5

3.10
3.24
3.00
6.00
5.30

11
12
11
8
12

2
22
2
44
7

1
1
0
0
0

0
1
0
1
1

525
526

11.56
3.50

16
14

5
5

0
1

1
0

···

Yerlan Kuzbakov

Linear Regression

5 / 17

Causal effects and identification

Economist’s goal: Infer the causal effect of one variable on another, holding
other relevant variables constant.
Example (agriculture): Fertilizer’s effect on crop yield; other factors
(rainfall, land quality) matter, but randomized experiments can be run.
Example (returns to education): Experience and ability affect earnings;
true randomized experiments are typically infeasible.
Implication: Observational data require careful modeling and assumptions
for causal inference.

Yerlan Kuzbakov

Linear Regression

6 / 17

Simple linear regression model and data-generating process

Sample: {(yi , xi ) | i = 1, . . . , N}.
Model:
yi = β1 + β2 xi + ui ,

with E(ui | x1 , . . . , xN ) = 0.

Interpretation:
β1 : intercept, β2 : slope (effect of x on y ).
yi : dependent variable (regressand), xi : independent variable (regressor).
ui : error term (disturbance).

Yerlan Kuzbakov

Linear Regression

7 / 17

Implications of exogeneity assumptions

Unconditional mean: E(ui ) = 0.
Orthogonality: E(xi ui ) = 0.
Uncorrelatedness: cov(xi , ui ) = 0.
Conditional mean of y :
E(yi | xi ) = β1 + β2 xi ,
which is the population regression function.

Yerlan Kuzbakov

Linear Regression

8 / 17

Expected causal effect

Marginal effect of x on y :
β2 =

Yerlan Kuzbakov

∂E(y | x)
.
∂x

Linear Regression

9 / 17

Mean shift and inclusion of a constant

If the regressors include a constant, we can absorb a nonzero mean in ui :
yi = β1 + β2 xi + ui ,

E(ui | x1 , . . . , xN ) = µ.

Reparameterization:
yi = (β1 + µ) + β2 xi + (ui − µ)
so we can write yi = β1 + β2 xi + ui with E(ui | x1 , . . . , xN ) = 0.

Yerlan Kuzbakov

Linear Regression

10 / 17

Analogy principle and moment conditions

Population moment conditions:
E(ui ) = 0,

E(xi ui ) = 0.

Using ui = yi − β1 − β2 xi :
E(yi − β1 − β2 xi ) = 0,



E xi (yi − β1 − β2 xi ) = 0.

Sample counterparts (normal equations):
N

1 X
(yi − β̂1 − β̂2 xi ) = 0,
N
i=1

Yerlan Kuzbakov

N

1 X
xi (yi − β̂1 − β̂2 xi ) = 0.
N

Linear Regression

i=1

11 / 17

OLS estimators from normal equations

Means: ȳ = N1

PN

1
i=1 yi , x̄ = N

PN

i=1 xi .

First normal equation: ȳ = β̂1 + β̂2 x̄, hence
β̂1 = ȳ − β̂2 x̄.
If

PN

2
i=1 (xi − x̄) ̸= 0, then

PN
β̂2 =

Yerlan Kuzbakov

i=1 (xi − x̄)(yi − ȳ )
,
PN
2
i=1 (xi − x̄)

Linear Regression

β̂1 = ȳ − β̂2 x̄.

12 / 17

Fitted values, residuals, and sample regression function

Fitted values: ŷi = β̂1 + β̂2 xi .
Residuals: ûi = yi − ŷi = yi − β̂1 − β̂2 xi .
Sample regression function: y = β̂1 + β̂2 x.

Yerlan Kuzbakov

Linear Regression

13 / 17

OLS as least squares minimization

Residual for hypothetical (b1 , b2 ): ri = yi − b1 − b2 xi .
Objective (SSR):
SSR(b1 , b2 ) =

N
X

2

(yi − b1 − b2 xi ) .

i=1

OLS estimate:
(β̂1 , β̂2 ) = arg min SSR(b1 , b2 ).
(b1 ,b2 )

Squaring imposes heavier penalties on large residuals.

Yerlan Kuzbakov

Linear Regression

14 / 17

First-order conditions for OLS


β̂
−
β̂
x
y
−
1
2 i = 0.
i
i=1


PN
FOC 2:
i=1 xi yi − β̂1 − β̂2 xi = 0.
FOC 1:

PN 

Yerlan Kuzbakov

Linear Regression

15 / 17

Key properties of OLS

PN

i=1 ûi = 0 (by FOC 1).
PN
Orthogonality in sample: i=1 xi ûi = 0 (by FOC 2).

Zero-sum residuals:

Regression through the means: ȳ = β̂1 + β̂2 x̄ (by FOC 1).

Yerlan Kuzbakov

Linear Regression

16 / 17

Summary

Economic models motivate econometric specifications.
Cross-sectional data and exogeneity yield population moments.
OLS solves sample analogs of population conditions via least squares.
Resulting estimates have intuitive properties and a clear causal interpretation
under assumptions.

Yerlan Kuzbakov

Linear Regression

17 / 17




-------------------------------------------
FILE: Lecture4_compressed.pdf
-------------------------------------------
STAT 220 Lecture Slides
Inference for Linear Regression

Yibi Huang
Department of Statistics
University of Chicago

Simple Linear Regression Models

Example: Pearson’s Father-and-Son Data

78

Father-son pairs are grouped by father’s height, to the nearest inch.
●

●
●

76

How do the

●

●

●

• mean of son’s

●

●

●
●

●

●

●

●
●
●●

●
●
●
● ●

●

72
70
68
66

●

64

●●

62

● ●
●
●

●

●

●
●

●

●●
●

●

●

●

●

●

●

●

●

●●

height (SH),

●

●

●
●
●
●
●
●
●
●
● ●●
●
●
●●
● ● ●●
●
●
●
●
●
● ● ● ●●
●●
●●
●
●
●
●
●
●
●
●
● ● ● ●
●
●
●
●
● ●
●
●● ●
● ●
●
●
●
● ●
●
● ●
● ●
● ● ● ●
●
●
●
●
●
●
●
●
●
●
●
● ●● ● ●
● ● ● ●●
● ● ● ●
● ●
●
●
●
●
●
●
●
●● ● ● ● ●
●
● ●● ●●●●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
● ●
●
●
● ●
● ●
●●
●●
● ●
●
● ●
● ●
●
● ●●
●
● ● ●
●
●
●●● ●
● ●● ● ●
●● ●
●
●
●
●●
●
●●
●
●●●
●
●
● ●
●
●● ●
●● ● ●
●
●●
●
●
●
●
●●
●
●●
●
●
●
● ● ● ● ●
●●
●
●
●
●●
● ●●●
●
●● ●
●
●●● ● ● ● ●●● ● ●
●
●
●
●● ●
● ●●
●●
●●
●
●
●
●●
● ●
●
●● ●
●●
● ●
●●
●
● ●●
●
● ● ● ●
●
●
●●
●●
●●
●
●
● ●
●●
●●●
●
●
●
● ● ● ● ●● ●●
● ●●
●●
●
● ● ●●
●
●
●
●
●
●
●●
●
●● ● ● ● ●
●
●●● ● ●
●
●
●
●
● ●●
● ●
●●
●
●● ●
●●
●
●
●
●● ● ●
● ●
●
●
● ●● ●
●
●
●●
●
●
●
●
●
● ●●
● ●
●
●
● ●
●
●
●
●
●
●
●
●
● ●
● ●
●
●
●
● ●●
● ● ●● ● ●
●
●
● ● ●●● ●
● ●
●●●
● ●●
●
● ●
●●
● ●●
●
●
●
● ● ●
●
●●
● ●● ●●● ●
● ●●
●●●
●●
●
●●
●
●
● ●
● ● ●
●●
● ●
●●
●
●●
●● ●
●
●
●
●
● ● ● ●●●●
● ●
●●
● ●● ● ●
●
●● ●
●
●
●
●
●
● ●
●
●● ●
●●
● ● ●● ●●
● ●●
●
●
● ● ●●●●● ● ●
●
●
● ● ● ●●
●
●●●
● ●●
●●
●
● ●● ● ● ●
● ● ●● ●●
● ●
● ●
●
●●● ● ●
●
●●
●
●●
●
●
●
● ●●
●
●
●
●
●
●
●●
●●●
● ●
● ●
●●
●
●● ●
●
●● ●
● ● ● ● ●●
●● ●
●●
●
●●
●
●
● ●
●
●
●●
●
● ● ● ●● ●● ●●
●●
●
●
● ●
●
● ●
●
●● ●● ● ●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
● ●
● ●
●
●
●● ●
●●●
● ●
●
● ●
● ●
●
● ●● ● ●
●
●
● ●●●
●
● ●●● ●
●
● ●
●●
●
●
● ●●●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
● ●
● ●
●
● ●
●●
●
● ● ●●
● ● ● ●● ● ●●●
●
●
●
● ● ● ●●
●
● ● ●
●
●
● ●● ●
●
●●●
●
● ●
●
●
●
●
●
●
●● ●
●
●●
●
●
●
●●
●
●
●
●
●
●●
● ●
●
●
● ●
●●
●
● ●
● ● ●
●
●
● ● ●●
●
●
●
● ●
●
●
●●
●
●
●
●
●
●
●
●
● ●● ● ●
●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
● ●
●● ●
●
●
●
●
●
●
●
●
●
●
●
● ●●●
●
●
●
●
● ●
●
●
● ● ●
●
●
●●
●
●
●
● ●
●
● ●
●
●
●
●
●
●
● ●
●
● ●
● ●● ●
●
●
●
●
●
●
●●
●
●
● ●
●
●
●
●●
●●
●
●
●●
●
●
●
● ●
● ● ●

●

●

●
●

●

●

● ●
●
●
●● ●
●

●

• SD of SH, and

●

●

●

●
●

●

• distribution of SH

●
●

(histogram of SH)?
within each group
change with father’s
height (FH)?

●

●
●

●

●

60

Son's height (inches)

74

●
●

60

62

64

66

68

70

Father's height (inches)

72

74

76

1

25
15
0 5

Percent per inch

64−inch fathers

60

62

64

66

68

70

72

74

76

78

80

25
15

67−inch fathers

0 5

Percent per inch

Son's Height (inches)

60

62

64

66

68

70

72

74

76

78

80

25
15

70−inch fathers

0 5

Percent per inch

Son's Height (inches)

60

62

64

66

68

70

72

74

Son's Height (inches)

76

78

80

2

Simple Linear Regression Model
Pearson’s father-and-son data inspire the following assumptions for the
simple linear regression (SLR) model:
1. The means of Y is a linear function of X , i.e.,
E (Y | X = x ) = β0 + β1 x
2. The SD of Y does not change with x, i.e.,
SD (Y | X = x ) = σ for every x
3. (Optional) Within each subpopulation, the distribution of Y is
normal.

3

Simple Linear Regression Model

Equivalently, the SLR model asserts the values of X and Y for
individuals in a population are related as follows
Y = β0 + β1 X + ε,
• the value of ε, called the error or the noise, varies from
observation to observation, follows a normal distribution

ε ∼ N (0, σ)
In the model, the line y = β0 + β1 x is called the population
regression line.

4

Inference for Simple Linear
Regression Models

Data for a Simple Linear Regression Model
Suppose we have a SRS of n individuals from a population.
From individual i we observe the response yi and the explanatory
variable xi :

(x1 , y1 ), (x2 , y2 ), (x3 , y3 ), . . . , (xn , yn )
The SLR model states that
yi = β0 + β1 xi + εi
Recall in the previous lecture, the least square line of the data
above is
y = b0 + b1 x
in which
b1 = r

sy
sx

P
=

i (xi − x )(yi − y )
,
P
2
i (xi − x )

b0 = y − b1 x

We can use b1 to estimate β1 and b0 to estimate β0 .
5

Caution: Sample v.s. Population
Note the population regression line
y = β0 + β1 x
is different from the least square regression line
y = b0 + b1 x
we learned in the previous lecture.
• The latter is merely the least square line for a sample, while
the former is the least square line for the entire population.
• The values of b0 and b1 will change from sample to sample.
b1 = r

sy
sx

P
=

i (xi − x )(yi − y )
,
P
2
i (xi − x )

b0 = y − b1 x

• We are interested in the population intercept β0 and slope β1 ,
NOT the sample counterparts b0 and b1 .
6

How Close Is b1 to β1 ?
Recall the slope of the least square line is

P
b1 =

i (xi − x )(yi − y )
P
2
i ( xi − x )

Under the SLR model: yi = β0 + β1 xi + εi , replacing yi in the
formula above by β0 + β1 xi + εi , we can show after some algebra
that

P
i (xi − x )εi
b1 = β1 + P
2
i (xi − x )

From the above, one can get the mean, the SD, and the sampling
distribution of b1 .
• E (b1 ) = β1 . . . . . . . . . . . . . . . . (b1 is an unbiased estimate of β1 )
• SD(b1 ) = ? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (See the next slide)
7

Variability of b1
One can show that
SD (b1 ) = pP

σ
(xi − x )2

σ
√

=
sx

n−1

,

qP
where sx =

(xi −x )2
is the sample SD of xi ’s.
n −1

How to reduce the SD of b1 (and make b1 closer to β1 ):
• increase the sample size n
• increase the range of xi ’s (and hence sx is increased)

But σ is unknown, we need to estimate it.
8

Estimate of σ
We want to estimate σ, SD of the error εi .
• An intuitive estimate of σ is the sample SD of the errors εi

rP
(εi − ε)2
b
σ=
n−1

where

εi = yi − β0 − β1 xi

However, this is not possible β0 and β1 are unknown.
• We can estimate β0 and β1 with b0 and b1 and approximate
the errors εi with the residuals
ei = yi − (b0 + b1 xi ) = yi − b
yi
We use the “sample SD” of the residuals ei to estimate σ:

s
P
se =

ei2

n−2
9

Estimate of σ

We use the “sample SD” of the residuals ei to estimate σ:

rP
se =

s
(ei − e )2
=
n−2

P

ei2

n−2

• Recall that the mean of residuals is 0, e =

i ei /n = 0

P

• Note here we divide by n − 2, not n − 1. Why?
• We lose two degrees of freedom because we estimate two
parameters, β0 and β1 .

10

Standard Error of b1

Recall that

σ

SD (b1 ) = pP

(xi − x )2

.

But σ is unknown, we estimate it with se . The estimated SD of b1
is called the standard error (SE) of b1
SE (b1 ) = pP

se

(xi − x )2

11

Sampling distribution of β1
The sampling distribution of b1 is normal





σ

b1 ∼ N β1 , pP
(xi − x )2

⇒

z=

b1 − β1
∼ N (0, 1)
pP
σ/
(xi − x )2

This is (approx.) valid
• either if the errors εi are i.i.d. N (0, σ)
• or if the errors εi are independent and the sample size n is
large
As σ is unknown, if replaced with se , the t-statistic below has a
t-distribution with n − 2 degrees of freedom
T=

b1 − β1
b 1 − β1
=
∼ tn−2 ,
pP
SE (b1 )
(xi − x )2
se /
12

Confidence Intervals for β1

The (1 − α) confidence interval for β1 is given as
b1 ± t ∗ SE (b1 )
where t ∗ is the critical value for the t(n−2) distribution at confidence
level 1 − α.

13

Tests for β1
To test the hypothesis H0 : β1 = a, we use the t-statistic
t=

b1 − a
∼ tn−2
SE (b1 )

The p-value can be computed using the t-table based on the Ha :
Ha

β1 , a

β1 < a

−|t|

t

β1 > a

P-value
|t|

t

Observe that testing H0 : β1 = 0 is equivalent to testing whether x
is useful in predicting y linearly.
• It is possible that r is small but β1 is significantly different from
0.
14

Inference for the Intercept β0
Though the population intercept β0 is rarely of interest, all the
results for the population slope β1 have their counterparts for β0 .
• b0 = β0 + ε −

P
x (xi −x )εi
Pi
2
i (xi −x )

• E (b0 ) = β0 r
. . . . . . . . . . . . . . . (b0 is an unbiased estimate of β0 )
• SD(b0 ) = σ

2

1
P x
n + (xi −x )2

r
• SE(b0 ) = se

2

1
P x
n + (xi −x )2

• The sampling distribution of b0 (when n is large) is

s


2


1
x

b0 ∼ N β0 , σ
+
P

n
(xi − x )2 
• (1 − α) C.I. for β0 : b0 ± t ∗ SE (b0 )
b0 − a
∼ tn−2 and the
SE (b0 )
P-value can be computed similarly as for β1

• The test statistic for H0 : β0 = a is t =

15

Example: Restaurant Tips
The owner of a bistro called First Crush in Potsdam, NY, collected
157 restaurant bills over a 2-week period that he believes provide a
good sample of his customers.
15

the payment and
tipping patterns of

10

●

●
●●

●

●● ●
●

Tip ($)

He wanted to study

●

●

5

its patrons.
0

●
●●

0

●

● ●●
●●
●
●
●●
●
●● ●
●
●
●
●
●●
●●● ●
●
●
● ●
●
● ● ●●
●
● ●●
●
●
● ●●
●
●
●
●
● ●●
●●
●●
●
●
● ●
●● ● ●
●●
●
●
●
●
●
●●
●
●●●
●●
●●●
●
●
●
●
●
●
●●●●
●
●
● ●●
●●
●
●
●●
●●
●●● ●●●●●
●
●●●
●●●
●
●●●●
●
●
●
●
●
●●
●
● ●

20

40

60

Bill ($)
16

Regression in R
Regression in R is as simple as lm(y ˜ x), in which “lm” stands for
“linear model”
> tips = read.table("RestaurantTips.txt",h=T)
> lm(Tip ˜ Bill, data=tips)
Call:
lm(formula = Tip ˜ Bill, data = tips)
Coefficients:
(Intercept)
-0.2923

Bill
0.1822

It is better to save the model as an object,
lmtips = lm(Tip ˜ Bill, data=tips)

and then we can get a more detailed output by viewing the summary() of
the model object. The output is shown in the next slide
17

Regression in R
> summary(lmtips)
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.292267
0.166160 -1.759
0.0806 .
Bill
0.182215
0.006451 28.247
<2e-16 ***
--Residual standard error: 0.9795 on 155 degrees of freedom
Multiple R-squared: 0.8373,Adjusted R-squared: 0.8363
F-statistic: 797.9 on 1 and 155 DF, p-value: < 2.2e-16

• The column “Estimate” gives the LS estimate for the intercept
b0 = −0.292267 and the slope b1 = 0.182215
• The column “Std. Error” gives SE(b0 ) and SE(b1 ):
SE (b0 ) = 0.166160,

SE (b1 ) = 0.006451
18

Example: Confidence Interval for β1
Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.292267
0.166160 -1.759 0.0806
Bill
0.182215
0.006451 28.247 <2e-16

As df = n − 2 = 157 − 2 = 155, t ∗ for a 95% CI is 1.975 (between
1.97 and 1.98).
one tail
two tails

0.1
0.2

0.05
0.10

0.025
0.050

0.01
0.02

0.005
0.010

df 150
200

1.29
1.29

1.66
1.65

1.98
1.97

2.35
2.35

2.61
2.60

Hence the 95% CI for β1 is
b1 ± t ∗ SE (b1 ) = 0.182215 ± 1.975 × 0.006451

= 0.182215 ± 0.01274 ≈ (0.169, 0.195).
Interpretation: With 95% confidence, for each additional dollar in
the bill, the customers gave 16.9 cents to 19.5 cents more tips on
average.

19

Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.292267
0.166160 -1.759
0.0806 .
Bill
0.182215
0.006451 28.247
<2e-16 ***

• Note t-values bi /SE (bi ) are simply the ratio of the numbers in
the “Estimate” column and the numbers in the “Std. Error”
column, e.g.,

−1.759 =

−0.292267
,
0.166160

28.247 =

0.182215
0.006451

• Testing H0 : β1 = 0 is equivalent to testing whether the
amount of tips is linearly related to the amount of the bill. The
small P-value < 2 × 10−16 asserts that the relation is
significant
20

Example: Test for the Slope β1
A general rule for waiters is to tip 15 to 20% of the pre-tax bill. That is,
β0 = 0 and β1 is between 0.15 to 0.20.
Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.292267
0.166160 -1.759 0.0806
Bill
0.182215
0.006451 28.247 <2e-16

• R tests β0 = 0 for us: t-statistic = −1.759, 2-sided p-value = 0.0806
• To test H0 : β1 = 0.2 v.s. HA : β1 < 0.2. The t-statistic is
t=

0.182215 − 0.2
b1 − 0.2
=
= −2.757
0.006451
SE (b1 )

with df = 155, the one-sided p-value is < 0.005.
one tail
two tails
df 150
200

0.1
0.2
1.29
1.29

0.05
0.10
1.66
1.65

0.025
0.050
1.98
1.97

0.01
0.02
2.35
2.35

0.005
0.010
2.61
2.60

Conclusion: Customers of this restaurant gave less than 20% the bill as
tips on average.

21

How to Read R Outputs for Regression?
Residual standard error: 0.9795 on 155 degrees of freedom
Multiple R-squared: 0.8373,Adjusted R-squared: 0.8363
F-statistic: 797.9 on 1 and 155 DF, p-value: < 2.2e-16

• Residual standard error: 0.9795 on 155 degrees of freedom
This gives the estimate se of σ, which is 0.9795.
df = n − 2 = 157 − 2 = 155
• Multiple R-squared: 0.8373 gives r 2 = 0.8373, Bill size
explained 83.73% of the variation in tipping amount.
The correlation between bill size and tips is

√

r=

r2 =

√

0.8373 = 0.915.

• Adjusted R-squared: Ignore this.
• F-statistic: 797.9 on 1 and 155 DF, p-value: < 2.2e-16 Skip.
22

Checking Conditions for Simple
Linear Regression Model

1. Linearity

5

2. Constant variability

Tip ($)
10

15

Conditions for Simple Linear Regression Model

3. (Optional) Nearly normal
0

residuals
0

10

20

30 40
Bill ($)

50

60

70

0

10

20

30 40
Bill ($)

50

60

70

If conditions are satisfied, points
should scatter evenly around the
zero line in the residual plot.

Residuals
−2 0 2 4

• Residual plot

6

Tools for checking conditions:

23

Checking Conditions – Linearity

0.5

What condition is this linear model
0.4

obviously violating?

0.2

(b) Linear relationship

0.1

y
0.3

(a) Constant variability

(c) Linear relationship

Residuals
0.0

0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
x

(d) Normal residuals
(e) No extreme outliers
Note the correlation between the
residuals and x remains zero, but

−0.2

zero correlation , no association.
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
x

It can be a non-linear association
24

Checking Conditions – Constant Variability

1.5

The variability of points around the

y
1.0

least-squares line should be
roughly constant, implying the

0.5

variability of residuals around the 0
line should be roughly constant as

0.0

well, called homoscedasticity.
0.0

0.2

0.4

0.6

0.8

1.0

1.2

1.4

0.4

x

If not, called heterocedasticity,

Residuals
−0.4
0.0

predictions made in areas of larger
variability will be worse. May try
weighted least-square method or
transforming the response.
0.0

0.2

0.4

0.6

0.8

1.0

1.2

1.4

x

25

histogram or boxplot of
residuals
• If the linearity or constant

4

frequency

• Diagnosis: Check the

2

two conditions

0

• Less relevant than the first

6

8

Conditions: Nearly Normal Residuals

−15

−10

−5

0

5

10

15

20

10

15

20

Residuals

variability condition is clearly
violated, there is no need to
check the normality of
residuals.

−15

−10

−5

0

5

Residuals

26

0

10

20

30 40
Bill ($)

50

60

70

30
20
0

0

10

5

frequency

Tip ($)
10

40

50

15

Checking Conditions for the Restaurant Tip Data

−2

0

2

4

6

Residuals
−2 0 2 4

6

Residuals

●

0

10

20

30 40
Bill ($)

50

60

70

●

−2

●

0

●●

2

●

4

●

●

6

Residuals

The constant variability condition seems to be violated.
The size of residual seems to increase with Bill.
27

Types of Outliers

Types of Outliers
How do outliers influence the

0

least squares line in this plot?

y

−5
−10

To answer this question think of

−15

where the regression line would
be with and without the outlier(s).

−20
0.0

0.5

1.0

1.5

Residual

x

Without the outliers the
regression line would be steeper,
and lie closer to the larger group

6
4
2
0
−2

of observations. With the outliers
the line is pulled up and away
from some of the observations in
0.0

0.5

1.0

1.5

the larger group.

x

28

Types of Outliers — Influential Points
12
10

y

8
6
4
2

How do outliers influence the

0

least squares line in this plot?

−2
0

1

2

3
x

4

5

6

evident relationship between

2
Residual

Without the outlier there is no
x and y.

1
0
−1
−2
0

1

2

3
x

4

5

6
29

Types of Outliers — Outlier but Not Influential

y

10

5

0

Does this outlier influence the
0.0

0.2

0.4

0.6

0.8

1.0

Residual

x

slope of the regression line?
Not much...

2
0
−2
−4
−6
0.0

0.2

0.4

0.6
x

0.8

1.0
30

Some Terminology

• Outliers are points that lie away from the cloud of points.
• Outliers that lie away from the center of the cloud in the
x-direction are called high leverage points.
• A point is influential if including or excluding the point would
considerably change the slope of the regression line.
• Influential points must be outliers with high leverages.

31

Types of Outliers
40
30

y

20
10

Which of the below best de-

0

scribes the outlier?

−10
−20

(a) influential
−0.5

0.0

0.5

1.0

Residual

x

(b) high leverage
(c) high leverage

2
1
0
−1
−2
−3

(d) none of the above
(e) there are no outliers
−0.5

0.0

0.5
x

1.0
32

Recap
Which of following is true?
(a) Influential points always change the intercept of the regression
line. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . change the slope
(b) Influential points always reduce R 2 . . . False. See the next slide
(c) It is much more likely for a low leverage point to be influential,
than a high leverage point.
(d) When the data set includes an influential point, the relationship
between the explanatory variable and the response variable is
always nonlinear.
(e) None of the above.
(f) None of the above.
33

Recap (cont.)
Influential point may also increase R 2
r = 0.08, R 2 = r 2 = 0.0064

r = 0.79, R 2 = r 2 ≈ 0.627
10

8

8

6

6

y

12

10

y

12

4

4

2

2

0

0

−2

−2
1

2

3
x

4

5

6

2

2

1

1

Residual

Residual

0

0
−1

0

1

2

3
x

4

5

6

0

1

2

3
x

4

5

6

0
−1
−2

−2
0

1

2

3
x

4

5

6

34

More Examples

Example: GPA and MathSAT
The scatter plot below shows the GPA and MathSAT of a random

r = 0.28

3.0
2.0

2.5

GPA

3.5

4.0

sample of 345 students in a college.

400

500

600

700

800

MathSAT

The correlation r = 0.28 is weak.
Can the slope β1 of the line be significantly different from 0?

35

> summary(lm(GPA ˜ VerbalSAT, data=stu))
Estimate Std. Error t value Pr(>|t|)
(Intercept) 2.1466877 0.1867166 11.497 < 2e-16 ***
MathSAT
0.0016544 0.0003036
5.449 9.68e-08 ***

To test H0 : β1 = 0 v.s. H1 : β1 , 0, the t-statistic is
t=

0.0016544
b1
=
= 5.449
0.0003036
SE (b1 )

with df = 345 − 2 = 343. Two-sided P-value = 9.68 × 10−8 .
• There is strong evidence that students’ GPA is linearly related with
their MathSAT, despite of their small correlation r = 0.28.
• It is possible that r is small but β1 is significantly different from 0,
especially when the sample size n is large.
• Students with higher MathSAT indeed have significantly higher GPA
on average, despite of the huge variability in GPA.
• As R 2 = r 2 = (0.28)2 = 0.0784, MathSAT merely explains 7.84% of
the variation in GPA.

36

Example: GPA and MathSAT – 95% CI for β1
Estimate Std. Error t value Pr(>|t|)
(Intercept) 2.1466877 0.1867166 11.497 < 2e-16 ***
MathSAT
0.0016544 0.0003036
5.449 9.68e-08 ***

df = 345 − 2 = 343. The t ∗ for a 95% CI is 1.97.
one tail
two tails

0.1
0.2

0.05
0.10

0.025
0.050

0.01
0.02

0.005
0.010

df 300
400

1.28
1.28

1.65
1.65

1.97
1.97

2.34
2.34

2.59
2.59

So a 95% confidence interval for β1 is
b1 ±t ∗ SE (b1 ) = 0.0016544±1.97×0.0003036 ≈ (0.00106, 0.00225)
Interpretation: We have 95% confidence that for students with 100
more points in their MathSAT scores, their GPA are 0.106 to 0.225
higher on average.

37

400

500

600
GPA

700

800

30
20
0

2.0

10

2.5

frequency

MathSAT
3.0
3.5

40

50

4.0

Example: GPA and MathSAT – Checking Conditions

−1.0

−0.5

0.0

0.5

1.0

0.5

1.0

Residuals
−1.0 0.0
1.0

Residuals

● ●
● ●●

400

500

600
MathSAT

700

800

−1.0

−0.5

0.0

Residuals

The linearity and constant variability conditions are fine.
The slight left-skewness of residuals is fine because of the large
sample size

38

Example: Fire Damage and Distance to Fire Station
A fire insurance company wanted to relate the amount of fire
damage in major residential fires to the distance between the
burning house and the nearest fire station. The study was
conducted in a large suburb of a major city; a
sample of 15 recent fires in this suburb was
selected. The amount of damage and the
distance between the fire and the nearest fire
station were recorded in each fire.
Damage ($1000)
15 25 35

Distance Damage
(mile)
($1000)
0.7
14.1
1.1
17.3
1.8
17.8
2.1
24.0
2.3
23.1
2.6
19.6
3.0
22.3
3.1
27.5
3.4
26.2
3.8
26.1
4.3
31.3
4.6
31.3
4.8
36.4
5.5
36.0
6.1
43.2

●

●

●

● ●
●
●

●

●

●

●
●

●

●

●

1

2
3
4
5
Distance (miles)

6
39

> fire = read.table("fire.txt",h=T)
> summary(lm(damage ˜ dist, data=fire))
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 10.2779
1.4203
7.237 6.59e-06 ***
dist
4.9193
0.3927 12.525 1.25e-08 ***

• estimate for the intercept b0 = 10.2779 and the slope b1 = 4.9193
• SE (b0 ) = 1.4203, SE (b1 ) = 0.3927
one tail
two tails

0.1
0.2

0.05
0.10

0.025
0.050

0.01
0.02

0.005
0.010

df 13

1.35

1.77

2.16

2.65

3.011

So a 95% confidence interval for β1 is
b1 ± t ∗ SE (b1 ) = 4.9193 ± 2.16 × 0.3927 ≈ 4.919 ± 0.848 ≈ (4.071, 5.767)
Interpretation: We have 95% confidence that every extra mile from the
nearest fire station increases the amount of damage by $4071 to $5767.
40

Example: Test for the Slope β1
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 10.2779
1.4203
7.237 6.59e-06 ***
dist
4.9193
0.3927 12.525 1.25e-08 ***

To test H0 : β1 = 4 v.s. H1 : β1 > 4, the t-statistic is
t=

b1 − 4
4.9193 − 4
= 2.3409
=
0.3927
SE (b1 )

Looking at the t-table for the row with df = 13, the one-sided
P-value is between 0.01 and 0.025.
one tail
two tails
df 13

0.1
0.2
1.35

0.05
0.10
1.77

0.025
0.050
2.16

0.01
0.02
2.65

0.005
0.010
3.011

Conclusion: At 5% level, the extra amount of damage for every
extra mile from the nearest fire station is significantly higher than
$4000.

41




-------------------------------------------
FILE: Lecture6_compressed.pdf
-------------------------------------------
k neighbours classifier
(k-Nearest Neighbors)

Predictive Analytics

Predictive Analytics

k neighbours classifier

1/2

Sources

Based on materials by Fragkiskos D. Malliaros.
École Polytechnique / CentraleSupélec (Université Paris–Saclay).

Predictive Analytics

k neighbours classifier

2/2

Non-parametric learning

13

Classification: Oranges and Lemons

!

•

We can construct a linear
decision boundary:!
y = sign(✓0 + ✓1 x1 + ✓2 x2 )
•
•

Parametric model!
Fixed number of parameters!

!
!
!
!

14!

Classification as Induction

•

Is there alternative way to
formulate the classification
problem?!

•

Classification as induction!
• Comparison to instances
already seen in training!
• Non-parametric learning!

15!

Non-parametric Learning
l

Non-parametric learning algorithm (does not mean NO parameters)!
-

The complexity of the decision function grows with the number
of data points!
-

Contrast with linear regression (≈ as many parameters as
features)!

-

Usually: decision function is expressed directly in terms of the
training examples!

-

Examples:!
l

k-nearest neighbors (today’s lecture)!

l

Tree-based methods (lecture 6)!

l

Some cases of SVMs (lecture 7)!
16!

How Would You Color the Blank Circles?

17!

How Would You Color the Blank Circles?

18!

Partitioning the Space

The training data partitions the entire space!
19!

Nearest Neighbors – The Idea
• Learning:!
– Store all the training examples!

• Prediction:!
– For a point x: assign the label of the training example closest to it!

20!

Nearest Neighbors – The Idea
• Learning:!
– Store all the training examples!

• Prediction:!
– For a point x: assign the label of the training example closest to it!
– Classification!
• Majority vote: predict the class of the most frequent label
among the k neighbors!
– Regression!
• Predict the average of the labels of the k neighbors!

21!

Instance-based Learning
• Learning!
– Store training instances!

Where the magic
happens!

• Prediction!
– Compute the label for a new instance based on its similarity with
the stored instances!

• Also called lazy learning!
• Similar to case-based reasoning!
-

Doctors treating a patient based on how patients with similar
symptoms were treated!

-

Judges ruling court cases based on legal precedent!

22!

Computing distances and
similarities

23

Distance Function
• Distance function!

d : X ! R+
!

• Properties of a distance function (or metric)!
1.
2.
3.
4.

d(x,
z) 0 non-negativity
!
d(x,
x) = 0 identity of indiscernibles
!
d(x,
z) = d(z, x) symmetry
!
d(x,
z)  d(x, u) + d(u, z) triangle inequality
!

24!

Distance Between Instances
• Euclidean distance (L2)!

d(x1 , x2 ) = ||x1

x2 ||2 =

• Manhattan distance (L1)!
1

2

d(x , x ) = ||x

1

2

x ||1 =

• Lp-norm!

d(x1 , x2 ) = ||x1

x2 ||p =

v
u
tX
n
j=1

n
X
j=1

x 2 Rn
(x1j

|x1j

n
X
j=1

x2j )2

Manhattan distance: The sum of the
horizontal and vertical distances
between points on a grid!

x2j |

!1/p
|x1j

Source: Wikipedia!

x2j |p
25!

From Distance to Similarity
• Pearson’s correlation!

Pn

s=

1
1+d

x̄ =

1X
xj
n

j=1 (x j

⇢(x, z) = qP

x̄)(z j z̄)
qP
n
x̄)2
j=1 (z j

n
j=1 (x j

!

z̄)2

n

j=1

• Assuming that the data is centered!

Pn
⇢(x, z) = qP

j=1 x j z j

n
2
j=1 x j

qP

n
2
j=1 z j

Geometric interpretation?!
26!

Pearson’s Correlation
• Pearson's correlation (centered data)!

Pn

inner product

hx, zi
j=1 x j z j
⇢(x, z) = qP
=
= cos ✓
qP
n
n
||x|| ||z||
2
2
x
z
j=1 j
j=1 j
• Cosine similarity: the dot product can be used to measure
similarities between vectors!

27!

Categorical Features
• Represent objects as the list of presence/absence (or counts)
of features that appear in it!
• Example: molecules!
– Features: atoms and bonds of a certain type!
– C, H, S, O, N, …!
– O-H, O=C, C-N, ...!

28!

Binary Representation (1/2)
0!

1!

1!

0!

0!

1!

0!

0!

no occurrence of the 1st
feature

•

0!

1!

0!

1!

0!

0!

1!

1+ occurrences
of the 10th feature

Hamming distance between two binary representations!
– Number of bits that are different!
1

2

d(x , x ) =

n
X

XOR operator!

(x1j XOR x2j )

j=1

– Equivalent to the L1 distance!
1

2

d(x , x ) =

n
X
j=1

|x1j

x2j |

Input
A
B!
0
0
0
1
1
0
1
1

Output!
0!
1!
1!
0!

29!

Binary Representation (2/2)
0!

1!

1!

0!

0!

1!

0!

0!

no occurrence of the 1st
feature

0!

1!

0!

1!

0!

0!

1!

1+ occurrences
of the 10th feature

• Jaccard similarity (or Tanimoto similarity)!
– Number of shared features (normalized)!

Pn

1
2
j=1 (x j AND x j )

J(x1 , x2 ) = Pn

1
2
j=1 (x j OR x j )

AND operator!

OR operator!

Input
A
B!
0
0
0
1
1
0
1
1

Input
A
B!
0
0
0
1
1
0
1
1

Output!
0!
0!
0!
1!

Jaccard index: intersection over union (Wikipedia: https://en.wikipedia.org/wiki/Jaccard_index)!

Output!
0!
1!
1!
1!

30!

Example
x = 010101001 !
y = 010011000!
!

• Hamming distance!
x = 010101001 !
y = 010011000!
Thus, d(x,y) = 3!
!

• Jaccard similarity!
J = (# of 11) / ( # of 01 + # of 10 + # of 11)!
= (2) / (1 + 2 + 2) = 2 / 5 = 0.4!

31!

Let’s go back to the
kNN classifier

32

Nearest Neighbor Algorithm
n

• Training examples in the Euclidean space! x 2 R
• Idea: The label of a test data point is estimated from the
known value of the nearest training example!
– The distance is typically defined to be the Euclidean one!
Algorithm 1

1. Find example (x*, y*) from the stored training set closest to
the test instance x. That is:!

x⇤ = argminxi 2 train. set d(xi , x)
2. Output y(x) = y*!

(The output label)!

33!

k-Nearest Neighbors (kNN) Algorithm
1NN!
Every example in
the blue shaded
area will be
misclassified as
the blue class!

3NN!
Every example in
the blue shaded
area will be
classified
correctly as the
red class!

• Algorithm 1 is sensitive to mis-labeled data (‘class noise’)!
• Consider the vote of the k nearest neighbors (majority vote)!
Algorithm 2

• Find k examples (x*i, y *i), i=1,…,k closest to the test instance x!
• The output is the majority class!
34!

Choice of Parameter k (1/2)
• Small k: noisy decision!
– The idea behind using more than 1 neighbors is to average out
the noise!

• Large k!
– May lead to better prediction performance!
– If we set k too large, we may end up looking at samples that are
not neighbors (are far away from the point of interest)!
– Also, computationally intensive. Why?!
– Extreme case: set k=m (number of points in the dataset)!
• For classification: the majority class!
• For regression: the average value!

35!

Choice of Parameter k (2/2)
Set k by cross validation, by examining the misclassification error !

Rule of thumb: !
k=7!

k=

p

m

m: # of training instances!

Source: https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/!

36!

Advantages of kNN
• Training is very fast!
– Just store the training examples!
– Can use smart indexing procedures to speed-up testing!

• The training data is part of the ‘model’!
– Useful in case we want to do something else with it!

• Quite robust to noisy data!
– Averaging k votes!

• Can learn complex functions (implicitly)!
!

37!

Drawbacks of kNN
• Memory requirements!
– Must store all training data!

• Prediction can be slow (will figure it out by yourself in the lab)!
– Complexity of labeling 1 new data point: O(knm)!
– But kNN works best with lots of samples!
– Can we further improve the running time?!
• Efficient data structures (e.g., k-D trees)!
• Approximate solutions based on hashing!

• High dimensional data and the curse of dimensionality!
– Computation of the distance in a high dimensional space may
become meaningless!
– Need more training data!
– Dimensionality reduction!
Wikipedia: https://en.wikipedia.org/wiki/K-d_tree!

38!

kNN – Some More Issues
• Normalize the scale of the attributes!
• Simple option: linearly scale the range of each feature to be,
e.g., in the range of [0,1]!
• Linearly scale each dimension to have 0 mean and variance 1!
– Compute the mean μ and variance σ2 for an attribute xj and scale:
(xj - μ)/σ2!

39!

Decision Boundary of kNN
• Decision boundary in classification!
– Line separating the positive from negative regions!

• What decision boundary is the kNN building?!
– The nearest neighbors algorithm does not explicitly compute
decision boundaries, but those can be inferred !

40!

Voronoi Tessellation
• Voronoi cell of x:!

Consider the case of 1NN!

– Set of all points of the space closer to x than any other point of
the training set!
– Polyhedron!

• Voronoi tessellation (or diagram) of the space!
– Union of all Voronoi cells!

x!

41!

Voronoi Tessellation
• The Voronoi diagram defines the decision boundary of the
1NN!
• The kNN algorithms also partitions the space but in a more
complex way!

Wikipedia: https://en.wikipedia.org/wiki/
Voronoi_diagram!
42!

kNN Variants
• Weighted kNN!
– Weight the vote of each neighbor xi according to the distance to
the test point x!

wi =

1
d(x, xi )2

– Other kernel functions can be used to weight the distance of
neighbors!

!
Source: https://epub.ub.uni-muenchen.de/1769/1/paper_399.pdf!

43!

scikit-learn

http://scikit-learn.org/stable/modules/generated/
sklearn.neighbors.KNeighborsClassifier.html!

44!




-------------------------------------------
FILE: LectureArima_compressed.pdf
-------------------------------------------
What is a Time Series?

Definition
A time series is a sequence of data points collected over time, typically at regular intervals.
Examples:
16

Monthly sales figures
Yearly population counts
Hourly stock prices

Value

Daily temperature readings

14
12
10
1

Weekly COVID-19 cases

Your Name (University)

ARIMA for Beginners

2

3

4 5
Time

6

7

8

December 5, 2025

1/3

Why Study Time Series?
Main Goal: Forecasting the Future!
Business

Finance

Science

Sales prediction

Weather forecasting

Stock prices

Inventory planning

Climate modeling

Interest rates

Budget allocation

Disease outbreaks

Risk management

Key Insight
Understanding the past patterns helps us predict future values!
Your Name (University)

ARIMA for Beginners

December 5, 2025

2/3

Time Series Notation
Basic Notation
We denote a time series as: {Yt } where t = 1, 2, 3, . . . , n
Example: Ice Cream Sales (units)
Month (t)
Sales (Yt )

1
100

2
110

3
125

4
140

5
155

6
165

7
180

Y1 = 100 (sales in month 1)
Y3 = 125 (sales in month 3)
Yt−1 means the previous observation (lag 1)
Yt−2 means two periods ago (lag 2)
Your Name (University)

ARIMA for Beginners

December 5, 2025

3/3

A Simple Example: Lemonade Stand
Scenario: You run a lemonade stand and track
daily sales.
35

Cups Sold
20
22
21
24
25
30
28

30
Cups Sold

Day
Monday
Tuesday
Wednesday
Thursday
Friday
Saturday
Sunday

25
20
15

M

T

W Th F
Day

Sa Su

Question: How many cups will you sell next
Monday?
Your Name

ARIMA for Beginners

December 5, 2025

1/5

Four Components of Time Series

1

Trend (T): Long-term direction
Upward, downward, or flat

2

Seasonality (S): Regular patterns
Weekly, monthly, yearly cycles

3

Cyclical (C): Irregular fluctuations
Economic cycles

4

Visual Examples:
Trend: Straight line up or down
Seasonality: Wave pattern
Noise: Random jumps
Combined: Real-world data!

Random/Noise (ϵ): Unpredictable
What we cannot explain

Your Name

ARIMA for Beginners

December 5, 2025

2/5

Sales (units)

Real-World Example: Ice Cream Sales
Year 1
Year 2

150

100

50
J

F

M

A

M

J
J
Month

A

S

O

N

D

Trend: Sales are gradually increasing (Year 2 ¿ Year 1)
Seasonality: Peak in summer (June-August), low in winter
Your Name

ARIMA for Beginners

December 5, 2025

3/5

What is Stationarity?

Definition (Simple)
A time series is stationary if its statistical properties (mean, variance) don’t change over time.
Stationary Series:

Non-Stationary Series:

Constant mean

Changing mean (trend)

Constant variance

May have changing variance

No trend

Upward/downward drift

✓ Good for ARIMA

× Needs transformation

Your Name

ARIMA for Beginners

December 5, 2025

4/5

Why Does Stationarity Matter?
Important!
ARIMA models require stationarity to work properly!
Think of it this way:
If a series is stationary, past patterns will repeat in the future
If non-stationary, the patterns keep changing, making prediction harder

Analogy
Imagine predicting the weather:
Stationary: Temperature in a stable climate - patterns repeat yearly
Non-stationary: Temperature with global warming - getting hotter each year
Your Name

ARIMA for Beginners

December 5, 2025

5/5

Making Data Stationary: Differencing
Differencing
Subtract consecutive observations to remove trends.
∇Yt = Yt − Yt−1
Example:
t
Yt
Yt − Yt−1
Original (non-stationary):
Shows upward trend
Your Name

1
10
–

2
12
2

3
15
3

4
19
4

5
24
5

6
30
6

After differencing:
Still not constant, but closer!
(May need 2nd differencing)
ARIMA for Beginners

December 5, 2025

1/6

What is an AR Model?
Key Idea
Today’s value depends on yesterday’s value (and maybe days before).

Yt = c + ϕ1 Yt−1 + ϵt
AR(1) Model Components:
Yt = today’s value
Yt−1 = yesterday’s value (lag 1)
ϕ1 = how much yesterday influences today (coefficient)
c = constant (baseline level)
ϵt = random error (unpredictable part)

Think of it like...
Tomorrow’s weather is similar to today’s weather, plus some randomness!
Your Name

ARIMA for Beginners

December 5, 2025

2/6

AR(1) Example: Daily Temperature
Model: Yt = 5 + 0.8Yt−1 + ϵt
Meaning:
Baseline: 5°C
80% of yesterday’s temperature carries over
Plus some random variation
Calculation Example:
If yesterday’s temperature was Yt−1 = 20C
Expected today: Yt = 5 + 0.8 × 20 = 5 + 16 = 21C
Actual might be 21 ± ϵ (e.g., 19°C or 23°C)

Your Name

ARIMA for Beginners

December 5, 2025

3/6

AR(1): Visual Understanding
ϕ1 = 0.9 (high persistence)

ϕ1 = 0.3 (low persistence)

20
Value

Value

12
15

10
8

10
0

5

10
Time

0

15

5

10
Time

15

Slow, smooth changes

Quick, jumpy changes

High correlation between neighbors

Low correlation between neighbors

Interpretation
Higher ϕ1 = stronger memory = smoother series

Your Name

ARIMA for Beginners

December 5, 2025

4/6

AR(2) Model: Looking Back Two Steps
Yt = c + ϕ1 Yt−1 + ϕ2 Yt−2 + ϵt
Today depends on:
Yesterday (Yt−1 ) with weight ϕ1
Two days ago (Yt−2 ) with weight ϕ2
Example: Yt = 10 + 0.5Yt−1 + 0.3Yt−2 + ϵt
If Yt−1 = 20 and Yt−2 = 18:
Yt = 10 + 0.5(20) + 0.3(18)
= 10 + 10 + 5.4
= 25.4
Your Name

ARIMA for Beginners

December 5, 2025

5/6

General AR(p) Model
AR(p) Formula
Yt = c + ϕ1 Yt−1 + ϕ2 Yt−2 + · · · + ϕp Yt−p + ϵt
p = order of the AR model
p = how many past values we use
Model
AR(1)
AR(2)
AR(3)
AR(p)

Your Name

Uses
1 past value
2 past values
3 past values
p past values

Example
Stock prices
Quarterly GDP
Some climate data
Depends on data

ARIMA for Beginners

December 5, 2025

6/6

AR Practice Problem
Exercise
Given the AR(1) model: Yt = 2 + 0.7Yt−1 + ϵt
If Y100 = 15, what is the expected value of Y101 ?

Your Name

ARIMA for Beginners

December 5, 2025

1/6

AR Practice Problem
Exercise
Given the AR(1) model: Yt = 2 + 0.7Yt−1 + ϵt
If Y100 = 15, what is the expected value of Y101 ?
Solution:
Y101 = 2 + 0.7 × Y100 + ϵ101

E [Y101 ] = 2 + 0.7 × 15 + 0

(expected error is 0)

E [Y101 ] = 2 + 10.5
E [Y101 ] = 12.5

Your Name

ARIMA for Beginners

December 5, 2025

1/6

AR Practice Problem
Exercise
Given the AR(1) model: Yt = 2 + 0.7Yt−1 + ϵt
If Y100 = 15, what is the expected value of Y101 ?
Solution:
Y101 = 2 + 0.7 × Y100 + ϵ101

E [Y101 ] = 2 + 0.7 × 15 + 0

(expected error is 0)

E [Y101 ] = 2 + 10.5
E [Y101 ] = 12.5

Note
The actual value will be 12.5 ± ϵ, where ϵ is random noise.
Your Name

ARIMA for Beginners

December 5, 2025

1/6

What is an MA Model?
Key Idea
Today’s value depends on past random shocks (errors), not past values!

Yt = µ + ϵt + θ1 ϵt−1
MA(1) Model Components:
Yt = today’s value
µ = mean (average level)
ϵt = today’s random shock
ϵt−1 = yesterday’s random shock
θ1 = how much yesterday’s shock affects today

Think of it like...
An unexpected event yesterday still affects you today!
Your Name

ARIMA for Beginners

December 5, 2025

2/6

MA vs AR: What’s the Difference?
AR: AutoRegressive

MA: Moving Average

Yt = c + ϕ1 Yt−1 + ϵt

Yt = µ + ϵt + θ1 ϵt−1

Depends on past values

Depends on past shocks

Example: If it was hot yesterday, it’s probably Example: If there was a surprise storm
hot today.
yesterday, today might still be affected.

Key Difference
AR: “History of the series itself”
MA: “History of the surprises/shocks”

Your Name

ARIMA for Beginners

December 5, 2025

3/6

MA(1) Example: Unexpected Events
Model: Yt = 100 + ϵt + 0.6ϵt−1
Scenario: Daily sales with average of 100 units
Suppose:
Yesterday there was a surprise promotion: ϵt−1 = +20 (sold 20 extra)
Today’s random event: ϵt = +5
Today’s sales:
Yt = 100 + 5 + 0.6 × 20
= 100 + 5 + 12
= 117 units

Interpretation
The surprise from yesterday (+20) still affects today (+12), but with a smaller impact (60%).
Your Name

ARIMA for Beginners

December 5, 2025

4/6

MA(q) General Model
MA(q) Formula
Yt = µ + ϵt + θ1 ϵt−1 + θ2 ϵt−2 + · · · + θq ϵt−q
q = order of the MA model
q = how many past shocks we consider
Model
MA(1)
MA(2)
MA(q)

Description
Today affected by shock from 1 period ago
Today affected by shocks from 1 and 2 periods ago
Today affected by shocks from 1, 2, ..., q periods ago

Key Property
MA(q) has “short memory” - effects die out after q periods!
Your Name

ARIMA for Beginners

December 5, 2025

5/6

ARMA Model: Best of Both Worlds
ARMA(p,q) Model
Yt = c + ϕ1 Yt−1 + · · · + ϕp Yt−p +ϵt + θ1 ϵt−1 + · · · + θq ϵt−q
|
{z
}
{z
}
|
AR part

ARMA combines:

MA part

Notation:

Past values (AR)

p = AR order

Past shocks (MA)

q = MA order

Special Cases
ARMA(1,0) = AR(1)
ARMA(0,1) = MA(1)
ARMA(1,1) = AR(1) + MA(1)
Your Name

ARIMA for Beginners

December 5, 2025

6/6

ARMA(1,1) Example
Model: Yt = 5 + 0.7Yt−1 + ϵt + 0.4ϵt−1
Given:
Yt−1 = 20 (yesterday’s value)
ϵt−1 = 3 (yesterday’s shock)
ϵt = −1 (today’s shock)
Calculate Yt :
Yt = 5 + 0.7(20) + (−1) + 0.4(3)
= 5 + 14 − 1 + 1.2
= 19.2

Breakdown
Constant (5) + AR effect (14) + Today’s shock (-1) + MA effect (1.2) = 19.2
Your Name

ARIMA for Beginners

December 5, 2025

1/6

The ”I” in ARIMA: Integration
The Problem
ARMA models only work for stationary data!
Real-world data often has trends (non-stationary).

The Solution: Differencing
Transform non-stationary data by taking differences!
First difference: ∇Yt = Yt − Yt−1
Second difference: ∇2 Yt = ∇Yt − ∇Yt−1

Your Name

ARIMA for Beginners

December 5, 2025

2/6

Differencing Example
Original Data (with trend):
t
Yt

1
10

2
13

3
17

4
22

t
Yt − Yt−1

2
3

3
4

3
1

4
1

5
28

6
35

7
43

4
5

5
6

6
7

7
8

5
1

6
1

7
1

8
1

8
52

First Difference (d = 1):
8
9

Second Difference (d = 2):
t
∇2 Yt

✓ After 2nd differencing: constant! (Stationary)
Your Name

ARIMA for Beginners

December 5, 2025

3/6

ARIMA(p,d,q) Model
ARIMA Notation

ARIMA( p , |{z}
d , q )
|{z}
|{z}
AR

Parameter
p
d
q

Your Name

diff

MA

Meaning
Number of AR terms (past values)
Number of differences (to make stationary)
Number of MA terms (past shocks)

ARIMA for Beginners

Typical Values
0, 1, 2, 3
0, 1, 2
0, 1, 2, 3

December 5, 2025

4/6

ARIMA Process: Step by Step
Raw Data
(Non-stationary)

Apply d differences
(Make stationary)

Fit ARMA(p,q)
to differenced data

Your Name

Make forecasts
ARIMA for Beginners
on differenced
scale

December 5, 2025

5/6

ARIMA Examples: Model Interpretation
ARIMA(1,1,0):
Difference once (d = 1)
Then fit AR(1) to differenced data
Formula: ∇Yt = c + ϕ1 ∇Yt−1 + ϵt
ARIMA(0,1,1):
Difference once (d = 1)
Then fit MA(1) to differenced data
Formula: ∇Yt = µ + ϵt + θ1 ϵt−1
This is called Exponential Smoothing!
ARIMA(1,1,1):
Difference once (d = 1)
Then fit ARMA(1,1) to differenced data
Formula: ∇Yt = c + ϕ1 ∇Yt−1 + ϵt + θ1 ϵt−1
Your Name

ARIMA for Beginners

December 5, 2025

6/6

Autocorrelation: Correlation with Yourself
Definition
Autocorrelation measures how correlated a time series is with its own past values.
Autocorrelation at lag k: How related is Yt to Yt−k ?
Lag
1
2
7
12

Correlation between...
Today and Yesterday
Today and 2 days ago
Today and 1 week ago
Today and 1 year ago (monthly data)

Notation: ρk or rk for autocorrelation at lag k
Your Name

ARIMA for Beginners

December 5, 2025

1/6

ACF: Autocorrelation Function
ACF
Plot of autocorrelations at different lags.

Autocorrelation

1
0.5
0
1

2

3

4

5

6

7

8

Lag

Red dashed lines: Significance bounds (95%)
Bars outside bounds are significant
Your Name

ARIMA for Beginners

December 5, 2025

2/6

PACF: Partial Autocorrelation Function
PACF

Partial Autocorrelation

Measures correlation at lag k after removing effects of lags 1 to k − 1.
1
0.5
0
1

2

3

4

5

6

7

8

Lag

Only lag 1 is significant ⇒ suggests AR(1) model!
Your Name

ARIMA for Beginners

December 5, 2025

3/6

Using ACF and PACF to Choose p and q
Model
AR(p)
MA(q)
ARMA(p,q)

ACF Pattern
Decays gradually
Cuts off after lag q
Decays gradually

PACF Pattern
Cuts off after lag p
Decays gradually
Decays gradually

Practical Rules
If PACF cuts off at lag p, and ACF decays ⇒ use AR(p)

If ACF cuts off at lag q, and PACF decays ⇒ use MA(q)
If both decay gradually ⇒ use ARMA(p,q)

Your Name

ARIMA for Beginners

December 5, 2025

4/6

ACF/PACF Pattern Summary

MA(1) Pattern:

AR(1) Pattern:
ACF: Slowly decays (exponentially)

ACF: Only lag 1 significant, then cuts off

PACF: Only lag 1 significant, then cuts off

PACF: Slowly decays
MA(2) Pattern:

AR(2) Pattern:
ACF: Slowly decays (may oscillate)
PACF: Lags 1 and 2 significant, then cuts
off

Your Name

ACF: Lags 1 and 2 significant, then cuts
off
PACF: Slowly decays

ARIMA for Beginners

December 5, 2025

5/6

ACF/PACF Visual Examples
How to read ACF/PACF plots:

What you see
ACF decays, PACF spike at 1
ACF decays, PACF spikes at 1,2
ACF spike at 1, PACF decays
ACF spikes at 1,2, PACF decays
Both decay

ACF
Decay
Decay
Cut at 1
Cut at 2
Decay

PACF
Cut at 1
Cut at 2
Decay
Decay
Decay

Model
AR(1)
AR(2)
MA(1)
MA(2)
ARMA

Important Note
In practice, patterns are rarely this clean! Use this as a starting point, then refine.
Your Name

ARIMA for Beginners

December 5, 2025

6/6

The Box-Jenkins Methodology
1. Identification
Choose p, d, q

2. Estimation
Fit the model

If bad fit

3. Diagnostic
Check residuals

4. Forecasting
Predict future
Your Name

ARIMA for Beginners

December 5, 2025

1/6

Step 1: Identification
Goal: Determine values of p, d, and q
1

Check for stationarity
Plot the data
Look for trends or changing variance
If non-stationary: difference the data (d = 1 or d = 2)

2

Examine ACF and PACF
ACF cuts off at lag q ⇒ MA(q)
PACF cuts off at lag p ⇒ AR(p)

3

Start simple!
Begin with low values: ARIMA(1,1,1) or ARIMA(1,1,0)
Add complexity only if needed
Your Name

ARIMA for Beginners

December 5, 2025

2/6

Step 2: Estimation
Goal: Estimate model parameters (ϕ, θ, etc.)

Methods
Maximum Likelihood Estimation (MLE) - most common
Least Squares - simpler alternative
In practice: Software does this automatically!

Software Options
R: arima(), auto.arima()
Python: statsmodels.ARIMA
Excel: Various add-ins
Your Name

ARIMA for Beginners

December 5, 2025

3/6

Step 3: Diagnostic Checking

Goal: Verify the model fits well

What to Check
Residuals should behave like white noise:
Mean = 0
Constant variance
No autocorrelation (ACF of residuals all near 0)
Approximately normal distribution

Your Name

ARIMA for Beginners

December 5, 2025

4/6

Step 4: Forecasting

Goal: Predict future values
Point Forecast:
Single best prediction

20
Value

Example: “Sales will be 150 units”
Interval Forecast:

15
10
0

Range of likely values

5

10

15

Time

Example: “95% CI: 130 to 170 units”

Your Name

ARIMA for Beginners

December 5, 2025

5/6

Model Selection Criteria
How to choose between different ARIMA models?

AIC (Akaike Information Criterion)
AIC = −2 log(L) + 2k
L = likelihood (how well model fits)
k = number of parameters
Lower AIC = Better model

BIC (Bayesian Information Criterion)
BIC = −2 log(L) + k log(n)
Your Name

ARIMA for Beginners

December 5, 2025

6/6

Example: Monthly Sales Data
Scenario: A store tracks monthly sales (in thousands of dollars)
1
50

2
52

Sales ($K)

Month
Sales

3
55

4
58

5
62

6
65

7
70

8
73

9
78

10
82

11
88

12
92

80

60
1

2

3

4

5

6 7 8
Month

9 10 11 12

Observation: Clear upward trend ⇒ non-stationary!
Your Name

ARIMA for Beginners

December 5, 2025

1/6

Example: Step 1 - Make Stationary
Apply first differencing (d = 1):

Differenced Sales

Month
∇Yt

2
2

3
3

4
3

5
4

6
3

7
5

5

6

7 8
Month

8
3

9
5

10
4

11
6

12
4

8
6
4
2
0

2

3

4

9

10 11 12

✓ Now looks more stationary (fluctuates around mean ≈ 3.8)
Your Name

ARIMA for Beginners

December 5, 2025

2/6

Example: Step 2 - Analyze ACF/PACF
Assume ACF and PACF analysis suggests:
PACF: Significant at lag 1, then cuts off
ACF: Gradual decay
⇒ This suggests an AR(1) model on differenced data
⇒ Overall model: ARIMA(1,1,0)

Model Selection

ARIMA(1, 1, 0)
p = 1 (one AR term), d = 1 (one difference), q = 0 (no MA terms)

Your Name

ARIMA for Beginners

December 5, 2025

3/6

Example: Step 3 - Estimate Parameters
Fitting ARIMA(1,1,0):
Model equation: ∇Yt = c + ϕ1 ∇Yt−1 + ϵt
Suppose estimation gives:
c = 2.5
ϕ1 = 0.3
Fitted model:
∇Yt = 2.5 + 0.3∇Yt−1 + ϵt
Or equivalently:
Yt − Yt−1 = 2.5 + 0.3(Yt−1 − Yt−2 ) + ϵt

Your Name

ARIMA for Beginners

December 5, 2025

4/6

Example: Step 4 - Forecast
Forecast for Month 13:
Given:
Y12 = 92
Y11 = 88
∇Y12 = Y12 − Y11 = 4
Calculate:
∇Y13 = 2.5 + 0.3 × ∇Y12
= 2.5 + 0.3 × 4

= 2.5 + 1.2 = 3.7
Convert back:
Y13 = Y12 + ∇Y13 = 92 + 3.7 = 95.7
Forecast: Your
Sales
in month 13 ≈ $95,700 ARIMA for Beginners
Name

December 5, 2025

5/6

Example: Forecast Multiple Periods
Continuing the forecast...
Month 14:
∇Y14 = 2.5 + 0.3 × 3.7 = 2.5 + 1.11 = 3.61
Y14 = 95.7 + 3.61 = 99.31

Month 15:
∇Y15 = 2.5 + 0.3 × 3.61 = 2.5 + 1.08 = 3.58
Y15 = 99.31 + 3.58 = 102.89

Month
Forecast
Your Name

13
95.7

14
99.3

ARIMA for Beginners

15
102.9
December 5, 2025

6/6

Tips for Beginners
Start Simple
Try ARIMA(1,1,0), ARIMA(0,1,1), or ARIMA(1,1,1) first
Only add complexity if diagnostics suggest it

Use Automatic Selection
R: auto.arima() function
Python: pmdarima.auto arima()
Let software suggest initial values

Always Visualize
Plot your data first
Plot ACF/PACF
Plot residuals
after fitting
Your Name

ARIMA for Beginners

December 5, 2025

1/5

Software: R Example
R Code for ARIMA
# Load data
sales <- c(50, 52, 55, 58, 62, 65, 70, 73, 78, 82, 88, 92)
ts sales <- ts(sales, frequency=12)
# Automatic ARIMA selection
library(forecast)
model <- auto.arima(ts sales)
summary(model)
# Or manual specification
model <- arima(ts sales, order=c(1,1,0))
# Forecast next 3 periods
forecast(model, h=3)
# Plot forecast
plot(forecast(model, h=3))
Your Name

ARIMA for Beginners

December 5, 2025

2/5

Software: Python Example
Python Code for ARIMA
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt
# Create data
sales = [50, 52, 55, 58, 62, 65, 70, 73, 78, 82, 88, 92]
# Fit ARIMA(1,1,0)
model = ARIMA(sales, order=(1,1,0))
fitted = model.fit()
print(fitted.summary())
# Forecast
forecast = fitted.forecast(steps=3)
print(forecast)

Your Name

ARIMA for Beginners

December 5, 2025

3/5

Interpreting Software Output

Key things to look for:
Output
Coefficients
AIC/BIC
Residual diagnostics
Ljung-Box test

Your Name

What to check
Are they significant? (p-value ¡ 0.05)
Lower is better (compare models)
Should be white noise
p-value ¿ 0.05 (no autocorrelation)

ARIMA for Beginners

December 5, 2025

4/5

What’s Next? Beyond ARIMA
Extensions of ARIMA:
SARIMA: Seasonal ARIMA for data with seasonal patterns
SARIMA(p,d,q)(P,D,Q)m
Example: Monthly data with yearly seasonality

ARIMAX: ARIMA with external variables (predictors)
GARCH: For modeling changing variance
VAR: Vector AR for multiple related time series
Machine Learning: LSTM, Prophet, etc.

Your Name

ARIMA for Beginners

December 5, 2025

5/5




-------------------------------------------
FILE: Predictive_Analytics-14_compressed.pdf
-------------------------------------------
Predictive Analytics in Excel
Functions, Formulas, and Analysis Techniques

Excel Predictive Analytics

1 / 10

Course Overview

What You’ll Learn:
Part 1: Foundations

Part 3: Forecasting

Statistical functions in Excel

FORECAST functions

Correlation and relationships

Moving averages

Data preparation techniques

Exponential smoothing

Part 2: Regression

Part 4: Advanced

Linear regression formulas

Automatic forecast sheets

Multiple regression

What-if analysis

Using Data Analysis ToolPak

Best practices

Excel Predictive Analytics

2 / 10

Why Excel for Predictive Analytics?

Advantages:
✓ Widely available
✓ No programming required
✓ Visual and interactive
✓ Easy to share results
✓ Built-in charting
✓ Good for small-medium data

Limitations:
× Row limit: 1,048,576
× Limited ML algorithms
× Manual process
× No version control
× Can be slow with big data

Excel Predictive Analytics

3 / 10

Essential Statistical Functions
Descriptive Statistics - Your Starting Point
Purpose

Function

Example

Average
Median
Mode
Std Deviation
Variance
Minimum
Maximum
Count

=AVERAGE(A1:A100)
=MEDIAN(A1:A100)
=MODE.SNGL(A1:A100)
=STDEV.S(A1:A100)
=VAR.S(A1:A100)
=MIN(A1:A100)
=MAX(A1:A100)
=COUNT(A1:A100)

Mean of values
Middle value
Most frequent
Sample std dev
Sample variance
Smallest value
Largest value
Number of values

Pro Tip: Use =STDEV.P() and =VAR.P() for population (not sample)
Excel Predictive Analytics

4 / 10

Percentiles and Distribution Analysis
Understanding Data Distribution:
Function

Description

=PERCENTILE.INC(A1:A100, 0.25)
=PERCENTILE.INC(A1:A100, 0.5)
=PERCENTILE.INC(A1:A100, 0.75)
=QUARTILE.INC(A1:A100, 1)
=QUARTILE.INC(A1:A100, 3)
=SKEW(A1:A100)
=KURT(A1:A100)

25th percentile (Q1)
50th percentile (Median)
75th percentile (Q3)
First quartile
Third quartile
Skewness of distribution
Kurtosis (tail heaviness)

Interquartile Range (IQR):
=QUARTILE.INC(A1:A100,3) - QUARTILE.INC(A1:A100,1)
Use IQR to identify outliers: values < Q1 − 1.5 × IQR or > Q3 + 1.5 × IQR
Excel Predictive Analytics

5 / 10

Correlation Analysis
What is Correlation?
Measures the strength and direction of linear relationship between two variables. Range: -1
to +1
The CORREL Function:
=CORREL(A1:A100, B1:B100)
Interpretation:
Value
0.7 to 1.0
0.4 to 0.7
0.0 to 0.4
-0.4 to 0.0
-0.7 to -0.4
-1.0 to -0.7

Interpretation
Strong positive relationship
Moderate positive relationship
Weak positive relationship
Weak negative relationship
Moderate negative relationship
Strong
negative relationship
Excel Predictive Analytics

6 / 10

Correlation Example

Scenario: Analyze relationship between advertising spend and sales
Formulas to Use:
Data Layout:

1
2
3
4
5
6

A

B

Ad Spend
1000
1500
2000
2500
3000

Sales
15000
18000
22000
25000
31000

Correlation:
=CORREL(A2:A6, B2:B6)
Result: 0.987 (strong positive)
Covariance:
=COVARIANCE.S(A2:A6, B2:B6)
Result: 10,000,000

Excel Predictive Analytics

7 / 10

Creating a Correlation Matrix
For multiple variables, create a correlation matrix:
Price
Ad Spend
Sales

Price
1
-0.45
-0.32

Ad Spend

Sales

1
0.89

1

How to Build in Excel:
1

Set up variable names in row 1 and column A

2

For cell B2: =CORREL($A$2:$A$50, B$2:B$50)

3

Copy formula across and down

4

Use mixed references ($) to lock appropriately

Shortcut: Use Data Analysis ToolPak → Correlation
Excel Predictive Analytics

8 / 10

Enabling the Data Analysis ToolPak
The ToolPak adds powerful statistical analysis features!
How to Enable:
1
2
3
4

Go to File → Options

Click Add-ins (left panel)
At bottom: Manage → Excel Add-ins → Go
Check Analysis ToolPak → OK

Now Available (Data Tab → Data Analysis):
Descriptive Statistics

Moving Average

Correlation

Exponential Smoothing

Covariance

Histogram

Regression

t-Test, ANOVA
Excel Predictive Analytics

9 / 10

Quick Descriptive Statistics with ToolPak
Steps:
1 Data tab → Data Analysis → Descriptive Statistics
2 Select Input Range (your data)
3 Check Summary statistics
4 Choose Output Range → OK
Output Includes:
Mean

Kurtosis

Standard Error

Skewness

Median

Range

Mode

Minimum / Maximum

Standard Deviation

Sum

Sample Variance

Count

Pro Tip: This is much faster than typing individual formulas!
Excel Predictive Analytics

10 / 10

Introduction to Regression in Excel
Regression Goal
Find the equation that best predicts Y (dependent variable) from X (independent variable).
Ŷ = β0 + β1 X
Excel provides multiple ways to perform regression:
1

Individual Functions: SLOPE, INTERCEPT, RSQ

2

Array Function: LINEST (comprehensive)

3

Prediction Function: FORECAST.LINEAR, TREND

4

Data Analysis ToolPak: Full regression output

5

Chart Trendline: Visual with equation
December 12, 2025

1 / 10

Method 1: Basic Regression Functions
Simple Linear Regression: Ŷ = β0 + β1 X
Coefficient

Function

Formula

Slope (β1 )
Intercept (β0 )
R-squared

SLOPE
INTERCEPT
RSQ

=SLOPE(Y range, X range)
=INTERCEPT(Y range, X range)
=RSQ(Y range, X range)

Example - Predicting Sales from Ad Spend:
Y values (Sales) in B2:B20
X values (Ad Spend) in A2:A20
=SLOPE(B2:B20, A2:A20) → 8.5
=INTERCEPT(B2:B20, A2:A20) → 5000
=RSQ(B2:B20, A2:A20) → 0.92

Equation: Sales = 5000 + 8.5 × AdSpend

December 12, 2025

2 / 10

Understanding Regression Output
Equation: Sales = 5000 + 8.5 × Ad Spend
Interpreting the Coefficients:
Intercept (5000): Expected sales when Ad Spend = 0
Slope (8.5): For every $1 increase in ad spend, sales increase by $8.50
Interpreting R-squared (0.92):
92% of the variation in sales is explained by ad spend
This is a strong model!
Range: 0 (no fit) to 1 (perfect fit)
Making a Prediction:
If Ad Spend = $2000, predicted Sales = ?
=5000 + 8.5 * 2000 = $22,000
December 12, 2025

3 / 10

Method 2: FORECAST.LINEAR Function
Directly predict a value without calculating coefficients:
=FORECAST.LINEAR(x, known y’s, known x’s)
Example:
A (Ad Spend)

B (Sales)

1000
1500
2000
2500
3000

13500
17750
22000
26250
30500

1
2
3
4
5

Predict sales for Ad Spend = $3500:
=FORECAST.LINEAR(3500, B1:B5, A1:A5)
Result: $34,750
December 12, 2025

4 / 10

Method 3: TREND Function
TREND returns predicted values for multiple X values:
=TREND(known y’s, known x’s, new x’s)
Use Case: Predict for a range of values

1-5
6
7

A

B

C

Known X
1000-3000

Known Y
13500-30500

New X to Predict
3500
4000
4500

Formula (select D1:D3, enter as array with Ctrl+Shift+Enter):
=TREND(B1:B5, A1:A5, C1:C3)
Or in Excel 365, just press Enter - it spills automatically!
December 12, 2025

5 / 10

Method 4: LINEST Function (Comprehensive)
LINEST returns complete regression statistics in one array:
=LINEST(known y’s, known x’s, [const], [stats])
Parameters:
const: TRUE = calculate intercept, FALSE = force through origin
stats: TRUE = return additional statistics
Full output (stats=TRUE) - 5 rows × 2 columns:
Slope (β1 )
Std Error of Slope
R-squared
F-statistic
Regression SS

Intercept (β0 )
Std Error of Intercept
Std Error of Y
Degrees of Freedom
Residual SS

Usage: Select 5×2 range, type formula, press Ctrl+Shift+Enter
December 12, 2025

6 / 10

LINEST Example
Full regression analysis with one formula:
Step 1: Select empty 5×2 range (e.g., D1:E5) Output:
Step 2: Enter formula:
=LINEST(B2:B20, A2:A20, TRUE, TRUE)

8.5
0.42
0.92
215.6
337000

5000
850
1250
18
28125

Step 3: Press Ctrl+Shift+Enter
Reading the Output:
Slope = 8.5, Intercept = 5000
R² = 0.92 (92% variance explained)
F-stat = 215.6 (model is significant)
Use t-test: coefficient / std error for significance

December 12, 2025

7 / 10

Method 5: Data Analysis ToolPak Regression
The easiest way to get comprehensive output!
Steps:
1 Data tab → Data Analysis → Regression
2 Input Y Range: Select dependent variable (e.g., B1:B20)
3 Input X Range: Select independent variable(s) (e.g., A1:A20)
4 Check Labels if first row has headers
5 Choose output location
6 Optional: Check Residuals, Residual Plots
7 Click OK
Output Includes:
Regression Statistics (R, R², Adjusted R², Std Error)
ANOVA table (F-test for overall significance)
Coefficients table with t-stats and p-values
Residual output (if selected)
December 12, 2025

8 / 10

Reading ToolPak Regression Output
Three Key Sections:
1. Regression Statistics:
Multiple R
R Square
Adjusted R Square
Standard Error
Observations

0.959
0.920
0.915
1250
20

2. ANOVA Table: Check if F-significance < 0.05 (model is valid)
3. Coefficients:

Intercept
Ad Spend

Coef

Std Error

t Stat

P-value

5000
8.5

850
0.42

5.88
20.24

0.0001
0.0000

Interpretation: Both coefficients significant (p-value < 0.05)
December 12, 2025

9 / 10

Method 6: Chart Trendline
Visual regression with automatic equation:
Steps:
1 Create a Scatter Plot (Insert → Charts → Scatter)
2 Click on data points in chart
3 Right-click → Add Trendline
4 Choose Linear
5 Check: Display Equation on chart
6 Check: Display R-squared value on chart
Result on Chart:
y = 8.5x + 5000
R 2 = 0.92
Other Trendline Options: Exponential, Logarithmic, Polynomial, Power, Moving Average
December 12, 2025

10 / 10

Multiple Regression in Excel
Predict using MULTIPLE independent variables:
Ŷ = β0 + β1 X1 + β2 X2 + β3 X3 + . . .
Example: Predict Sales using:
X1 : Advertising Spend
X2 : Number of Salespeople
X3 : Price
Data Layout Required:
1
2
3
...

A

B

C

D

Ad Spend
1000
1500
...

Salespeople
5
7
...

Price
50
45
...

Sales
15000
22000
...
December 12, 2025

1 / 10

Multiple Regression with ToolPak
Steps:
1 Data → Data Analysis → Regression
2 Input Y Range: D1:D50 (Sales column)
3 Input X Range: A1:C50 (ALL predictor columns together)
4 Check Labels
5 OK
Output - Coefficients Table:

Intercept
Ad Spend
Salespeople
Price

Coefficient

Std Error

t Stat

P-value

8500
6.2
1200
-150

2100
0.35
180
45

4.05
17.71
6.67
-3.33

0.0002
0.0000
0.0000
0.0018

Equation: Sales = 8500 + 6.2(AdSpend) + 1200(Staff) - 150(Price)
December 12, 2025

2 / 10

Multiple Regression with LINEST
Using LINEST for multiple predictors:
=LINEST(D2:D50, A2:C50, TRUE, TRUE)
Important: Output columns are in reverse order!
β3 (Price) β2 (Staff) β1 (Ads) β0 (Intercept)
-150
1200
6.2
8500
SE of β3
SE of β2
SE of β1
SE of β0
R-squared
Std Error of Y
... more stats ...

Tip: Label your output cells to avoid confusion!

December 12, 2025

3 / 10

Making Predictions with Multiple Regression
Model: Sales = 8500 + 6.2(AdSpend) + 1200(Staff) - 150(Price)
Scenario: What if AdSpend=2000, Staff=8, Price=55?
Method 1: Direct Calculation
=8500 + 6.2*2000 + 1200*8 - 150*55
= 8500 + 12400 + 9600 - 8250 = $22,250
Method 2: Using TREND
=TREND(D2:D50, A2:C50, {2000, 8, 55})
Method 3: Cell References (Better for What-If)
Put inputs in cells F1:H1, then:
=TREND(D2:D50, A2:C50, F1:H1)
December 12, 2025

4 / 10

Practical Exercise: Build a Sales Predictor
Create an interactive prediction calculator:

1
2
3
4
5
6
7
8
9
10
11
12

A

B

Input Variables
Ad Spend ($):
Staff Count:
Price ($):

2500
6
48

Coefficients
Intercept:
β1 (Ads):
β2 (Staff):
β3 (Price):

=INTERCEPT(...)
=INDEX(LINEST(...),1,3)
=INDEX(LINEST(...),1,2)
=INDEX(LINEST(...),1,1)

Predicted Sales:

=B7+B8*B2+B9*B3+B10*B4
December 12, 2025

5 / 10

Checking Model Quality
Key Metrics to Evaluate:
Metric

Formula

Good Value

R-squared
Adjusted R²
RMSE
MAE
MAPE

=RSQ(actual, predicted)
From ToolPak output
=SQRT(AVERAGE((actual-pred)^2))
=AVERAGE(ABS(actual-pred))
=AVERAGE(ABS((act-pred)/act))

> 0.7
> 0.7
Lower is better
Lower is better
< 10% ideal

Calculating MAPE (Mean Absolute Percentage Error):
If Actual in A2:A20, Predicted in B2:B20:
=AVERAGE(ABS((A2:A20-B2:B20)/A2:A20))*100
Interpretation: ”On average, predictions are X% off”
December 12, 2025

6 / 10

Residual Analysis
Residuals = Actual - Predicted (the errors)
Why Analyze Residuals?
Check if model assumptions are met
Identify patterns the model missed
Detect outliers
Calculating Residuals:
=A2 - B2

(where A=Actual, B=Predicted)

What to Look For in Residual Plot:
Good:

Bad:

Random scatter around zero

Curved pattern (try polynomial)

No patterns

Funnel shape (heteroscedasticity)

Constant spread

Clusters of errors
December 12, 2025

7 / 10

Polynomial Regression (Non-Linear)
When linear isn’t enough - fit a curve!
Ŷ = β0 + β1 X + β2 X 2 + β3 X 3
How to Do It in Excel:
1 Create new columns with X 2 , X 3 , etc.
Column B: =A2^2
Column C: =A2^3
2
3

Run regression with all X columns (A, B, C)
OR use chart trendline → Polynomial

Using Trendline:
1 Create scatter plot
2 Add Trendline → Polynomial
3 Choose Order (2=quadratic, 3=cubic, etc.)
4 Display equation on chart
December 12, 2025

8 / 10

Dummy Variables for Categories
How to include categorical variables in regression:
Example: Include ”Region” (North, South, East, West) in sales model
Create Dummy Variables (0 or 1):
Region

D North

D South

D East

Sales

North
South
East
West

1
0
0
0

0
1
0
0

0
0
1
0

15000
18000
12000
16000

Note: Use n − 1 dummies for n categories (West is baseline)
Formula to Create Dummy:
=IF(A2="North", 1, 0)
December 12, 2025

9 / 10

Regression Summary: Quick Reference

Task

Function/Method

When to Use

Get slope only

=SLOPE(y, x)

Quick coefficient

Get intercept only

=INTERCEPT(y, x)

Quick coefficient

Get R² only

=RSQ(y, x)

Check model fit

Predict single value

=FORECAST.LINEAR(x, y, x range) One prediction

Predict multiple values
Full statistics (simple)
Full statistics (multiple)
Visual + equation

=TREND(y, x, new x)

Many predictions

=LINEST(y, x, TRUE, TRUE)
Data Analysis ToolPak

Detailed analysis
Best for reports

Chart Trendline

Presentations

December 12, 2025

10 / 10

Time-Series Forecasting in Excel
Predict future values based on historical patterns
Excel Time-Series Tools:
1

Moving Average - Smooth out noise

2

Exponential Smoothing - Weight recent data more

3

FORECAST.ETS - Automatic seasonal forecasting

4

FORECAST.LINEAR - Linear trend projection

5

Forecast Sheet - One-click forecasting (Excel 2016+)

Data Requirements:
Consistent time intervals (daily, weekly, monthly)
No missing values (or minimal gaps)
Sufficient history (2+ seasonal cycles recommended)
December 12, 2025

1 / 10

Moving Average
Smooth data by averaging over a window of periods
Formula (3-period moving average):
=AVERAGE(B2:B4)
Copy down for each period:

2
3
4
5
6

A (Month)

B (Sales)

C (3-Mo MA)

Jan
Feb
Mar
Apr
May

100
120
110
130
125

–
–
=AVERAGE(B2:B4) = 110
=AVERAGE(B3:B5) = 120
=AVERAGE(B4:B6) = 122

General formula in row n:
=AVERAGE(B(n-2):Bn)

or

=AVERAGE(OFFSET(Bn,-2,0,3,1))
December 12, 2025

2 / 10

Moving Average with Data Analysis ToolPak
Steps:
1 Data → Data Analysis → Moving Average
2 Input Range: Your time series data
3 Interval: Number of periods to average (e.g., 3, 12)
4 Output Range: Where to put results
5 Optional: Check Chart Output
6 OK
Choosing the Interval:
Short interval (3-5): More responsive, less smooth
Long interval (12+): Smoother, but lags behind trends
Seasonal data: Use interval = season length (12 for monthly)
Forecasting: Last MA value becomes next period forecast
December 12, 2025

3 / 10

Weighted Moving Average
Give more importance to recent observations:
Example: Weights of 0.5, 0.3, 0.2 (most recent first)
=0.5*B6 + 0.3*B5 + 0.2*B4
Or using SUMPRODUCT:
=SUMPRODUCT(B4:B6, {0.2, 0.3, 0.5})
Weights should sum to 1.0
Month

Sales

Simple MA

Weighted MA

Apr
May
Jun

100
120
140

–
–
120

–
–
128

Weighted MA (128) reacts faster to the upward trend!
December 12, 2025

4 / 10

Exponential Smoothing
Automatically weights all past data, with decay:
Ft+1 = α · Yt + (1 − α) · Ft

Ft+1 = Forecast for next period
Yt = Actual value this period
Ft = Forecast for this period
α = Smoothing constant (0 to 1)
Excel Formula:

=alpha*B2 + (1-alpha)*C1
Where: B = Actuals, C = Forecasts, alpha in a named cell
Choosing Alpha:
High α (0.7-0.9): More reactive to recent changes
Low α (0.1-0.3): Smoother, stable forecasts
December 12, 2025

5 / 10

Exponential Smoothing Example
α = 0.3, First forecast = First actual

1
2
3
4
5
6

A

B (Actual)

C (Forecast)

D (Formula)

Month 1
Month 2
Month 3
Month 4
Month 5
Month 6

100
120
110
130
125
?

100
100
106
107.2
114.0
117.3

(start value)
=0.3*100+(1-0.3)*100
=0.3*120+(1-0.3)*100
=0.3*110+(1-0.3)*106
=0.3*130+(1-0.3)*107.2
=0.3*125+(1-0.3)*114.0

General formula (row 3 onwards):
=$G$1*B2+(1-$G$1)*C2
where G1 contains alpha value (0.3)
Forecast for Month 6: 117.3
December 12, 2025

6 / 10

Exponential Smoothing with ToolPak
Steps:
1
2
3
4

Data → Data Analysis → Exponential Smoothing
Input Range: Your time series

Damping Factor: Enter 1 − α (e.g., 0.7 for α=0.3)
Output Range: Where to put results

5

Optional: Check Chart Output

6

OK

Warning: Damping Factor!
ToolPak asks for Damping Factor = 1 − α
If you want α = 0.3, enter 0.7 as damping factor!
Pro Tip: Try different alphas and pick one with lowest error
December 12, 2025

7 / 10

FORECAST.ETS Function (Excel 2016+)
Automatic seasonal forecasting with one function!

=FORECAST.ETS(target date, values, timeline, [seasonality], [data completion],
Example:
Sales data in B2:B25 (24 months)
Dates in A2:A25
Forecast for next month (in A26):
=FORECAST.ETS(A26, B2:B25, A2:A25)
Parameters:
seasonality: 0=none, 1=auto-detect, or specify (12 for monthly)
data completion: 1=interpolate missing values
aggregation: How to handle duplicate dates (1=average)
December 12, 2025

8 / 10

FORECAST.ETS Related Functions
Complete ETS Function Family:
Function

Purpose

=FORECAST.ETS()
=FORECAST.ETS.CONFINT()
=FORECAST.ETS.SEASONALITY()
=FORECAST.ETS.STAT()

Point forecast for target date
Confidence interval for forecast
Detected seasonality length
Model statistics (alpha, beta, etc.)

Example with Confidence Interval:
=FORECAST.ETS(A26, B2:B25, A2:A25) → 150
=FORECAST.ETS.CONFINT(A26, B2:B25, A2:A25, 0.95) → 25
Interpretation: Forecast = 150 ± 25 (95% confidence)
Range: 125 to 175
December 12, 2025

9 / 10

One-Click Forecast Sheet (Excel 2016+)
The easiest forecasting method in Excel!
Steps:
1 Select your data (dates + values)
2 Go to Data tab → Forecast group → Forecast Sheet
3 Set Forecast End date
4 Click Options to customize:
Seasonality (auto or manual)
Confidence interval (default 95%)
Fill missing points
5

Click Create

Output:
New worksheet with forecast table
Professional chart with confidence bands
Upper and lower confidence bounds
Perfect for quick business forecasting!
December 12, 2025

10 / 10

Seasonal Decomposition
Break time series into: Trend + Seasonality + Residual
Step 1: Calculate Centered Moving Average (for trend)
For monthly data (12-period CMA):
=AVERAGE(OFFSET(B2,-6,0,12,1))
Step 2: Calculate Seasonal Ratio
=B2/C2

(Actual / Trend)

Step 3: Average Seasonal Ratios by Month
=AVERAGEIF(A:A, "Jan", D:D)
Step 4: Deseasonalize Data
=B2/E2

(Actual / Seasonal Index)

Step 5: Forecast trend, then reseasonalize
December 12, 2025

1 / 10

Linear Trend Forecasting
For data with clear upward or downward trend:
Method 1: FORECAST.LINEAR
=FORECAST.LINEAR(x, known y’s, known x’s)
Example with periods 1-12, forecast period 13:
=FORECAST.LINEAR(13, B2:B13, A2:A13)
Method 2: TREND for multiple forecasts
=TREND(B2:B13, A2:A13, A14:A16)
Returns forecasts for periods 13, 14, 15
Method 3: Calculate manually
=INTERCEPT(B2:B13, A2:A13) + SLOPE(B2:B13, A2:A13)*13
December 12, 2025

2 / 10

Forecast Accuracy Metrics
How to measure forecast quality:
Metric

Excel Formula

Interpretation

MAE
MSE
RMSE
MAPE

=AVERAGE(ABS(A2:A20-B2:B20))
Avg absolute error
=AVERAGE((A2:A20-B2:B20)^2)
Avg squared error
=SQRT(AVERAGE((A2:A20-B2:B20)^2))
Root mean sq error
=AVERAGE(ABS((A-B)/A))*100
Avg % error

A = Actual, B = Forecast

MAPE Interpretation:
< 10%: Excellent forecast
10 − 20%: Good forecast

20 − 50%: Reasonable forecast
> 50%: Poor forecast

December 12, 2025

3 / 10

What-If Analysis: Data Tables
See how changing inputs affects predictions:
One-Variable Data Table:
1 Set up your prediction formula (e.g., in cell C1)
2 List input values in a column (A2:A10)
3 Put formula reference in B1: =C1
4 Select range A1:B10
5 Data → What-If Analysis → Data Table
6 Column input cell = the cell your formula references
Example: Sales = 5000 + 8×AdSpend
Ad Spend

Predicted Sales

1000
2000
3000

13000
21000
29000
December 12, 2025

4 / 10

What-If Analysis: Goal Seek
Find the input needed to achieve a target output:
Example: ”What Ad Spend do we need for $50,000 in Sales?”
Setup:
Cell B1: Ad Spend input (start with any value)
Cell B2: =5000 + 8*B1 (prediction formula)
Steps:
1
2

Data → What-If Analysis → Goal Seek
Set cell: B2

3

To value: 50000

4

By changing cell: B1

5

Click OK

Result: B1 shows $5,625 (the required Ad Spend)
December 12, 2025

5 / 10

What-If Analysis: Scenario Manager
Compare multiple scenarios side by side:
Example Scenarios:
Best Case: High ad spend, low price, high staff
Base Case: Current values
Worst Case: Low ad spend, high price, low staff
Steps:
1
2

Data → What-If Analysis → Scenario Manager
Click Add to create each scenario

3

Define changing cells (your inputs)

4

Enter values for each scenario

5

Click Summary to generate comparison table

Output: New worksheet showing all scenarios and their predicted outcomes
December 12, 2025

6 / 10

Using Solver for Optimization
Find optimal inputs to maximize/minimize predictions
Example: Maximize profit given constraints
Enable Solver:
1 File → Options → Add-ins
2 Manage: Excel Add-ins → Go
3 Check Solver Add-in → OK
Using Solver:
1 Data → Solver
2 Set Objective: Your prediction/profit cell
3 To: Max, Min, or Value
4 By Changing: Your input cells
5 Subject to Constraints: Add budget limits, etc.
6 Solve
December 12, 2025

7 / 10

Best Practices for Excel Analytics
Data Preparation:
✓Remove duplicates (Data → Remove Duplicates)
✓Handle missing values (delete, average, or flag)
✓Check for outliers (use IQR method)
✓Ensure consistent data types
Model Building:
✓Start simple, add complexity if needed
✓Always hold out test data (don’t train on 100%)
✓Check residuals visually
✓Compare multiple models
Documentation:
✓Use named ranges for clarity
✓Add comments to complex formulas
✓Create a summary sheet with key metrics
December 12, 2025

8 / 10

Quick Reference: All Key Functions

Category

Function

Purpose

3*Statistics

AVERAGE, MEDIAN, STDEV.S
CORREL, COVARIANCE.S
PERCENTILE.INC, QUARTILE.INC

Descriptive stats
Relationship analysis
Distribution analysis

4*Regression

SLOPE, INTERCEPT
RSQ
LINEST
TREND

Get coefficients
R-squared value
Full regression stats
Predict values

4*Forecasting

FORECAST.LINEAR
FORECAST.ETS
FORECAST.ETS.CONFINT
FORECAST.ETS.SEASONALITY

Linear projection
Seasonal forecasting
Confidence intervals
Detect seasonality

2*Error Metrics

ABS, SQRT, AVERAGE
SUMPRODUCT

Build MAE, RMSE
Weighted calculations

December 12, 2025

9 / 10

Summary and Next Steps
What We Covered:
1

Statistical foundations (descriptive stats, correlation)

2

Linear & multiple regression (5 different methods!)

3

Time-series forecasting (MA, ES, ETS)

4

What-if analysis (Data Tables, Goal Seek, Scenarios)

5

Model evaluation (MAE, RMSE, MAPE)

To Go Further:
Practice with real datasets (Kaggle, data.gov)
Learn Power Query for data preparation
Explore Power Pivot for larger datasets
Consider Python/R for advanced analytics
December 12, 2025

10 / 10




-------------------------------------------
FILE: stats_handout_compressed.pdf
-------------------------------------------
  






 

 




  

       #   #$
        


 

     
     # 
μ  &   




 ( 

 

      ( ! 
 #    !  * !
    +


   

   
 #      
   ,
     #    
  '

 (x  μ) 
 =
2



n





2

   s &

 (x  x ) 

 x  μ 
 =
2

2



2

2

n

     #'




 

s2 =

n 1

x 
2

2







s2 =

( x ) 2

n 1

n



   

   
       *  !
      s     


 

 

 +

       )  $     
 
"!     
  %

s

n



 !! %

1 1
s12 s22
+ 
    s  +
 &   
n1 n 2
 n1 n 2 
2



   

!"!%" " '!" #"#"! " !$#



#'"!!243+!""!"""#"!#"!$#2μ43
" "$'"!!23+!""!"""!"#1! " "1!!!"μ4
"-!""!"+!" (!" "% μ4

t=

x  μ0

s
n



 ! 23>

"""-!" #" "$#! "-$#2 "'"""!%!"'
$μ4!"#"3#!"#""-!""!" ! ,
+μAμ4""-!""!"!'!"$* "!$
+μ@μ4""-!""!"!'"$*""-!" #"!!'" ! " "'!""!""!"% !"$
"+""-!""!"!"/% 0!)"-$#!5#!"$" "
+μ?μ4 "-$#!""-!""!"% !"$#"2"! "!!" " 
"3
"-$#!!!"" " $# !2!#!#'4,493) ""#
'"!!""" "$'"!!,

  +
# &  !!!! """"#!!#"&"',#"
!# "!#$!'# "%" ' %,#  !#"! $%,  
&#""$ ! %" !4,491,!"!#$'# "%" &
""2!!#>4,493.

4+μ>4,49*+μA4,49
'
#
1
#""!" $"'# !+
5
4,495
x = 0.0508 
6
4,4949
(x  x ) 2 (0.051 0.0508) 2 + (0.0505  0.0508) 2 + etc...
2
s
=
=
= 9.15 107 
7
4,48=
6
n
1
8
4,495:
s = s2 = 9.56 104
9
4,496

:
4,494<
;
4,494:




"-!""!"!+ t =

x  μ0 0.0508  0.05
=
= 2.17 " !  >;-5>:
s
9.56 104
n
7

"""-!" #" "$#!")6,5;%": ! !"%4,49>4,469,
!!"""-$#!!!"4,49)!'# "4#"""!#$'# "
%" &!"",








 

!"!%" "!"%#"! !"' " " 
 
$# # !! "'"$#""  #*+ " $#!"  #
" "" $#""
#" """%$#! $#""!"$#!2" !3#!
μ4>4"  -!"-"!"
  
"%#"! "
4+!""!"""!""%#"! #2μ5>μ63
+!""!"""!""%#"! ## ! " """ 2μ5?μ6)μ5Aμ6)μ5@μ63


!+ !! !)
 ""# ) t =

x1  x 2
x1  x 2
 """# ) t =

2
2



1
1
s
s
1
2
s2 + 
 + 
n
n
n
n
 1
 1
2
2

 ;.1+1/:.2+1/

!!!+ !"!!#" !+#"" !"!!+ !! ! 
* !! !!+ !! !. "!!!  /!
"!+#"μ1<μ2*

 
 ! 18! *12$ !!!##700& . !
#!;21& /* !6" !! !##668& . !
#!;30& /*  !!! ! ! . ";0*05/,

μ1;700'1;21'1;12(μ2;668'2;30'2;6
0)μ1;μ2
)μ1=μ2." $&  !!!   /
! "!!!# !!$"! "" !!! " !
!#!& *
!+ !! ! ) t =

x1  x 2
 s12 s22 
 + 
 n1 n 2 

=

700  668
212 30 2
+
12
6

= 2.342 

 ;.1+1/:.2+1/;.12+1/:.6+1/;16
!!+ !"!!'!+#" !$0*010*02' $!0*  !!
!  ! ! *




  

 

   

 $!! #!!!!  # !"!
0)! #!!! !# !"!
)! #!! !!!# !"!

(O  E) 2
. ! ##" !%!#"/
+ " !! ! )  = 
E
2

 ;"! ! !"!-1

!!+#"!!2!#" " !"!2#" *!+#"  !'!
 #! !!!%! !"!*='!!&! !%! !"!


 
""  $"!!!!!!!"!&*
"  !"!" * ""'"$!
"'" *#" !1!"* 2  $
$*! !"!& !!&. ";0*05/,
 "
 
" "
" 
143
60
55
18

! !! !&!9)3)3)1' )
0)! # !"!2 ! 9)3)3)1 !"!
)! # !"!2  !!9)3)3)1 !"!
%!#" )
 "
 
" "
" 
155*25
51*75
51*75
17*25

2 = 

(O  E) 2 (143 155.25) 2 (60  51.75) 2 (55  51.75) 2 (18 17.25) 2
=
+
+
+
= 2.519 
E
155.25
51.75
51.75
17.25

<5,2<4
!##3!#%$")#,%$"!#!#1+36)"#"!#!$!!#(
""!##($""+

 
$!"#$(###!"!"$"#!#!#
!!"#"!($"+#"$+#$""($!
"#!$#.""$<1+16/-

1*#!"$!"#!$#$"
*#!"#$!"#!$#$"
!!##36$")"#!"$!"#!$#)#!"$3+889
$"!" $!+ !#$"!%%$"!3)5)5)4)4)4)3)4)2)"#3"##"#"*

2 = 




 (2  2.778) 2   (3  2.778) 2   (4  2.778) 2 
(O  E) 2 (1 2.778) 2
=
+ 2
+ 4
+ 2
= 2.72 
E
2.778
 2.778   2.778   2.778 

<:,2<9
!##3!#%$")#,%$"!#!#1+36)"&#!#1+ $""(
$!"#!$#+

  

  


"&#!#&#!%!"!!#!#./
1*##&%!"!#
*##&%!"!##
"#(""$#"$#'#"#!$#
"!%%$".02)03)04)05/!$"$(!"#"#+!&"#!(%!2
$"#!(%!3+


!2
#"


#!( #!( 
!3
#!( 02
0 3
02;03
#!( 04
0 5
04;05
#"

02;04
03;05
02;03;04;05
!!##!(%!2"#$!%$"#!(%(###$!
 +""$)#'#$!%$"##&##!(
#1 +# 3
%$" 

 #1 +# 2 +# 3 +# 4 
%!3"#!!##!($#(#$!%$"#!(


#1 +# 3
+ $")#'#%$"*

( #1 +# 2 )
 #1 +# 2 +# 3 +# 4 

E=

(#1 +# 3 )(#1 +# 2 ) (row total)(column total) 
=
#1 +# 2 +# 3 +# 4
grand total

!"!<.,2/.,2/ &!"#$!!&""#$!$"
2
," $!"##"#""#  2 =  (O  E) 
E

#,%$"!##3!#%$"+

 
%##&)"#!!#"#&#""%"#".""$<1+16/-

#"" %


&
$, &
$,

%!"
224
224
221
26:
5:6
!!"!"
22:
246
283
2:1
727
2#:!##"(
88
:2
97
76
42:
=21!##"(
292
263
235
84
641

5:1
5:2
5:3
598
2:71


,&!   
 &!   
%"   # '%# &

E=

(row total)(column total) (495)(490)
=
= 123.75 
grand total
1960



!

-5  $
7-,  $

2 = 

!
"
 ("  (
-./'31 -.0
-.0'.2
-10
-10'/-
-10'2/
35'31
35'5-
4,',4
-/.'1
-/.'33
-//',0


-..'55
-1/',2
35'.2
-/-'25

(O  E) 2 (113 123.75) 2 (113 124) 2 (110 124.26) 2
=
+
+
+ etc... = 91.73 
E
123.75
124
124.26

6*(-+*(-+6*0(-+*0(-+65

 . !  %(!   ,',,-%",    
  "!  '


 




 


 



$   $



$    $




   


  







$!  

P(A and B) = P(A)  P(B) "!  

   





$!! 

P(A or B) = P(A) + P(B)  P(A and B) 

    







$! ! !  



P(A | B) =

P(A and B)

P(B)







P(A | B) =

P(B | A)  P(A)
 
P(B)



 &
  $   $  $"" '" 
 ' "($ "$     ($"  !'    $'  

$   !  $)


$

"    "

0.5  0.5  0.5 = 0.125 = P(A and B) = P(2 children = bb and 1st child bb) 
 $
" . % P(B) = P(1st child = bb) = 0.5 
P(A and B) 0.125
$ P(2 children = bb | 1st child bb) = P(A | B) =
=
0.25 
P(B)
0.5
 
    "$      1-2   #  /-2 
  !' $   #&     "$  

"  #(

)%+ 3 #*3 ,
) + 3 # 3 ,    & $%

P(A | B) =



P(B | A)  P(A)
P(B)


P(S = Z |W = Zz) = 0.5 +0-2    ,
P(W = Zz) = 0.7 +,
P(S = Z) = (0.7  0.5) + (0.3 1) = 0.65 +       #
0.5  0.7
P(W = Zz | S = Z) =
= 0.538 
0.65

      

 
  



 " 
m

P(X = m) =

,

$" "

(nm )

n! p  (1 p)
m!(n  m)!



    3
" 
 
 
"    "(

P(1 boy of 5 children) =

 

  

5!0.51  0.5 4
= 0.15625 
1!(4)!



           $   
 "&
       &

P(X = m) =

enp  (n  p) m

m!

%    ! 






